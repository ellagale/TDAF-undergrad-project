{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a249b443",
   "metadata": {},
   "source": [
    "# What causes a molecule to be toxic and how can we predict it?\n",
    "\n",
    "\n",
    "The “Toxicology in the 21st Century” (Tox21) initiative created a public database measuring toxicity of compounds, which has been used in the 2014 Tox21 Data Challenge. This dataset contains qualitative toxicity measurements for 8k compounds on 12 different targets, including nuclear receptors and stress response pathways.\n",
    "\n",
    "Random splitting is recommended for this dataset.\n",
    "\n",
    "The raw data csv file contains columns below:\n",
    "\n",
    "“smiles” - SMILES representation of the molecular structure\n",
    "\n",
    "“NR-XXX” - Nuclear receptor signaling bioassays results\n",
    "\n",
    "“SR-XXX” - Stress response bioassays results\n",
    "\n",
    "please refer to https://tripod.nih.gov/tox21/challenge/data.jsp for details.\n",
    "\n",
    "This problem is a classification problem: Toxic/Non-toxic. \n",
    "\n",
    "References:\n",
    "1. https://moleculenet.org/ website and paper on the datasets\n",
    "2. https://deepchem.readthedocs.io/en/latest/index.html manual\n",
    "3. https://github.com/deepchem/deepchem/tree/master/examples/tutorials tutorials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load our libraries\n",
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"TensorFlow version: \" + tf.__version__)\n",
    "print(\"DeepChem version: \" + dc.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e993e49",
   "metadata": {},
   "source": [
    "# Loading the dataset example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29709bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line loads the  dataset - we are using the extended connectivity fingerprints here, \n",
    "# there are several featurizers for you to try\n",
    "# the splitter splits the dataset for you, refer to the MoleculeNet paper to see which one you need\n",
    "tasks, datasets, transformers = dc.molnet.load_tox21(\n",
    "    featurizer='ECFP', \n",
    "    splitter='random')\n",
    "# the datasets object is already split into the train, validation and test dataset \n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "## N.B. Some molecules may not featurize and you'll get a warning this is OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb104d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset contains the input fingerprints as X, the output is stored in y, and the IDS are the smiles strings\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d4fe2",
   "metadata": {},
   "source": [
    "# Using RDKit\n",
    "\n",
    "RDkit is a chemistry package that allows you to create chemical features or do very simple computational chemistry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this draws a nice image of the first 12 molecules of the test_dataset\n",
    "SMILES_strings = test_dataset.ids[:12]\n",
    "ms = [Chem.MolFromSmiles(x) for x in SMILES_strings] \n",
    "Draw.MolsToGridImage(ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993037f",
   "metadata": {},
   "source": [
    "RDKit does not give us just a picture of the molecule, it creates a molecule object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_number = 1\n",
    "formula = SMILES_strings[molecule_number]\n",
    "a_molecule = Chem.MolFromSmiles(formula)\n",
    "a_molecule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca0a00",
   "metadata": {},
   "source": [
    "The function `dir()` gives a list of the possible things that object can do. (Ignore those that start with ``__`` these are internal functions that you want to leave alone). For example, you could run `a_molecule.GetNumAtoms` to get the number of atoms. More details of what you can calculated with RDKit is here:\n",
    "https://rdkit.org/docs/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(a_molecule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help brings up the manual for the object\n",
    "help(a_molecule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63674e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_molecule.GetNumAtoms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b2dcd",
   "metadata": {},
   "source": [
    "# More about the dataset\n",
    "The input, `X`, is a fingerprint, a list of `1`s and `0`s for wether a feature is present or not (a feature like a functional group). There are other types of input that you get from other featurizers that are needed for different ML models.\n",
    "\n",
    "The output `y` is the is molecule's toxicity against various targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be17d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.X[molecule_number]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959cf92d",
   "metadata": {},
   "source": [
    "The `1.`s below indicate that this model failed a toxicity test! I.e. it was toxic against that receptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d89462",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.y[molecule_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8084cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the molecule again.\n",
    "a_molecule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd4355",
   "metadata": {},
   "source": [
    "These are the tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd64a59",
   "metadata": {},
   "source": [
    "#### How many toxic molecules do we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02724502",
   "metadata": {},
   "source": [
    " You can investigate the data, for example, below is the count of toxic drug candiates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will probably want to do this for all the datasets\n",
    "# but lets just look at the test dataset here\n",
    "# this is the number of toxic molecules for each task\n",
    "sum(test_dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b91f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how many molecules there are in this dataset\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot the percentages of toxic molecules per task\n",
    "ax=plt.bar(train_dataset.tasks[:7],100*sum(train_dataset.y[:,:7])/len(train_dataset))\n",
    "plt.bar(train_dataset.tasks[7:],100*sum(train_dataset.y[:,7:])/len(train_dataset))\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.ylabel(\"Toxic molecules %\")\n",
    "#plt.xlabel(\"Tzxcasks\")\n",
    "plt.title(\"Test Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46135d19",
   "metadata": {},
   "source": [
    "### THIS IS A SMALL NUMBER OF TOXIC MOLECULES!\n",
    "\n",
    "This is a very unbalanced dataset, you only have around 5-10% toxic molecules and the rest are non-toxic, this is an unbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the nuclear reactor tasks\n",
    "test_dataset.tasks[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30bdd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stress receptor tasks\n",
    "test_dataset.tasks[7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb16699",
   "metadata": {},
   "source": [
    "# Machine learning example\n",
    "(see also the 4th and 5th notebooks in the Machine Learning for Chemist's course).\n",
    "The code below uses one of `deepchem`'s models. There are also relevant models in the `sklearn` module. There are more options that be tuned. The process of changing these options is called *hyperparameter optimisation*.\n",
    "\n",
    "Here we train a classifier that will work on more than one task at a time. Classifiers put molecules into two classes: toxic or not toxic and multiclass classifiers do it on a per assay basis.\n",
    "\n",
    "Here we train a MultitaskClassifier on the NR datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd7b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads in a general purpose regression model\n",
    "model = dc.models.MultitaskClassifier(\n",
    "    n_tasks = len(test_dataset.tasks), # size of y, we have one output task here: finding toxicity\n",
    "    n_features = len(test_dataset.X[0]), # number of input features, i.e. the length of the ECFPs\n",
    "    layer_size = [1000],\n",
    "    weight_init_stddevs = 0.02,\n",
    "    bias_init_consts = 1.0,\n",
    "    weight_decay_penalty = 0.001,\n",
    "    weight_decay_penalty_type = 'l2',\n",
    "    dropouts = 0.2,\n",
    "    n_classes = 2,\n",
    "    residual = False\n",
    ")\n",
    "\n",
    "############################################\n",
    "# Now we fit the training dataset!         #\n",
    "############################################\n",
    "model.fit(train_dataset, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68f0b5",
   "metadata": {},
   "source": [
    "This tests the trained model. Classifiers are usually tested by looking at the Area Under the Curve of the Receiver Operator Curve or **ROC AUC**, these are the numbers in deepchem that you will compare to. ROC AUC goes from 0 to 1, 0.5 means it's as good as random guessing, 1 is perfect, and anything less than 0.5 means it's worse than guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bdbc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line tells deepchem what metric to use to score the datasets\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "\n",
    "# model.evaluate() tests the model. \n",
    "# we have to give it the data to use, the metric (or set of metrics) and the transformer used\n",
    "print('Over all tasks:')\n",
    "print(\"Training set score:\", model.evaluate(train_dataset, [metric], transformers))\n",
    "print(\"Valid set score:\", model.evaluate(valid_dataset, [metric], transformers))\n",
    "print(\"Test set score:\", model.evaluate(test_dataset, [metric], transformers))\n",
    "ground_truth = test_dataset.y\n",
    "predictions = model.predict(test_dataset)\n",
    "metric = dc.metrics.roc_auc_score\n",
    "print('Per task:')\n",
    "print('Task\\t\\tROC AUC')\n",
    "for i in range(len(test_dataset.tasks)):\n",
    "    score = metric(dc.metrics.to_one_hot(ground_truth[:,i]), predictions[:,i])\n",
    "    print('{:}\\t\\t{:.3}'.format(test_dataset.tasks[i],score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89116ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_names=tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc183de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff(x):\n",
    "    if x>0.55:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task_names = ['SR-ARE','SR-ATAD5','SR-HSE','SR-MMP','SR-p53']\n",
    "for task_number in range (5):\n",
    "    metrics.RocCurveDisplay.from_predictions(ground_truth[:,task_number], predictions[:,task_number][:,1])\n",
    "    plt.plot([0,1], [0,1])\n",
    "    plt.axis('square')\n",
    "    plt.title(test_dataset.tasks[task_number])\n",
    "    print('f1_score of {} is'.format(task_names[task_number]), f1_score(ground_truth[:,task_number], [cutoff(x) for x in predictions[:,task_number][:,1]], average='binary'))\n",
    "    print('No. of predicted toxic molecules of {} is'.format(task_names[task_number]), sum([cutoff(x) for x in predictions[:,task_number][:,1]]))\n",
    "    print('Ground truth of {} is'.format(task_names[task_number]), sum(test_dataset.y)[task_number])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3acb6b7",
   "metadata": {},
   "source": [
    "Here is a ROC AUC for one of the tasks. It looks OK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_number = 1\n",
    "metrics.RocCurveDisplay.from_predictions(ground_truth[:,task_number], predictions[:,task_number][:,1])\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.axis('square')\n",
    "plt.title(test_dataset.tasks[task_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f5c11",
   "metadata": {},
   "source": [
    "But, in this field you can score the accuracy of your model in several ways, the metric you use for classificiation where you have unbalanced datasets is the F1 score (there are also other valid choices). And the f1 score for this task is terrible! (F1 goes between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(ground_truth[:,task_number], [round(x) for x in predictions[:,task_number][:,1]], average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92439b",
   "metadata": {},
   "source": [
    "This code counts how many molecules were predicted to be toxic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ccc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([round(x) for x in predictions[:,task_number][:,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480a9e1",
   "metadata": {},
   "source": [
    "This model has solved the problem by just saying everything is non-toxic, this is not what we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ccd57",
   "metadata": {},
   "source": [
    "## Training with early stopping\n",
    "\n",
    "Rather than training for a set number of epochs, it is better to use early stopping. This will train until the point where the valdiation results start to get worse. This avoids overtraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f01b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how many epochs wait to see if the training gets better\n",
    "patience = 15\n",
    "# these are some nice metrics\n",
    "metric1 = dc.metrics.Metric(dc.metrics.balanced_accuracy_score)\n",
    "metric2 = dc.metrics.Metric(dc.metrics.prc_auc_score)\n",
    "metric3 = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "metric4 = dc.metrics.Metric(dc.metrics.f1_score)\n",
    "#metric3 = dc.metrics.Metric(dc.metrics.mae_score)\n",
    "metrics = [metric1, metric2, metric3, metric4]\n",
    "selected_metric = 2 #which metric to use for callback \n",
    "        #i.e. which to train with\n",
    "metric_selector=2\n",
    "metric_labels = ['balanced_accuracy_score',#0\n",
    "                 'prc_auc_score',#1\n",
    "                 'roc_auc_score',#2\n",
    "                 'f1_score']#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db811e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads in a general purpose regression model\n",
    "model = dc.models.MultitaskClassifier(\n",
    "    n_tasks = len(test_dataset.tasks), # size of y, we have one output task here: finding toxicity\n",
    "    n_features = len(test_dataset.X[0]), # number of input features, i.e. the length of the ECFPs\n",
    "    layer_size = [1000,500],\n",
    "    weight_init_stddevs = 0.02,\n",
    "    bias_init_consts = 1.0,\n",
    "    weight_decay_penalty = 0.001,\n",
    "    weight_decay_penalty_type = 'l2',\n",
    "    dropouts = 0.2,\n",
    "    n_classes = 2,\n",
    "    residual = False\n",
    ")\n",
    "\n",
    "# this sets up a callback on the validation\n",
    "callback = dc.models.ValidationCallback(\n",
    "            valid_dataset,\n",
    "            patience,\n",
    "            metrics[metric_selector])\n",
    "# fit da model\n",
    "model.fit(train_dataset, nb_epoch=2, callbacks=callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d469b",
   "metadata": {},
   "source": [
    "## This is a nice function to create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb3f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_them_metrics(\n",
    "        model,\n",
    "        datasets,\n",
    "        metrics,\n",
    "        metric_labels,\n",
    "        transformers=[],\n",
    "):\n",
    "    \"\"\"calculates metrics for a run\n",
    "    model: trained model\n",
    "    # datasets: tuple of datasets\n",
    "    # metrics: list of metric objects\n",
    "    # metric labels: sensible labels\"\"\"\n",
    "    ugh = []\n",
    "    for dataset in datasets:\n",
    "        if transformers == []:\n",
    "            egg = model.evaluate(\n",
    "                dataset,\n",
    "                metrics)\n",
    "        else:\n",
    "            egg = model.evaluate(\n",
    "                dataset,\n",
    "                metrics,\n",
    "                transformers=transformers)\n",
    "        for metric_label in metric_labels:\n",
    "            if metric_label == 'rmse':\n",
    "                ugh.append(np.sqrt(egg['mean_squared_error']))\n",
    "            else:\n",
    "                ugh.append(egg[metric_label])\n",
    "    return ugh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# little function to calc metrics on this data\n",
    "out=get_them_metrics(\n",
    "            model,\n",
    "            datasets,\n",
    "            metrics,\n",
    "            metric_labels,\n",
    "            transformers)\n",
    "# makes a nice dataframe\n",
    "pd_out = pd.DataFrame([out], columns=['tr_mse', 'tr_r2', 'tr_mae', 'tr_rmse',\n",
    "                                        'val_mse', 'val_r2', 'val_mae', 'val_rmse',\n",
    "                                        'te_mse', 'te_r2', 'te_mae', 'te_rmse'])\n",
    "pd_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aad799",
   "metadata": {},
   "source": [
    "You can save this as a .csv file using `pd_out.to_csv('filename.csv')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43a11c",
   "metadata": {},
   "source": [
    "# Training models\n",
    "\n",
    "This approach (ECFP + multiclass classifier) isn't very good, it's gotten nice AUC ROC curves but this is misleading. You will have to have a look at how to train with unbalanced classes. \n",
    "\n",
    "See for example:\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/\n",
    "https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\n",
    "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/\n",
    "and the balancing and unbalancing transformers\n",
    "https://deepchem.readthedocs.io/en/latest/api_reference/transformers.html?highlight=dc.trans.BalancingTransformer#deepchem.trans.BalancingTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550b3a7",
   "metadata": {},
   "source": [
    "Now go and have a play"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
