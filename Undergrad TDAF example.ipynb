{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df6a859",
   "metadata": {},
   "source": [
    "# Example of how to use topological data analysis features\n",
    "\n",
    "This includes examples of regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f37e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#sys.path.append(r\"C:\\Users\\ella_\\Documents\\GitHub\\graphs_and_topology_for_chemistry\")\n",
    "#sys.path.append(r\"C:\\Users\\ella_\\Documents\\GitHub\\icosahedron_projection\")\n",
    "\n",
    "import deepchem as dc\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import rdkit\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri\n",
    "import rdkit.Chem as Chem\n",
    "import rdkit.Chem.AllChem as AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import mpl_toolkits.mplot3d\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "\n",
    "print(\"TensorFlow version: \" + tf.__version__)\n",
    "\n",
    "# topology stuff\n",
    "from gtda.plotting import plot_point_cloud\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "from gtda.diagrams import PersistenceEntropy\n",
    "from gtda.diagrams import NumberOfPoints\n",
    "from gtda.diagrams import Amplitude\n",
    "from sklearn.pipeline import make_union, Pipeline\n",
    "\n",
    "# fixc this at some point\n",
    "sys.path.append(os.path.join(os.getcwd(), 'graphs_and_topology_for_chemistry'))\n",
    "#sys.path.append(r\"C:\\Users\\ella_\\Documents\\GitHub\\icosahedron_projection\")\n",
    "\n",
    "#import projection\n",
    "#from projection.molecule import Molecule\n",
    "#from projection.pdbmolecule import PDBMolecule\n",
    "#from projection.mol2molecule import Mol2Molecule\n",
    "\n",
    "import graphs_and_topology_for_chemistry.helper_functions as h\n",
    "#from projection.face import Face\n",
    "\n",
    "# $UN THIS\n",
    "data_dir= os.getcwd() \n",
    "results_dir=os.path.join(os.getcwd(), 'results')\n",
    "test_file='qm7.csv'\n",
    "data_file_name='qm7_topological_features.hdf5'\n",
    "make_dataset=False # whether to recalc the dataset\n",
    "\n",
    "\n",
    "print(f\"DeepChem version: {dc.__version__}\")\n",
    "\n",
    "############################### settings for all experiments #################\n",
    "\n",
    "num_repeats=10\n",
    "num_epochs = 500\n",
    "\n",
    "metric_labels=['mean_squared_error','pearson_r2_score',\n",
    "               'mae_score', 'rmse']\n",
    "\n",
    "\n",
    "metric1 = dc.metrics.Metric(dc.metrics.mean_squared_error)\n",
    "metric2 = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "metric3 = dc.metrics.Metric(dc.metrics.mae_score)\n",
    "metrics = [metric1, metric2, metric3]\n",
    "selected_metric = 2 #which metric to use for callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58284f0",
   "metadata": {},
   "source": [
    "#### This loads in the topological features (for qm7)\n",
    "And gives you some information about the file. Note that the molecule counting is broken!\n",
    "\n",
    "`fh` is the file handle for the topological features file, we use this to read data from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb83c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name='qm7_topological_features.hdf5'\n",
    "fh = h5py.File(os.path.join(data_dir,hdf5_file_name), 'r+')\n",
    "num_of_rows, num_of_molecules = h.basic_info_hdf5_dataset(fh, label='molID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa48f053",
   "metadata": {},
   "source": [
    "This gives you the keys to the database. The topological features are the ones in the `feature_name_list` (see below). The `PCA_x` are another set of features you could use (they result from a principal component analysis of the topological features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f437a",
   "metadata": {},
   "source": [
    "For this dataset, the target is `u0_atom` which is also in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ed76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh['u0_atom'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of topological features, this is what we learn from\n",
    "feature_name_list = ['pers_S_1', 'pers_S_2', 'pers_S_3',\n",
    "                    'no_p_1', 'no_p_2', 'no_p_3',\n",
    "                    'bottle_1', 'bottle_2', 'bottle_3',\n",
    "                    'wasser_1', 'wasser_2', 'wasser_3',\n",
    "                    'landsc_1', 'landsc_2', 'landsc_3',\n",
    "                    'pers_img_1', 'pers_img_2', 'pers_img_3']\n",
    "\n",
    "# alternative features\n",
    "PCA_list = ['PCA_1', 'PCA_2', 'PCA_3',\n",
    "           'PCA_4', 'PCA_5', 'PCA_6',\n",
    "           'PCA_7', 'PCA_8', 'PCA_9',\n",
    "           'PCA_10', 'PCA_11', 'PCA_12',\n",
    "           'PCA_13', 'PCA_14', 'PCA_15',\n",
    "           'PCA_16', 'PCA_17', 'PCA_18']\n",
    "\n",
    "# tasks to do\n",
    "tasks = ['u0_atom']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28688ff0",
   "metadata": {},
   "source": [
    "This code sets up our `X` and `y` data, creates the proper transformers  (normalisation for this dataset, use balancing transformers for the classification datasets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading data from the hdf5 file\n",
    "# takes data from the hdf5 file and puts it into a variable called X_data\n",
    "X_data = h.load_all_hdf5(\n",
    "    fh,\n",
    "    num_of_rows, \n",
    "    column_headers=feature_name_list)\n",
    "\n",
    "PCA_X_data = h.load_all_hdf5(\n",
    "    fh,\n",
    "    num_of_rows, \n",
    "    column_headers=PCA_list)\n",
    "\n",
    "y_data = h.load_all_hdf5(\n",
    "    fh,\n",
    "    num_of_rows, \n",
    "    column_headers=tasks)\n",
    "\n",
    "# makes a list of SMILES strings\n",
    "SMILES_list = np.array(fh['SMILES'])\n",
    "\n",
    "# this makes the actual topological datasets\n",
    "topol_dataset = dc.data.DiskDataset.from_numpy(\n",
    "    X_data, \n",
    "    y_data, \n",
    "    ids=SMILES_list)\n",
    "\n",
    "pca_dataset = dc.data.DiskDataset.from_numpy(\n",
    "    PCA_X_data, \n",
    "    y_data, \n",
    "    ids=SMILES_list)\n",
    "\n",
    "# setting up the splitters for the task\n",
    "Splitter_Object = dc.splits.SingletaskStratifiedSplitter()\n",
    "\n",
    "\n",
    "# doing a transform on the data to make it easier for hte NN\n",
    "# normalises both y and x\n",
    "## for use with topol_dataset\n",
    "transformers_tf = [\n",
    "    dc.trans.NormalizationTransformer(\n",
    "        transform_X=True, \n",
    "        dataset=topol_dataset),\n",
    "    dc.trans.NormalizationTransformer(\n",
    "        transform_y=True, \n",
    "        dataset=topol_dataset)]\n",
    "# normalises only y (as doing a PCA is a type of normalisation)\n",
    "# for use with pca_dataset\n",
    "transformers_pca = [\n",
    "    dc.trans.NormalizationTransformer(\n",
    "        transform_y=True, \n",
    "        dataset=pca_dataset)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579eac9",
   "metadata": {},
   "source": [
    "Plot of $y$ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14efd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_data)\n",
    "plt.xlabel('Atomisation energy')\n",
    "plt.ylabel(\"No.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b668a3d",
   "metadata": {},
   "source": [
    "### Now the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14d0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the splitter we're using, look at the DeepChem \n",
    "# paper to see which one you should use\n",
    "Splitter_Object = dc.splits.SingletaskStratifiedSplitter()\n",
    "# This shuffles the data and makes \n",
    "# the train, test and validate datasets from\n",
    "# the topologcal data\n",
    "train_dataset, valid_dataset, test_dataset = Splitter_Object.train_valid_test_split(\n",
    "    dataset=topol_dataset,\n",
    "    frac_train=0.8,\n",
    "    frac_valid=0.1,\n",
    "    frac_test=0.1)\n",
    "# this builds the datasets for training\n",
    "\n",
    "print(f\"Training with {len(train_dataset.y)} points\")\n",
    "print(f\"Validation with {len(valid_dataset.y)} points\")\n",
    "print(f\"Testing with {len(test_dataset.y)} points\")\n",
    "print(f\"Total dataset size: {len(train_dataset.y) + len(valid_dataset.y) + len(test_dataset.y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b541d13",
   "metadata": {},
   "source": [
    "Now we must do the transform on our data. If we were using the standard deepchem datasets it is set up to pick the correct transformer and do it for us. With the topological data, we have to to the transforms ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58bf459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms datasets wooo\n",
    "train_dataset = h.do_transform(transformers_tf, train_dataset)\n",
    "valid_dataset = h.do_transform(transformers_tf, valid_dataset)\n",
    "test_dataset = h.do_transform(transformers_tf, test_dataset)\n",
    "datasets = [train_dataset, valid_dataset, test_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15623292",
   "metadata": {},
   "source": [
    "Now we build the model with the validation callback. This is a standard model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 15 # try for 15 more epochs if you think it's trained\n",
    "\n",
    "### actual model here\n",
    "model = dc.models.MultitaskRegressor(\n",
    "    n_tasks=len(tasks), # size of y\n",
    "    n_features=len(train_dataset.X[3]), # size of input to model\n",
    "    # layer_sizes=[1000,1000,500,20],\n",
    "    dropouts=0.2,\n",
    "    #learning_rate=0.001,\n",
    "    residual=True)\n",
    "\n",
    "## this does the early stopping ##\n",
    "callback = dc.models.ValidationCallback(\n",
    "    valid_dataset, # which dataset to use for valdiation\n",
    "    patience, # how long to wait if the program thinks the model is trained\n",
    "    metrics[selected_metric]) # metric to use to do the validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387b61e",
   "metadata": {},
   "source": [
    "Now we fit the model... (and time how long it takes) (the other number it returns is the loss of the trained model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac593971",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# fit da model\n",
    "model.fit(train_dataset, nb_epoch=num_epochs, callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a8eb7",
   "metadata": {},
   "source": [
    "Now we get the errors on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85024567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# little function to calc metrics on this data\n",
    "out=h.get_them_metrics(\n",
    "    model,\n",
    "    datasets,\n",
    "    metrics,\n",
    "    metric_labels,\n",
    "    transformers=transformers_tf)\n",
    "\n",
    "pd_out = pd.DataFrame([out], \n",
    "        columns=['tr_mse', 'tr_r2', 'tr_mae', 'tr_rmse', # training metrics\n",
    "        'val_mse', 'val_r2', 'val_mae', 'val_rmse', # valdiation metrics\n",
    "        'te_mse', 'te_r2', 'te_mae', 'te_rmse']) # testing metrics\n",
    "pd_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc3d53",
   "metadata": {},
   "source": [
    "You probably want to put all this code in a loop so you can do repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f7a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
