{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d4bdf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.7.0\n",
      "DeepChem version: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "#sys.path.append(r\"C:\\Users\\ella_\\Documents\\GitHub\\graphs_and_topology_for_chemistry\")\n",
    "#sys.path.append(r\"C:\\Users\\ella_\\Documents\\GitHub\\icosahedron_projection\")\n",
    "\n",
    "import deepchem as dc\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import rdkit\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri\n",
    "import rdkit.Chem as Chem\n",
    "import rdkit.Chem.AllChem as AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import mpl_toolkits.mplot3d\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "\n",
    "print(\"TensorFlow version: \" + tf.__version__)\n",
    "\n",
    "# topology stuff\n",
    "from gtda.plotting import plot_point_cloud\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "from gtda.diagrams import PersistenceEntropy\n",
    "from gtda.diagrams import NumberOfPoints\n",
    "from gtda.diagrams import Amplitude\n",
    "from sklearn.pipeline import make_union, Pipeline\n",
    "\n",
    "# fixc this at some point\n",
    "sys.path.append(os.path.join(os.getcwd(), 'graphs_and_topology_for_chemistry'))\n",
    "#sys.path.append(r\"C:\\Users\\ella_\\Documents\\GitHub\\icosahedron_projection\")\n",
    "\n",
    "#import projection\n",
    "#from projection.molecule import Molecule\n",
    "#from projection.pdbmolecule import PDBMolecule\n",
    "#from projection.mol2molecule import Mol2Molecule\n",
    "\n",
    "import graphs_and_topology_for_chemistry.helper_functions as h\n",
    "#from projection.face import Face\n",
    "\n",
    "# $UN THIS\n",
    "data_dir= os.getcwd() \n",
    "results_dir=os.path.join(os.getcwd(), 'results')\n",
    "test_file='qm7.csv'\n",
    "data_file_name='qm7_topological_features.hdf5'\n",
    "make_dataset=False # whether to recalc the dataset\n",
    "\n",
    "\n",
    "print(f\"DeepChem version: {dc.__version__}\")\n",
    "\n",
    "############################### settings for all experiments #################\n",
    "\n",
    "num_repeats=10\n",
    "num_epochs = 500\n",
    "\n",
    "metric_labels=['mean_squared_error','pearson_r2_score',\n",
    "               'mae_score', 'rmse']\n",
    "\n",
    "\n",
    "metric1 = dc.metrics.Metric(dc.metrics.mean_squared_error)\n",
    "metric2 = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "metric3 = dc.metrics.Metric(dc.metrics.mae_score)\n",
    "metrics = [metric1, metric2, metric3]\n",
    "selected_metric = 2 #which metric to use for callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "573a7483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_ rows is:\t6838\n",
      "num_of_molecules is:\t 128\n",
      "Warning: Unbalanced dataset\n",
      "MolID: count\n",
      "Counter({127: 6711, 0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 1, 36: 1, 37: 1, 38: 1, 39: 1, 40: 1, 41: 1, 42: 1, 43: 1, 44: 1, 45: 1, 46: 1, 47: 1, 48: 1, 49: 1, 50: 1, 51: 1, 52: 1, 53: 1, 54: 1, 55: 1, 56: 1, 57: 1, 58: 1, 59: 1, 60: 1, 61: 1, 62: 1, 63: 1, 64: 1, 65: 1, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1, 76: 1, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 83: 1, 84: 1, 85: 1, 86: 1, 87: 1, 88: 1, 89: 1, 90: 1, 91: 1, 92: 1, 93: 1, 94: 1, 95: 1, 96: 1, 97: 1, 98: 1, 99: 1, 100: 1, 101: 1, 102: 1, 103: 1, 104: 1, 105: 1, 106: 1, 107: 1, 108: 1, 109: 1, 110: 1, 111: 1, 112: 1, 113: 1, 114: 1, 115: 1, 116: 1, 117: 1, 118: 1, 119: 1, 120: 1, 121: 1, 122: 1, 123: 1, 124: 1, 125: 1, 126: 1})\n"
     ]
    }
   ],
   "source": [
    "hdf5_file_name='qm7_topological_features.hdf5'\n",
    "fh = h5py.File(os.path.join(data_dir,hdf5_file_name), 'r+')\n",
    "num_of_rows, num_of_molecules = h.basic_info_hdf5_dataset(fh, label='molID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19ed52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of topological features, this is what we learn from\n",
    "feature_name_list = ['pers_S_1', 'pers_S_2', 'pers_S_3',\n",
    "                    'no_p_1', 'no_p_2', 'no_p_3',\n",
    "                    'bottle_1', 'bottle_2', 'bottle_3',\n",
    "                    'wasser_1', 'wasser_2', 'wasser_3',\n",
    "                    'landsc_1', 'landsc_2', 'landsc_3',\n",
    "                    'pers_img_1', 'pers_img_2', 'pers_img_3']\n",
    "\n",
    "# alternative features\n",
    "PCA_list = ['PCA_1', 'PCA_2', 'PCA_3',\n",
    "           'PCA_4', 'PCA_5', 'PCA_6',\n",
    "           'PCA_7', 'PCA_8', 'PCA_9',\n",
    "           'PCA_10', 'PCA_11', 'PCA_12',\n",
    "           'PCA_13', 'PCA_14', 'PCA_15',\n",
    "           'PCA_16', 'PCA_17', 'PCA_18']\n",
    "\n",
    "# tasks to do\n",
    "tasks = ['u0_atom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7ecd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading data from the hdf5 file\n",
    "# takes data from the hdf5 file and puts it into a variable called X_data\n",
    "X_data = h.load_all_hdf5(\n",
    "    fh,\n",
    "    num_of_rows, \n",
    "    column_headers=feature_name_list)\n",
    "\n",
    "PCA_X_data = h.load_all_hdf5(\n",
    "    fh,\n",
    "    num_of_rows, \n",
    "    column_headers=PCA_list)\n",
    "\n",
    "y_data = h.load_all_hdf5(\n",
    "    fh,\n",
    "    num_of_rows, \n",
    "    column_headers=tasks)\n",
    "\n",
    "# makes a list of SMILES strings\n",
    "SMILES_list = np.array(fh['SMILES'])\n",
    "\n",
    "# this makes the actual topological datasets\n",
    "topol_dataset = dc.data.DiskDataset.from_numpy(\n",
    "    X_data, \n",
    "    y_data, \n",
    "    ids=SMILES_list)\n",
    "\n",
    "pca_dataset = dc.data.DiskDataset.from_numpy(\n",
    "    PCA_X_data, \n",
    "    y_data, \n",
    "    ids=SMILES_list)\n",
    "\n",
    "# setting up the splitters for the task\n",
    "Splitter_Object = dc.splits.SingletaskStratifiedSplitter()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c842f54b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'No.')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_data)\n",
    "plt.xlabel('Atomisation energy')\n",
    "plt.ylabel(\"No.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5f0044d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the data without shuffling or splitting\n",
    "tasks, datasets, transformers = dc.molnet.load_qm7(\n",
    "    shard_size=2000,\n",
    "    featurizer=dc.feat.MACCSKeysFingerprint(),\n",
    "    #featurizer=h.My_Dummy_Featurizer(None),\n",
    "    splitter=None)\n",
    "\n",
    "# This loads the data without shuffling or splitting\n",
    "#tasks, datasets, transformers = dc.molnet.load_qm7(\n",
    "#    shard_size=2000,\n",
    "#    featurizer=dc.feat.CoulombMatrix\n",
    "#    (max_atoms=23),\n",
    "#    #featurizer=h.My_Dummy_Featurizer(None),\n",
    "#    splitter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4fbc0406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].X[3].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da6cb08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'[H].[H].[H][C][H]', b'[H]C([H])([H])C([H])([H])[H]',\n",
       "       b'[H]C([H])=C([H])[H]', ...,\n",
       "       b'[H]/C(=C(\\\\C#N)C([H])([H])C([H])([H])[H])C([H])([H])[H]',\n",
       "       b'[H]C([H])=C1C([H])([H])C([H])=C([H])[C@]1([H])C([H])([H])[H]',\n",
       "       b'[H]C1([H])C([H])([H])C([H])([H])C2(C1([H])[H])C([H])([H])C2([H])[H]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f9d0d140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[H].[H].[H][C][H]', '[H]C([H])([H])C([H])([H])[H]',\n",
       "       '[H]C([H])=C([H])[H]', ...,\n",
       "       '[H]/C(=C(\\\\C#N)C([H])([H])C([H])([H])[H])C([H])([H])[H]',\n",
       "       '[H]C([H])=C1C([H])([H])C([H])=C([H])[C@]1([H])C([H])([H])[H]',\n",
       "       '[H]C1([H])C([H])([H])C([H])([H])C2(C1([H])[H])C([H])([H])C2([H])[H]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e0b59c40",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'new_y_data' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16244/1788493091.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mdataset_2_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0msmiles_list_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0msmiles_list_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m combined_dataset = dc.data.DiskDataset.from_numpy(\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16244/1788493091.py\u001b[0m in \u001b[0;36mjoin_featurisation_vectors\u001b[1;34m(dataset_1, dataset_2, dataset_2_y, smiles_list_1, smiles_list_2)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_X_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_y_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_smiles_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m new_X_data,new_y_data,new_smiles_list = join_featurisation_vectors(\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'new_y_data' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# deepchem features\n",
    "dataset_1 = datasets[0].X\n",
    "smiles_list_1=datasets[0].ids\n",
    "\n",
    "# tdaf features\n",
    "dataset_2 = X_data\n",
    "dataset_2_y = y_data\n",
    "smiles_list_2=SMILES_list\n",
    "\n",
    "\n",
    "def join_featurisation_vectors(\n",
    "    dataset_1,\n",
    "    dataset_2,\n",
    "    dataset_2_y,\n",
    "    smiles_list_1=[],\n",
    "    smiles_list_2=[]):\n",
    "    \"\"\"sticks together two featurisation vectors\n",
    "    dataset_1 = lefthand dataset\n",
    "    dataset_2 = righthand_dataset\n",
    "    smiles_list_1 = lefthand dataset\n",
    "    smiles_list_2 = righthand dataset\n",
    "    N.B. expects datasets to a list of lists or 2D nparray \n",
    "    i.e. x samples by y features\n",
    " \"\"\"\n",
    "    new_X_data = []\n",
    "    if len(dataset_2) == len(dataset_1):\n",
    "        #  datasets have the same number\n",
    "        for i in range(len(dataset_2)):\n",
    "            # this grabs the deepchem features and copies it\n",
    "            # remove the flatten\n",
    "            deepchem_feature = [float(x) for x in dataset_1[i]]\n",
    "    #        deepchem_feature = [float(x) for x in dataset_1[i]]\n",
    "            # this grabs the tdaf and copies it\n",
    "            tdaf_feature = [float(x) for x in dataset_2[i]]\n",
    "            # this sticks them together\n",
    "            new_X_data.append(np.array(deepchem_feature + tdaf_feature))\n",
    "    else:\n",
    "        # doing smiles comparison\n",
    "        print('Datasets are different sizes')\n",
    "        print('Using smiles strings to sort this out')\n",
    "        # frist check change type from bytes\n",
    "        if type(smiles_list_1[3]) == bytes:\n",
    "            smiles_list_1 = [i.decode() for i in smiles_list_1]\n",
    "        if type(smiles_list_2[3]) == bytes:\n",
    "            smiles_list_2 = [i.decode() for i in smiles_list_2]  \n",
    "            \n",
    "        print(smiles_list_1[3])\n",
    "        print(smiles_list_2[3])\n",
    "        print(smiles_list_1[3]==smiles_list_2[3])\n",
    "        new_X_data = []\n",
    "        new_y_data = []\n",
    "        new_smiles_list=[]\n",
    "        for i in range(len(dataset_2)):\n",
    "            # this grabs the deepchem features and copies it\n",
    "            # remove the flatten\n",
    "            for i_1 in range(len(dataset_1)):\n",
    "                    if smiles_list_1[i_1]==smiles_list_2[i]:\n",
    "                        #i_new=smiles_list_1.index(smiles_list_2[i])\n",
    "                        #print(smiles_list_2[i])\n",
    "                        #print(smiles_list_1[i_1])\n",
    "                        deepchem_feature = [float(x) for x in dataset_1[i_1]]\n",
    "                        tdaf_feature = [float(x) for x in dataset_2[i]]\n",
    "                        new_X_data.append(np.array(deepchem_feature + tdaf_feature))  \n",
    "                        new_y_data.append(np.array(dataset_2_y[i]))\n",
    "                        new_smiles_list.append(smiles_list_2[i])\n",
    "              \n",
    "    \n",
    "    return new_X_data, new_y_data, new_smiles_list\n",
    "\n",
    "new_X_data,new_y_data,new_smiles_list = join_featurisation_vectors(\n",
    "    dataset_1,\n",
    "    dataset_2,\n",
    "    dataset_2_y,\n",
    "    smiles_list_1,\n",
    "    smiles_list_2\n",
    "    )\n",
    "combined_dataset = dc.data.DiskDataset.from_numpy(\n",
    "    new_X_data, \n",
    "    new_y_data, \n",
    "    ids=new_smiles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2d0b7435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ella_\\.conda\\envs\\tdaf-tf2p7h2\\lib\\site-packages\\numpy\\lib\\format.py:363: UserWarning: metadata on a dtype may be saved or ignored, but will raise if saved when read. Use another form of storage.\n",
      "  d['descr'] = dtype_to_descr(array.dtype)\n"
     ]
    }
   ],
   "source": [
    "# this makes the actual topological datasets\n",
    "combined_dataset = dc.data.DiskDataset.from_numpy(\n",
    "    new_X_data, \n",
    "    y_data, \n",
    "    ids=SMILES_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2ed8dcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1124a53a408>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(topol_dataset.y, datasets[0].y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a86dd6",
   "metadata": {},
   "source": [
    "# fixing bbbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "de55528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_ rows is:\t2036\n",
      "num_of_molecules is:\t 2036\n",
      "num_of_augments is:\t1\n"
     ]
    }
   ],
   "source": [
    "# $UN THIS\n",
    "data_dir= os.getcwd() \n",
    "results_dir=os.path.join(os.getcwd(), 'results')\n",
    "test_file='bbbp.csv'\n",
    "data_file_name='bbbp_topological_features.hdf5'\n",
    "make_dataset=False # whether to recalc the dataset\n",
    "\n",
    "hdf5_file_name='bbbp_topological_features.hdf5'\n",
    "fh = h5py.File(os.path.join(data_dir,hdf5_file_name), 'r+')\n",
    "num_of_rows, num_of_molecules = h.basic_info_hdf5_dataset(fh, label='molID')\n",
    "\n",
    "# list of topological features, this is what we learn from\n",
    "feature_name_list = ['pers_S_1', 'pers_S_2', 'pers_S_3',\n",
    "                    'no_p_1', 'no_p_2', 'no_p_3',\n",
    "                    'bottle_1', 'bottle_2', 'bottle_3',\n",
    "                    'wasser_1', 'wasser_2', 'wasser_3',\n",
    "                    'landsc_1', 'landsc_2', 'landsc_3',\n",
    "                    'pers_img_1', 'pers_img_2', 'pers_img_3']\n",
    "\n",
    "# alternative features\n",
    "PCA_list = ['PCA_1', 'PCA_2', 'PCA_3',\n",
    "           'PCA_4', 'PCA_5', 'PCA_6',\n",
    "           'PCA_7', 'PCA_8', 'PCA_9',\n",
    "           'PCA_10', 'PCA_11', 'PCA_12',\n",
    "           'PCA_13', 'PCA_14', 'PCA_15',\n",
    "           'PCA_16', 'PCA_17', 'PCA_18']\n",
    "\n",
    "# tasks to do\n",
    "tasks = ['p_np']\n",
    "\n",
    "## loading data from the hdf5 file\n",
    "# takes data from the hdf5 file and puts it into a variable called X_data\n",
    "X_data = h.load_all_hdf5(\n",
    "    fh,\n",
    "    num_of_rows, \n",
    "    column_headers=feature_name_list)\n",
    "\n",
    "PCA_X_data = h.load_all_hdf5(\n",
    "    fh,\n",
    "    num_of_rows, \n",
    "    column_headers=PCA_list)\n",
    "\n",
    "y_data = h.load_all_hdf5(\n",
    "    fh,\n",
    "    num_of_rows, \n",
    "    column_headers=tasks)\n",
    "\n",
    "# makes a list of SMILES strings\n",
    "SMILES_list = np.array(fh['SMILES'])\n",
    "\n",
    "# this makes the actual topological datasets\n",
    "topol_dataset = dc.data.DiskDataset.from_numpy(\n",
    "    X_data, \n",
    "    y_data, \n",
    "    ids=SMILES_list)\n",
    "\n",
    "pca_dataset = dc.data.DiskDataset.from_numpy(\n",
    "    PCA_X_data, \n",
    "    y_data, \n",
    "    ids=SMILES_list)\n",
    "\n",
    "\n",
    "# This loads the data without shuffling or splitting\n",
    "tasks, datasets, transformers = dc.molnet.load_bbbp(\n",
    "    shard_size=2000,\n",
    "    featurizer=dc.feat.MACCSKeysFingerprint(),\n",
    "    #featurizer=h.My_Dummy_Featurizer(None),\n",
    "    splitter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "10e7411d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 1624 points\n",
      "Validation with 203 points\n",
      "Testing with 203 points\n",
      "Total dataset size: 2030\n"
     ]
    }
   ],
   "source": [
    "# This is the splitter we're using, look at the DeepChem \n",
    "# paper to see which one you should use\n",
    "Splitter_Object = dc.splits.SingletaskStratifiedSplitter()\n",
    "# This shuffles the data and makes \n",
    "# the train, test and validate datasets from\n",
    "# the topologcal data\n",
    "train_dataset, valid_dataset, test_dataset = Splitter_Object.train_valid_test_split(\n",
    "    dataset=topol_dataset,\n",
    "    frac_train=0.8,\n",
    "    frac_valid=0.1,\n",
    "    frac_test=0.1)\n",
    "# this builds the datasets for training\n",
    "\n",
    "print(f\"Training with {len(train_dataset.y)} points\")\n",
    "print(f\"Validation with {len(valid_dataset.y)} points\")\n",
    "print(f\"Testing with {len(test_dataset.y)} points\")\n",
    "print(f\"Total dataset size: {len(train_dataset.y) + len(valid_dataset.y) + len(test_dataset.y)}\")\n",
    "\n",
    "# transforms datasets wooo\n",
    "#train_dataset = h.do_transform(transformers_tf, train_dataset)\n",
    "#valid_dataset = h.do_transform(transformers_tf, valid_dataset)\n",
    "#test_dataset = h.do_transform(transformers_tf, test_dataset)\n",
    "datasets = [train_dataset, valid_dataset, test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cc4b53e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are different sizes\n",
      "Using smiles strings to sort this out\n",
      "C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C\n",
      "C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# deepchem features\n",
    "dataset_1 = datasets[0].X\n",
    "smiles_list_1=datasets[0].ids\n",
    "\n",
    "# tdaf features\n",
    "dataset_2 = X_data\n",
    "dataset_2_y = y_data\n",
    "smiles_list_2=SMILES_list\n",
    "\n",
    "\n",
    "def join_featurisation_vectors(\n",
    "    dataset_1,\n",
    "    dataset_2,\n",
    "    dataset_2_y,\n",
    "    smiles_list_1=[],\n",
    "    smiles_list_2=[]):\n",
    "    \"\"\"sticks together two featurisation vectors\n",
    "    dataset_1 = lefthand dataset\n",
    "    dataset_2 = righthand_dataset\n",
    "    smiles_list_1 = lefthand dataset\n",
    "    smiles_list_2 = righthand dataset\n",
    "    N.B. expects datasets to a list of lists or 2D nparray \n",
    "    i.e. x samples by y features\n",
    " \"\"\"\n",
    "    new_X_data = []\n",
    "    if len(dataset_2) == len(dataset_1):\n",
    "        #  datasets have the same number\n",
    "        for i in range(len(dataset_2)):\n",
    "            # this grabs the deepchem features and copies it\n",
    "            # remove the flatten\n",
    "            deepchem_feature = [float(x) for x in dataset_1[i]]\n",
    "    #        deepchem_feature = [float(x) for x in dataset_1[i]]\n",
    "            # this grabs the tdaf and copies it\n",
    "            tdaf_feature = [float(x) for x in dataset_2[i]]\n",
    "            # this sticks them together\n",
    "            new_X_data.append(np.array(deepchem_feature + tdaf_feature))\n",
    "    else:\n",
    "        # doing smiles comparison\n",
    "        print('Datasets are different sizes')\n",
    "        print('Using smiles strings to sort this out')\n",
    "        # frist check change type from bytes\n",
    "        if type(smiles_list_1[3]) == bytes:\n",
    "            smiles_list_1 = [i.decode() for i in smiles_list_1]\n",
    "        if type(smiles_list_2[3]) == bytes:\n",
    "            smiles_list_2 = [i.decode() for i in smiles_list_2]  \n",
    "            \n",
    "        print(smiles_list_1[3])\n",
    "        print(smiles_list_2[3])\n",
    "        print(smiles_list_1[3]==smiles_list_2[3])\n",
    "        new_X_data = []\n",
    "        new_y_data = []\n",
    "        new_smiles_list=[]\n",
    "        for i in range(len(dataset_2)):\n",
    "            # this grabs the deepchem features and copies it\n",
    "            # remove the flatten\n",
    "            for i_1 in range(len(dataset_1)):\n",
    "                    if smiles_list_1[i_1]==smiles_list_2[i]:\n",
    "                        #i_new=smiles_list_1.index(smiles_list_2[i])\n",
    "                        #print(smiles_list_2[i])\n",
    "                        #print(smiles_list_1[i_1])\n",
    "                        deepchem_feature = [float(x) for x in dataset_1[i_1]]\n",
    "                        tdaf_feature = [float(x) for x in dataset_2[i]]\n",
    "                        new_X_data.append(np.array(deepchem_feature + tdaf_feature))  \n",
    "                        new_y_data.append(dataset_2_y[i])\n",
    "                        new_smiles_list.append(smiles_list_2[i])\n",
    "              \n",
    "    \n",
    "    return new_X_data, new_y_data, new_smiles_list\n",
    "\n",
    "new_X_data,new_y_data,new_smiles_list = join_featurisation_vectors(\n",
    "    dataset_1,\n",
    "    dataset_2,\n",
    "    dataset_2_y,\n",
    "    smiles_list_1,\n",
    "    smiles_list_2\n",
    "    )\n",
    "combined_dataset = dc.data.DiskDataset.from_numpy(\n",
    "    new_X_data, \n",
    "    new_y_data, \n",
    "    ids=new_smiles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2df1a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = dc.data.DiskDataset.from_numpy(\n",
    "    new_X_data, \n",
    "    new_y_data, \n",
    "    ids=new_smiles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0bf7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset=joint_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9d650dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing a transform on the data to make it easier for hte NN\n",
    "# normalises both y and x\n",
    "## for use with topol_dataset\n",
    "transformers_tf = [\n",
    "    dc.trans.NormalizationTransformer(\n",
    "        transform_X=True, \n",
    "        dataset=combined_dataset),\n",
    "    dc.trans.NormalizationTransformer(\n",
    "        transform_y=True, \n",
    "        dataset=combined_dataset)]\n",
    "# normalises only y (as doing a PCA is a type of normalisation)\n",
    "# for use with pca_dataset\n",
    "transformers_pca = [\n",
    "    dc.trans.NormalizationTransformer(\n",
    "        transform_y=True, \n",
    "        dataset=pca_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a07064ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 1624 points\n",
      "Validation with 203 points\n",
      "Testing with 203 points\n",
      "Total dataset size: 2030\n"
     ]
    }
   ],
   "source": [
    "# This is the splitter we're using, look at the DeepChem \n",
    "# paper to see which one you should use\n",
    "Splitter_Object = dc.splits.SingletaskStratifiedSplitter()\n",
    "# This shuffles the data and makes \n",
    "# the train, test and validate datasets from\n",
    "# the topologcal data\n",
    "train_dataset, valid_dataset, test_dataset = Splitter_Object.train_valid_test_split(\n",
    "    dataset=combined_dataset,\n",
    "    frac_train=0.8,\n",
    "    frac_valid=0.1,\n",
    "    frac_test=0.1)\n",
    "# this builds the datasets for training\n",
    "\n",
    "print(f\"Training with {len(train_dataset.y)} points\")\n",
    "print(f\"Validation with {len(valid_dataset.y)} points\")\n",
    "print(f\"Testing with {len(test_dataset.y)} points\")\n",
    "print(f\"Total dataset size: {len(train_dataset.y) + len(valid_dataset.y) + len(test_dataset.y)}\")\n",
    "\n",
    "# transforms datasets wooo\n",
    "#train_dataset = h.do_transform(transformers_tf, train_dataset)\n",
    "#valid_dataset = h.do_transform(transformers_tf, valid_dataset)\n",
    "#test_dataset = h.do_transform(transformers_tf, test_dataset)\n",
    "datasets = [train_dataset, valid_dataset, test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "18d3c6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " ...]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ac68f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 3 # try for 15 more epochs if you think it's trained\n",
    "\n",
    "### actual model here\n",
    "model = dc.models.MultitaskRegressor(\n",
    "    n_tasks=len(tasks), # size of y\n",
    "    n_features=len(train_dataset.X[3]), # size of input to model\n",
    "    layer_sizes=[1000,500],\n",
    "    dropouts=0.2,\n",
    "    #learning_rate=0.001,\n",
    "    residual=True)\n",
    "\n",
    "## this does the early stopping ##\n",
    "callback = dc.models.ValidationCallback(\n",
    "    valid_dataset, # which dataset to use for valdiation\n",
    "    patience, # how long to wait if the program thinks the model is trained\n",
    "    metrics[selected_metric]) # metric to use to do the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d99e4b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 4337 calls to <function KerasModel._compute_model at 0x000001F1D79CD828> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Step 3 validation: mae_score=1.62057\n",
      "Step 6 validation: mae_score=2.55766\n",
      "Step 9 validation: mae_score=0.494902\n",
      "Step 12 validation: mae_score=0.722176\n",
      "Step 15 validation: mae_score=0.85705\n",
      "Step 18 validation: mae_score=0.70874\n",
      "Step 21 validation: mae_score=0.445681\n",
      "Step 24 validation: mae_score=0.268769\n",
      "Step 27 validation: mae_score=0.361329\n",
      "Step 30 validation: mae_score=0.403385\n",
      "Step 33 validation: mae_score=0.417206\n",
      "Step 36 validation: mae_score=0.416284\n",
      "Step 39 validation: mae_score=0.406784\n",
      "Step 42 validation: mae_score=0.393394\n",
      "Step 45 validation: mae_score=0.383966\n",
      "Step 48 validation: mae_score=0.380492\n",
      "Step 51 validation: mae_score=0.374728\n",
      "Step 54 validation: mae_score=0.368337\n",
      "Step 57 validation: mae_score=0.362612\n",
      "Step 60 validation: mae_score=0.356372\n",
      "Step 63 validation: mae_score=0.351968\n",
      "Step 66 validation: mae_score=0.34859\n",
      "Step 69 validation: mae_score=0.337949\n",
      "Step 72 validation: mae_score=0.331614\n",
      "Step 75 validation: mae_score=0.332336\n",
      "Step 78 validation: mae_score=0.336813\n",
      "Step 81 validation: mae_score=0.341379\n",
      "Step 84 validation: mae_score=0.34236\n",
      "Step 87 validation: mae_score=0.345475\n",
      "Step 90 validation: mae_score=0.347447\n",
      "Step 93 validation: mae_score=0.348666\n",
      "Step 96 validation: mae_score=0.351533\n",
      "Step 99 validation: mae_score=0.353296\n",
      "Step 102 validation: mae_score=0.355078\n",
      "Step 105 validation: mae_score=0.359772\n",
      "Step 108 validation: mae_score=0.354364\n",
      "Step 111 validation: mae_score=0.352838\n",
      "Step 114 validation: mae_score=0.348241\n",
      "Step 117 validation: mae_score=0.34436\n",
      "Step 120 validation: mae_score=0.34205\n",
      "Step 123 validation: mae_score=0.344372\n",
      "Step 126 validation: mae_score=0.358058\n",
      "Step 129 validation: mae_score=0.350959\n",
      "Step 132 validation: mae_score=0.332719\n",
      "Step 135 validation: mae_score=0.327083\n",
      "Step 138 validation: mae_score=0.330427\n",
      "Step 141 validation: mae_score=0.339687\n",
      "Step 144 validation: mae_score=0.340837\n",
      "Step 147 validation: mae_score=0.340646\n",
      "Step 150 validation: mae_score=0.335802\n",
      "Step 153 validation: mae_score=0.322229\n",
      "Step 156 validation: mae_score=0.323887\n",
      "Step 159 validation: mae_score=0.332035\n",
      "Step 162 validation: mae_score=0.31739\n",
      "Step 165 validation: mae_score=0.304715\n",
      "Step 168 validation: mae_score=0.308299\n",
      "Step 171 validation: mae_score=0.319891\n",
      "Step 174 validation: mae_score=0.295986\n",
      "Step 177 validation: mae_score=0.295658\n",
      "Step 180 validation: mae_score=0.300964\n",
      "Step 183 validation: mae_score=0.303584\n",
      "Step 186 validation: mae_score=0.303119\n",
      "Step 189 validation: mae_score=0.297747\n",
      "Step 192 validation: mae_score=0.293633\n",
      "Step 195 validation: mae_score=0.293881\n",
      "Step 198 validation: mae_score=0.288027\n",
      "Step 201 validation: mae_score=0.291298\n",
      "Step 204 validation: mae_score=0.289688\n",
      "Step 207 validation: mae_score=0.293972\n",
      "Step 210 validation: mae_score=0.29703\n",
      "Step 213 validation: mae_score=0.305404\n",
      "Step 216 validation: mae_score=0.298455\n",
      "Step 219 validation: mae_score=0.279567\n",
      "Step 222 validation: mae_score=0.271372\n",
      "Step 225 validation: mae_score=0.272542\n",
      "Step 228 validation: mae_score=0.284285\n",
      "Step 231 validation: mae_score=0.284573\n",
      "Step 234 validation: mae_score=0.277638\n",
      "Step 237 validation: mae_score=0.274041\n",
      "Step 240 validation: mae_score=0.279732\n",
      "Step 243 validation: mae_score=0.283178\n",
      "Step 246 validation: mae_score=0.290772\n",
      "Step 249 validation: mae_score=0.296237\n",
      "Step 252 validation: mae_score=0.301191\n",
      "Step 255 validation: mae_score=0.309918\n",
      "Step 258 validation: mae_score=0.278234\n",
      "Step 261 validation: mae_score=0.268832\n",
      "Step 264 validation: mae_score=0.268878\n",
      "Step 267 validation: mae_score=0.281034\n",
      "Step 270 validation: mae_score=0.284519\n",
      "Step 273 validation: mae_score=0.281962\n",
      "Step 276 validation: mae_score=0.286815\n",
      "Step 279 validation: mae_score=0.289227\n",
      "Step 282 validation: mae_score=0.283919\n",
      "Step 285 validation: mae_score=0.280884\n",
      "Step 288 validation: mae_score=0.284345\n",
      "Step 291 validation: mae_score=0.286694\n",
      "Step 294 validation: mae_score=0.272584\n",
      "Step 297 validation: mae_score=0.270723\n",
      "Step 300 validation: mae_score=0.265322\n",
      "Step 303 validation: mae_score=0.267157\n",
      "Step 306 validation: mae_score=0.289486\n",
      "Step 309 validation: mae_score=0.269659\n",
      "Step 312 validation: mae_score=0.267823\n",
      "Step 315 validation: mae_score=0.281185\n",
      "Step 318 validation: mae_score=0.290048\n",
      "Step 321 validation: mae_score=0.277348\n",
      "Step 324 validation: mae_score=0.275495\n",
      "Step 327 validation: mae_score=0.271054\n",
      "Step 330 validation: mae_score=0.263052\n",
      "Step 333 validation: mae_score=0.2605\n",
      "Step 336 validation: mae_score=0.272687\n",
      "Step 339 validation: mae_score=0.277172\n",
      "Step 342 validation: mae_score=0.262268\n",
      "Step 345 validation: mae_score=0.259676\n",
      "Step 348 validation: mae_score=0.270799\n",
      "Step 351 validation: mae_score=0.275727\n",
      "Step 354 validation: mae_score=0.276536\n",
      "Step 357 validation: mae_score=0.270486\n",
      "Step 360 validation: mae_score=0.258711\n",
      "Step 363 validation: mae_score=0.253614\n",
      "Step 366 validation: mae_score=0.271535\n",
      "Step 369 validation: mae_score=0.289278\n",
      "Step 372 validation: mae_score=0.269906\n",
      "Step 375 validation: mae_score=0.241489\n",
      "Step 378 validation: mae_score=0.250298\n",
      "Step 381 validation: mae_score=0.286767\n",
      "Step 384 validation: mae_score=0.26695\n",
      "Step 387 validation: mae_score=0.25434\n",
      "Step 390 validation: mae_score=0.257485\n",
      "Step 393 validation: mae_score=0.267546\n",
      "Step 396 validation: mae_score=0.28662\n",
      "Step 399 validation: mae_score=0.27831\n",
      "Step 402 validation: mae_score=0.275314\n",
      "Step 405 validation: mae_score=0.276183\n",
      "Step 408 validation: mae_score=0.281677\n",
      "Step 411 validation: mae_score=0.283431\n",
      "Step 414 validation: mae_score=0.276947\n",
      "Step 417 validation: mae_score=0.25685\n",
      "Step 420 validation: mae_score=0.252246\n",
      "Step 423 validation: mae_score=0.250534\n",
      "Step 426 validation: mae_score=0.254791\n",
      "Step 429 validation: mae_score=0.267923\n",
      "Step 432 validation: mae_score=0.275548\n",
      "Step 435 validation: mae_score=0.26209\n",
      "Step 438 validation: mae_score=0.260906\n",
      "Step 441 validation: mae_score=0.261296\n",
      "Step 444 validation: mae_score=0.267864\n",
      "Step 447 validation: mae_score=0.276381\n",
      "Step 450 validation: mae_score=0.256374\n",
      "Step 453 validation: mae_score=0.255541\n",
      "Step 456 validation: mae_score=0.2601\n",
      "Step 459 validation: mae_score=0.275485\n",
      "Step 462 validation: mae_score=0.272454\n",
      "Step 465 validation: mae_score=0.258047\n",
      "Step 468 validation: mae_score=0.266099\n",
      "Step 471 validation: mae_score=0.285705\n",
      "Step 474 validation: mae_score=0.264205\n",
      "Step 477 validation: mae_score=0.252188\n",
      "Step 480 validation: mae_score=0.252227\n",
      "Step 483 validation: mae_score=0.269682\n",
      "Step 486 validation: mae_score=0.259841\n",
      "Step 489 validation: mae_score=0.244745\n",
      "Step 492 validation: mae_score=0.251558\n",
      "Step 495 validation: mae_score=0.26503\n",
      "Step 498 validation: mae_score=0.26499\n",
      "Step 501 validation: mae_score=0.28233\n",
      "Step 504 validation: mae_score=0.272354\n",
      "Step 507 validation: mae_score=0.247284\n",
      "Step 510 validation: mae_score=0.239955\n",
      "Step 513 validation: mae_score=0.242762\n",
      "Step 516 validation: mae_score=0.256864\n",
      "Step 519 validation: mae_score=0.260939\n",
      "Step 522 validation: mae_score=0.249804\n",
      "Step 525 validation: mae_score=0.254362\n",
      "Step 528 validation: mae_score=0.273279\n",
      "Step 531 validation: mae_score=0.293494\n",
      "Step 534 validation: mae_score=0.280386\n",
      "Step 537 validation: mae_score=0.260226\n",
      "Step 540 validation: mae_score=0.2475\n",
      "Step 543 validation: mae_score=0.242665\n",
      "Step 546 validation: mae_score=0.247466\n",
      "Step 549 validation: mae_score=0.267165\n",
      "Step 552 validation: mae_score=0.238606\n",
      "Step 555 validation: mae_score=0.242187\n",
      "Step 558 validation: mae_score=0.258623\n",
      "Step 561 validation: mae_score=0.282694\n",
      "Step 564 validation: mae_score=0.294013\n",
      "Step 567 validation: mae_score=0.264496\n",
      "Step 570 validation: mae_score=0.23967\n",
      "Step 573 validation: mae_score=0.252313\n",
      "Step 576 validation: mae_score=0.26322\n",
      "Step 579 validation: mae_score=0.272693\n",
      "Step 582 validation: mae_score=0.260262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 585 validation: mae_score=0.243195\n",
      "Step 588 validation: mae_score=0.245242\n",
      "Step 591 validation: mae_score=0.269539\n",
      "Step 594 validation: mae_score=0.294688\n",
      "Step 597 validation: mae_score=0.271466\n",
      "Step 600 validation: mae_score=0.258073\n",
      "Step 603 validation: mae_score=0.259228\n",
      "Step 606 validation: mae_score=0.271289\n",
      "Step 609 validation: mae_score=0.277667\n",
      "Step 612 validation: mae_score=0.256014\n",
      "Step 615 validation: mae_score=0.246669\n",
      "Step 618 validation: mae_score=0.249877\n",
      "Step 621 validation: mae_score=0.277545\n",
      "Step 624 validation: mae_score=0.257791\n",
      "Step 627 validation: mae_score=0.245307\n",
      "Step 630 validation: mae_score=0.255462\n",
      "Step 633 validation: mae_score=0.259694\n",
      "Step 636 validation: mae_score=0.25621\n",
      "Step 639 validation: mae_score=0.261621\n",
      "Step 642 validation: mae_score=0.267159\n",
      "Step 645 validation: mae_score=0.266196\n",
      "Step 648 validation: mae_score=0.253073\n",
      "Step 651 validation: mae_score=0.242368\n",
      "Step 654 validation: mae_score=0.249863\n",
      "Step 657 validation: mae_score=0.25738\n",
      "Step 660 validation: mae_score=0.262433\n",
      "Step 663 validation: mae_score=0.251555\n",
      "Step 666 validation: mae_score=0.259977\n",
      "Step 669 validation: mae_score=0.269247\n",
      "Step 672 validation: mae_score=0.267001\n",
      "Step 675 validation: mae_score=0.247687\n",
      "Step 678 validation: mae_score=0.231146\n",
      "Step 681 validation: mae_score=0.248563\n",
      "Step 684 validation: mae_score=0.273314\n",
      "Step 687 validation: mae_score=0.248778\n",
      "Step 690 validation: mae_score=0.242214\n",
      "Step 693 validation: mae_score=0.263349\n",
      "Step 696 validation: mae_score=0.261485\n",
      "Step 699 validation: mae_score=0.240074\n",
      "Step 702 validation: mae_score=0.240537\n",
      "Step 705 validation: mae_score=0.260852\n",
      "Step 708 validation: mae_score=0.251224\n",
      "Step 711 validation: mae_score=0.24878\n",
      "Step 714 validation: mae_score=0.279763\n",
      "Step 717 validation: mae_score=0.260138\n",
      "Step 720 validation: mae_score=0.254577\n",
      "Step 723 validation: mae_score=0.248201\n",
      "Step 726 validation: mae_score=0.240998\n",
      "Step 729 validation: mae_score=0.259088\n",
      "Step 732 validation: mae_score=0.281433\n",
      "Step 735 validation: mae_score=0.262528\n",
      "Step 738 validation: mae_score=0.239324\n",
      "Step 741 validation: mae_score=0.245414\n",
      "Step 744 validation: mae_score=0.251388\n",
      "Step 747 validation: mae_score=0.254859\n",
      "Step 750 validation: mae_score=0.265321\n",
      "Step 753 validation: mae_score=0.263343\n",
      "Step 756 validation: mae_score=0.246904\n",
      "Step 759 validation: mae_score=0.255528\n",
      "Step 762 validation: mae_score=0.246592\n",
      "Step 765 validation: mae_score=0.250215\n",
      "Step 768 validation: mae_score=0.264749\n",
      "Step 771 validation: mae_score=0.250571\n",
      "Step 774 validation: mae_score=0.241326\n",
      "Step 777 validation: mae_score=0.258735\n",
      "Step 780 validation: mae_score=0.254654\n",
      "Step 783 validation: mae_score=0.238468\n",
      "Step 786 validation: mae_score=0.238785\n",
      "Step 789 validation: mae_score=0.26388\n",
      "Step 792 validation: mae_score=0.267721\n",
      "Step 795 validation: mae_score=0.251831\n",
      "Step 798 validation: mae_score=0.265362\n",
      "Step 801 validation: mae_score=0.268478\n",
      "Step 804 validation: mae_score=0.25198\n",
      "Step 807 validation: mae_score=0.24116\n",
      "Step 810 validation: mae_score=0.245323\n",
      "Step 813 validation: mae_score=0.254455\n",
      "Step 816 validation: mae_score=0.250482\n",
      "Step 819 validation: mae_score=0.252571\n",
      "Step 822 validation: mae_score=0.279483\n",
      "Step 825 validation: mae_score=0.26633\n",
      "Step 828 validation: mae_score=0.248928\n",
      "Step 831 validation: mae_score=0.227398\n",
      "Step 834 validation: mae_score=0.229268\n",
      "Step 837 validation: mae_score=0.284521\n",
      "Step 840 validation: mae_score=0.258558\n",
      "Step 843 validation: mae_score=0.254197\n",
      "Step 846 validation: mae_score=0.257169\n",
      "Step 849 validation: mae_score=0.267646\n",
      "Step 852 validation: mae_score=0.285198\n",
      "Step 855 validation: mae_score=0.243384\n",
      "Step 858 validation: mae_score=0.232483\n",
      "Step 861 validation: mae_score=0.241654\n",
      "Step 864 validation: mae_score=0.267516\n",
      "Step 867 validation: mae_score=0.274961\n",
      "Step 870 validation: mae_score=0.248861\n",
      "Step 873 validation: mae_score=0.255511\n",
      "Step 876 validation: mae_score=0.251892\n",
      "Step 879 validation: mae_score=0.235599\n",
      "Step 882 validation: mae_score=0.266409\n",
      "Step 885 validation: mae_score=0.307299\n",
      "Step 888 validation: mae_score=0.248925\n",
      "Step 891 validation: mae_score=0.236256\n",
      "Step 894 validation: mae_score=0.2588\n",
      "Step 897 validation: mae_score=0.266031\n",
      "Step 900 validation: mae_score=0.241373\n",
      "Step 903 validation: mae_score=0.240556\n",
      "Step 906 validation: mae_score=0.265304\n",
      "Step 909 validation: mae_score=0.281876\n",
      "Step 912 validation: mae_score=0.251775\n",
      "Step 915 validation: mae_score=0.239384\n",
      "Step 918 validation: mae_score=0.250964\n",
      "Step 921 validation: mae_score=0.278165\n",
      "Step 924 validation: mae_score=0.266027\n",
      "Step 927 validation: mae_score=0.248713\n",
      "Step 930 validation: mae_score=0.242148\n",
      "Step 933 validation: mae_score=0.255031\n",
      "Step 936 validation: mae_score=0.251401\n",
      "Step 939 validation: mae_score=0.24567\n",
      "Step 942 validation: mae_score=0.259601\n",
      "Step 945 validation: mae_score=0.257851\n",
      "Step 948 validation: mae_score=0.266285\n",
      "Step 951 validation: mae_score=0.26147\n",
      "Step 954 validation: mae_score=0.24874\n",
      "Step 957 validation: mae_score=0.240731\n",
      "Step 960 validation: mae_score=0.250528\n",
      "Step 963 validation: mae_score=0.264216\n",
      "Step 966 validation: mae_score=0.26076\n",
      "Step 969 validation: mae_score=0.247574\n",
      "Step 972 validation: mae_score=0.247643\n",
      "Step 975 validation: mae_score=0.253048\n",
      "Step 978 validation: mae_score=0.233455\n",
      "Step 981 validation: mae_score=0.260361\n",
      "Step 984 validation: mae_score=0.275791\n",
      "Step 987 validation: mae_score=0.252429\n",
      "Step 990 validation: mae_score=0.239201\n",
      "Step 993 validation: mae_score=0.284436\n",
      "Step 996 validation: mae_score=0.25289\n",
      "Step 999 validation: mae_score=0.252312\n",
      "Step 1002 validation: mae_score=0.264429\n",
      "Step 1005 validation: mae_score=0.27728\n",
      "Step 1008 validation: mae_score=0.248306\n",
      "Step 1011 validation: mae_score=0.247051\n",
      "Step 1014 validation: mae_score=0.257652\n",
      "Step 1017 validation: mae_score=0.245808\n",
      "Step 1020 validation: mae_score=0.230558\n",
      "Step 1023 validation: mae_score=0.248822\n",
      "Step 1026 validation: mae_score=0.285878\n",
      "Step 1029 validation: mae_score=0.258253\n",
      "Step 1032 validation: mae_score=0.241303\n",
      "Step 1035 validation: mae_score=0.246456\n",
      "Step 1038 validation: mae_score=0.247286\n",
      "Step 1041 validation: mae_score=0.233423\n",
      "Step 1044 validation: mae_score=0.246798\n",
      "Step 1047 validation: mae_score=0.258821\n",
      "Step 1050 validation: mae_score=0.244742\n",
      "Step 1053 validation: mae_score=0.250018\n",
      "Step 1056 validation: mae_score=0.268534\n",
      "Step 1059 validation: mae_score=0.25381\n",
      "Step 1062 validation: mae_score=0.236349\n",
      "Step 1065 validation: mae_score=0.263394\n",
      "Step 1068 validation: mae_score=0.25197\n",
      "Step 1071 validation: mae_score=0.243464\n",
      "Step 1074 validation: mae_score=0.249604\n",
      "Step 1077 validation: mae_score=0.269125\n",
      "Step 1080 validation: mae_score=0.269751\n",
      "Step 1083 validation: mae_score=0.237154\n",
      "Step 1086 validation: mae_score=0.22955\n",
      "Step 1089 validation: mae_score=0.257972\n",
      "Step 1092 validation: mae_score=0.248784\n",
      "Step 1095 validation: mae_score=0.237436\n",
      "Step 1098 validation: mae_score=0.248418\n",
      "Step 1101 validation: mae_score=0.244279\n",
      "Step 1104 validation: mae_score=0.253086\n",
      "Step 1107 validation: mae_score=0.271017\n",
      "Step 1110 validation: mae_score=0.2666\n",
      "Step 1113 validation: mae_score=0.2491\n",
      "Step 1116 validation: mae_score=0.265707\n",
      "Step 1119 validation: mae_score=0.274011\n",
      "Step 1122 validation: mae_score=0.234982\n",
      "Step 1125 validation: mae_score=0.224661\n",
      "Step 1128 validation: mae_score=0.270236\n",
      "Step 1131 validation: mae_score=0.274415\n",
      "Step 1134 validation: mae_score=0.254606\n",
      "Step 1137 validation: mae_score=0.265445\n",
      "Step 1140 validation: mae_score=0.275292\n",
      "Step 1143 validation: mae_score=0.242642\n",
      "Step 1146 validation: mae_score=0.223894\n",
      "Step 1149 validation: mae_score=0.238731\n",
      "Step 1152 validation: mae_score=0.243223\n",
      "Step 1155 validation: mae_score=0.279169\n",
      "Step 1158 validation: mae_score=0.309919\n",
      "Step 1161 validation: mae_score=0.236542\n",
      "Step 1164 validation: mae_score=0.233804\n",
      "Step 1167 validation: mae_score=0.267321\n",
      "Step 1170 validation: mae_score=0.265261\n",
      "Step 1173 validation: mae_score=0.239286\n",
      "Step 1176 validation: mae_score=0.234004\n",
      "Step 1179 validation: mae_score=0.258859\n",
      "Step 1182 validation: mae_score=0.264292\n",
      "Step 1185 validation: mae_score=0.263463\n",
      "Step 1188 validation: mae_score=0.249332\n",
      "Step 1191 validation: mae_score=0.247085\n",
      "Step 1194 validation: mae_score=0.257496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1197 validation: mae_score=0.282671\n",
      "Step 1200 validation: mae_score=0.234595\n",
      "Step 1203 validation: mae_score=0.237414\n",
      "Step 1206 validation: mae_score=0.271557\n",
      "Step 1209 validation: mae_score=0.268936\n",
      "Step 1212 validation: mae_score=0.2491\n",
      "Step 1215 validation: mae_score=0.249306\n",
      "Step 1218 validation: mae_score=0.243343\n",
      "Step 1221 validation: mae_score=0.236153\n",
      "Step 1224 validation: mae_score=0.252007\n",
      "Step 1227 validation: mae_score=0.255204\n",
      "Step 1230 validation: mae_score=0.253429\n",
      "Step 1233 validation: mae_score=0.279401\n",
      "Step 1236 validation: mae_score=0.240673\n",
      "Step 1239 validation: mae_score=0.225733\n",
      "Step 1242 validation: mae_score=0.240513\n",
      "Step 1245 validation: mae_score=0.272501\n",
      "Step 1248 validation: mae_score=0.252461\n",
      "Step 1251 validation: mae_score=0.242145\n",
      "Step 1254 validation: mae_score=0.263483\n",
      "Step 1257 validation: mae_score=0.244719\n",
      "Step 1260 validation: mae_score=0.226184\n",
      "Step 1263 validation: mae_score=0.238286\n",
      "Step 1266 validation: mae_score=0.240566\n",
      "Step 1269 validation: mae_score=0.251011\n",
      "Step 1272 validation: mae_score=0.258601\n",
      "Step 1275 validation: mae_score=0.271977\n",
      "Step 1278 validation: mae_score=0.249007\n",
      "Step 1281 validation: mae_score=0.236503\n",
      "Step 1284 validation: mae_score=0.260597\n",
      "Step 1287 validation: mae_score=0.245775\n",
      "Step 1290 validation: mae_score=0.255564\n",
      "Step 1293 validation: mae_score=0.266212\n",
      "Step 1296 validation: mae_score=0.244179\n",
      "Step 1299 validation: mae_score=0.244115\n",
      "Step 1302 validation: mae_score=0.264774\n",
      "Step 1305 validation: mae_score=0.244135\n",
      "Step 1308 validation: mae_score=0.242776\n",
      "Step 1311 validation: mae_score=0.265101\n",
      "Step 1314 validation: mae_score=0.254986\n",
      "Step 1317 validation: mae_score=0.238671\n",
      "Step 1320 validation: mae_score=0.242391\n",
      "Step 1323 validation: mae_score=0.241326\n",
      "Step 1326 validation: mae_score=0.25557\n",
      "Step 1329 validation: mae_score=0.274159\n",
      "Step 1332 validation: mae_score=0.271907\n",
      "Step 1335 validation: mae_score=0.24907\n",
      "Step 1338 validation: mae_score=0.234069\n",
      "Step 1341 validation: mae_score=0.232357\n",
      "Step 1344 validation: mae_score=0.242664\n",
      "Step 1347 validation: mae_score=0.256386\n",
      "Step 1350 validation: mae_score=0.232468\n",
      "Step 1353 validation: mae_score=0.24831\n",
      "Step 1356 validation: mae_score=0.279138\n",
      "Step 1359 validation: mae_score=0.248196\n",
      "Step 1362 validation: mae_score=0.230873\n",
      "Step 1365 validation: mae_score=0.261722\n",
      "Step 1368 validation: mae_score=0.245138\n",
      "Step 1371 validation: mae_score=0.253445\n",
      "Step 1374 validation: mae_score=0.251331\n",
      "Step 1377 validation: mae_score=0.234788\n",
      "Step 1380 validation: mae_score=0.242844\n",
      "Step 1383 validation: mae_score=0.253774\n",
      "Step 1386 validation: mae_score=0.253857\n",
      "Step 1389 validation: mae_score=0.245142\n",
      "Step 1392 validation: mae_score=0.226305\n",
      "Step 1395 validation: mae_score=0.233851\n",
      "Step 1398 validation: mae_score=0.243203\n",
      "Step 1401 validation: mae_score=0.264698\n",
      "Step 1404 validation: mae_score=0.23995\n",
      "Step 1407 validation: mae_score=0.239795\n",
      "Step 1410 validation: mae_score=0.274381\n",
      "Step 1413 validation: mae_score=0.252491\n",
      "Step 1416 validation: mae_score=0.230253\n",
      "Step 1419 validation: mae_score=0.258606\n",
      "Step 1422 validation: mae_score=0.265965\n",
      "Step 1425 validation: mae_score=0.234312\n",
      "Step 1428 validation: mae_score=0.246517\n",
      "Step 1431 validation: mae_score=0.259247\n",
      "Step 1434 validation: mae_score=0.237425\n",
      "Step 1437 validation: mae_score=0.262297\n",
      "Step 1440 validation: mae_score=0.248661\n",
      "Step 1443 validation: mae_score=0.245043\n",
      "Step 1446 validation: mae_score=0.256532\n",
      "Step 1449 validation: mae_score=0.264246\n",
      "Step 1452 validation: mae_score=0.233335\n",
      "Step 1455 validation: mae_score=0.246391\n",
      "Step 1458 validation: mae_score=0.256835\n",
      "Step 1461 validation: mae_score=0.238586\n",
      "Step 1464 validation: mae_score=0.241731\n",
      "Step 1467 validation: mae_score=0.255594\n",
      "Step 1470 validation: mae_score=0.25175\n",
      "Step 1473 validation: mae_score=0.245669\n",
      "Step 1476 validation: mae_score=0.260516\n",
      "Step 1479 validation: mae_score=0.257644\n",
      "Step 1482 validation: mae_score=0.240915\n",
      "Step 1485 validation: mae_score=0.254368\n",
      "Step 1488 validation: mae_score=0.240018\n",
      "Step 1491 validation: mae_score=0.231922\n",
      "Step 1494 validation: mae_score=0.242942\n",
      "Step 1497 validation: mae_score=0.250555\n",
      "Step 1500 validation: mae_score=0.250549\n",
      "Step 1503 validation: mae_score=0.256719\n",
      "Step 1506 validation: mae_score=0.233001\n",
      "Step 1509 validation: mae_score=0.236049\n",
      "Step 1512 validation: mae_score=0.245466\n",
      "Step 1515 validation: mae_score=0.246957\n",
      "Step 1518 validation: mae_score=0.249107\n",
      "Step 1521 validation: mae_score=0.270676\n",
      "Step 1524 validation: mae_score=0.237903\n",
      "Step 1527 validation: mae_score=0.236214\n",
      "Step 1530 validation: mae_score=0.252037\n",
      "Step 1533 validation: mae_score=0.249639\n",
      "Step 1536 validation: mae_score=0.229975\n",
      "Step 1539 validation: mae_score=0.25113\n",
      "Step 1542 validation: mae_score=0.263777\n",
      "Step 1545 validation: mae_score=0.234488\n",
      "Step 1548 validation: mae_score=0.232829\n",
      "Step 1551 validation: mae_score=0.275516\n",
      "Step 1554 validation: mae_score=0.223299\n",
      "Step 1557 validation: mae_score=0.229935\n",
      "Step 1560 validation: mae_score=0.304291\n",
      "Step 1563 validation: mae_score=0.233265\n",
      "Step 1566 validation: mae_score=0.220638\n",
      "Step 1569 validation: mae_score=0.270419\n",
      "Step 1572 validation: mae_score=0.261628\n",
      "Step 1575 validation: mae_score=0.229572\n",
      "Step 1578 validation: mae_score=0.246313\n",
      "Step 1581 validation: mae_score=0.271468\n",
      "Step 1584 validation: mae_score=0.235618\n",
      "Step 1587 validation: mae_score=0.236329\n",
      "Step 1590 validation: mae_score=0.259005\n",
      "Step 1593 validation: mae_score=0.238827\n",
      "Step 1596 validation: mae_score=0.243662\n",
      "Step 1599 validation: mae_score=0.249946\n",
      "Step 1602 validation: mae_score=0.236184\n",
      "Step 1605 validation: mae_score=0.244798\n",
      "Step 1608 validation: mae_score=0.260644\n",
      "Step 1611 validation: mae_score=0.226622\n",
      "Step 1614 validation: mae_score=0.22832\n",
      "Step 1617 validation: mae_score=0.266598\n",
      "Step 1620 validation: mae_score=0.251185\n",
      "Step 1623 validation: mae_score=0.243254\n",
      "Step 1626 validation: mae_score=0.261809\n",
      "Step 1629 validation: mae_score=0.225411\n",
      "Step 1632 validation: mae_score=0.222087\n",
      "Step 1635 validation: mae_score=0.23863\n",
      "Step 1638 validation: mae_score=0.257158\n",
      "Step 1641 validation: mae_score=0.229197\n",
      "Step 1644 validation: mae_score=0.234687\n",
      "Step 1647 validation: mae_score=0.27101\n",
      "Step 1650 validation: mae_score=0.245747\n",
      "Step 1653 validation: mae_score=0.273934\n",
      "Step 1656 validation: mae_score=0.254113\n",
      "Step 1659 validation: mae_score=0.226351\n",
      "Step 1662 validation: mae_score=0.230001\n",
      "Step 1665 validation: mae_score=0.247211\n",
      "Step 1668 validation: mae_score=0.230941\n",
      "Step 1671 validation: mae_score=0.222491\n",
      "Step 1674 validation: mae_score=0.238859\n",
      "Step 1677 validation: mae_score=0.242164\n",
      "Step 1680 validation: mae_score=0.248077\n",
      "Step 1683 validation: mae_score=0.260485\n",
      "Step 1686 validation: mae_score=0.233735\n",
      "Step 1689 validation: mae_score=0.227711\n",
      "Step 1692 validation: mae_score=0.266869\n",
      "Step 1695 validation: mae_score=0.23405\n",
      "Step 1698 validation: mae_score=0.239913\n",
      "Step 1701 validation: mae_score=0.255443\n",
      "Step 1704 validation: mae_score=0.232301\n",
      "Step 1707 validation: mae_score=0.236841\n",
      "Step 1710 validation: mae_score=0.251695\n",
      "Step 1713 validation: mae_score=0.234841\n",
      "Step 1716 validation: mae_score=0.265409\n",
      "Step 1719 validation: mae_score=0.267638\n",
      "Step 1722 validation: mae_score=0.228721\n",
      "Step 1725 validation: mae_score=0.21848\n",
      "Step 1728 validation: mae_score=0.234269\n",
      "Step 1731 validation: mae_score=0.245286\n",
      "Step 1734 validation: mae_score=0.231553\n",
      "Step 1737 validation: mae_score=0.240728\n",
      "Step 1740 validation: mae_score=0.256819\n",
      "Step 1743 validation: mae_score=0.225157\n",
      "Step 1746 validation: mae_score=0.241434\n",
      "Step 1749 validation: mae_score=0.248137\n",
      "Step 1752 validation: mae_score=0.239654\n",
      "Step 1755 validation: mae_score=0.244651\n",
      "Step 1758 validation: mae_score=0.272611\n",
      "Step 1761 validation: mae_score=0.233128\n",
      "Step 1764 validation: mae_score=0.224214\n",
      "Step 1767 validation: mae_score=0.293339\n",
      "Step 1770 validation: mae_score=0.241799\n",
      "Step 1773 validation: mae_score=0.222179\n",
      "Step 1776 validation: mae_score=0.232645\n",
      "Step 1779 validation: mae_score=0.268493\n",
      "Step 1782 validation: mae_score=0.237309\n",
      "Step 1785 validation: mae_score=0.227259\n",
      "Step 1788 validation: mae_score=0.270155\n",
      "Step 1791 validation: mae_score=0.231625\n",
      "Step 1794 validation: mae_score=0.214556\n",
      "Step 1797 validation: mae_score=0.230707\n",
      "Step 1800 validation: mae_score=0.249782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1803 validation: mae_score=0.233598\n",
      "Step 1806 validation: mae_score=0.246373\n",
      "Step 1809 validation: mae_score=0.260554\n",
      "Step 1812 validation: mae_score=0.223154\n",
      "Step 1815 validation: mae_score=0.227605\n",
      "Step 1818 validation: mae_score=0.279556\n",
      "Step 1821 validation: mae_score=0.240481\n",
      "Step 1824 validation: mae_score=0.212419\n",
      "Step 1827 validation: mae_score=0.258874\n",
      "Step 1830 validation: mae_score=0.251882\n",
      "Step 1833 validation: mae_score=0.2189\n",
      "Step 1836 validation: mae_score=0.239376\n",
      "Step 1839 validation: mae_score=0.257315\n",
      "Step 1842 validation: mae_score=0.235577\n",
      "Step 1845 validation: mae_score=0.22771\n",
      "Step 1848 validation: mae_score=0.242695\n",
      "Step 1851 validation: mae_score=0.232336\n",
      "Step 1854 validation: mae_score=0.229212\n",
      "Step 1857 validation: mae_score=0.2532\n",
      "Step 1860 validation: mae_score=0.258147\n",
      "Step 1863 validation: mae_score=0.226414\n",
      "Step 1866 validation: mae_score=0.2216\n",
      "Step 1869 validation: mae_score=0.244221\n",
      "Step 1872 validation: mae_score=0.24263\n",
      "Step 1875 validation: mae_score=0.238601\n",
      "Step 1878 validation: mae_score=0.234844\n",
      "Step 1881 validation: mae_score=0.237776\n",
      "Step 1884 validation: mae_score=0.222155\n",
      "Step 1887 validation: mae_score=0.267501\n",
      "Step 1890 validation: mae_score=0.242834\n",
      "Step 1893 validation: mae_score=0.22506\n",
      "Step 1896 validation: mae_score=0.233217\n",
      "Step 1899 validation: mae_score=0.246692\n",
      "Step 1902 validation: mae_score=0.242435\n",
      "Step 1905 validation: mae_score=0.238579\n",
      "Step 1908 validation: mae_score=0.237217\n",
      "Step 1911 validation: mae_score=0.227238\n",
      "Step 1914 validation: mae_score=0.238112\n",
      "Step 1917 validation: mae_score=0.255383\n",
      "Step 1920 validation: mae_score=0.233075\n",
      "Step 1923 validation: mae_score=0.244869\n",
      "Step 1926 validation: mae_score=0.250937\n",
      "Step 1929 validation: mae_score=0.228144\n",
      "Step 1932 validation: mae_score=0.236747\n",
      "Step 1935 validation: mae_score=0.243952\n",
      "Step 1938 validation: mae_score=0.228999\n",
      "Step 1941 validation: mae_score=0.22328\n",
      "Step 1944 validation: mae_score=0.241559\n",
      "Step 1947 validation: mae_score=0.253278\n",
      "Step 1950 validation: mae_score=0.229558\n",
      "Step 1953 validation: mae_score=0.23951\n",
      "Step 1956 validation: mae_score=0.247038\n",
      "Step 1959 validation: mae_score=0.224585\n",
      "Step 1962 validation: mae_score=0.228973\n",
      "Step 1965 validation: mae_score=0.232555\n",
      "Step 1968 validation: mae_score=0.226778\n",
      "Step 1971 validation: mae_score=0.229642\n",
      "Step 1974 validation: mae_score=0.255784\n",
      "Step 1977 validation: mae_score=0.223008\n",
      "Step 1980 validation: mae_score=0.237377\n",
      "Step 1983 validation: mae_score=0.255526\n",
      "Step 1986 validation: mae_score=0.240391\n",
      "Step 1989 validation: mae_score=0.236317\n",
      "Step 1992 validation: mae_score=0.236178\n",
      "Step 1995 validation: mae_score=0.249513\n",
      "Step 1998 validation: mae_score=0.233736\n",
      "Step 2001 validation: mae_score=0.222062\n",
      "Step 2004 validation: mae_score=0.241358\n",
      "Step 2007 validation: mae_score=0.237276\n",
      "Step 2010 validation: mae_score=0.231739\n",
      "Step 2013 validation: mae_score=0.240363\n",
      "Step 2016 validation: mae_score=0.249648\n",
      "Step 2019 validation: mae_score=0.250321\n",
      "Step 2022 validation: mae_score=0.222258\n",
      "Step 2025 validation: mae_score=0.240399\n",
      "Step 2028 validation: mae_score=0.283712\n",
      "Step 2031 validation: mae_score=0.217188\n",
      "Step 2034 validation: mae_score=0.216709\n",
      "Step 2037 validation: mae_score=0.234418\n",
      "Step 2040 validation: mae_score=0.227259\n",
      "Step 2043 validation: mae_score=0.23987\n",
      "Step 2046 validation: mae_score=0.23593\n",
      "Step 2049 validation: mae_score=0.229711\n",
      "Step 2052 validation: mae_score=0.260545\n",
      "Step 2055 validation: mae_score=0.241099\n",
      "Step 2058 validation: mae_score=0.235636\n",
      "Step 2061 validation: mae_score=0.229803\n",
      "Step 2064 validation: mae_score=0.248962\n",
      "Step 2067 validation: mae_score=0.231748\n",
      "Step 2070 validation: mae_score=0.226613\n",
      "Step 2073 validation: mae_score=0.232275\n",
      "Step 2076 validation: mae_score=0.247686\n",
      "Step 2079 validation: mae_score=0.235528\n",
      "Step 2082 validation: mae_score=0.232743\n",
      "Step 2085 validation: mae_score=0.223034\n",
      "Step 2088 validation: mae_score=0.233414\n",
      "Step 2091 validation: mae_score=0.280581\n",
      "Step 2094 validation: mae_score=0.2255\n",
      "Step 2097 validation: mae_score=0.233325\n",
      "Step 2100 validation: mae_score=0.247561\n",
      "Step 2103 validation: mae_score=0.237721\n",
      "Step 2106 validation: mae_score=0.223297\n",
      "Step 2109 validation: mae_score=0.222753\n",
      "Step 2112 validation: mae_score=0.232901\n",
      "Step 2115 validation: mae_score=0.239107\n",
      "Step 2118 validation: mae_score=0.23453\n",
      "Step 2121 validation: mae_score=0.256492\n",
      "Step 2124 validation: mae_score=0.23622\n",
      "Step 2127 validation: mae_score=0.234919\n",
      "Step 2130 validation: mae_score=0.267418\n",
      "Step 2133 validation: mae_score=0.229896\n",
      "Step 2136 validation: mae_score=0.236968\n",
      "Step 2139 validation: mae_score=0.253473\n",
      "Step 2142 validation: mae_score=0.229738\n",
      "Step 2145 validation: mae_score=0.24466\n",
      "Step 2148 validation: mae_score=0.244944\n",
      "Step 2151 validation: mae_score=0.236918\n",
      "Step 2154 validation: mae_score=0.246625\n",
      "Step 2157 validation: mae_score=0.245378\n",
      "Step 2160 validation: mae_score=0.228549\n",
      "Step 2163 validation: mae_score=0.244465\n",
      "Step 2166 validation: mae_score=0.215714\n",
      "Step 2169 validation: mae_score=0.219224\n",
      "Step 2172 validation: mae_score=0.252911\n",
      "Step 2175 validation: mae_score=0.250054\n",
      "Step 2178 validation: mae_score=0.224388\n",
      "Step 2181 validation: mae_score=0.237701\n",
      "Step 2184 validation: mae_score=0.243938\n",
      "Step 2187 validation: mae_score=0.241127\n",
      "Step 2190 validation: mae_score=0.266934\n",
      "Step 2193 validation: mae_score=0.241409\n",
      "Step 2196 validation: mae_score=0.233303\n",
      "Step 2199 validation: mae_score=0.234418\n",
      "Step 2202 validation: mae_score=0.221425\n",
      "Step 2205 validation: mae_score=0.242811\n",
      "Step 2208 validation: mae_score=0.258067\n",
      "Step 2211 validation: mae_score=0.226855\n",
      "Step 2214 validation: mae_score=0.245002\n",
      "Step 2217 validation: mae_score=0.217283\n",
      "Step 2220 validation: mae_score=0.220659\n",
      "Step 2223 validation: mae_score=0.225256\n",
      "Step 2226 validation: mae_score=0.244599\n",
      "Step 2229 validation: mae_score=0.237486\n",
      "Step 2232 validation: mae_score=0.234795\n",
      "Step 2235 validation: mae_score=0.270158\n",
      "Step 2238 validation: mae_score=0.229306\n",
      "Step 2241 validation: mae_score=0.233628\n",
      "Step 2244 validation: mae_score=0.242931\n",
      "Step 2247 validation: mae_score=0.225162\n",
      "Step 2250 validation: mae_score=0.24555\n",
      "Step 2253 validation: mae_score=0.237872\n",
      "Step 2256 validation: mae_score=0.23182\n",
      "Step 2259 validation: mae_score=0.230698\n",
      "Step 2262 validation: mae_score=0.239157\n",
      "Step 2265 validation: mae_score=0.245016\n",
      "Step 2268 validation: mae_score=0.229225\n",
      "Step 2271 validation: mae_score=0.242789\n",
      "Step 2274 validation: mae_score=0.237137\n",
      "Step 2277 validation: mae_score=0.222393\n",
      "Step 2280 validation: mae_score=0.230276\n",
      "Step 2283 validation: mae_score=0.236407\n",
      "Step 2286 validation: mae_score=0.24037\n",
      "Step 2289 validation: mae_score=0.233132\n",
      "Step 2292 validation: mae_score=0.219771\n",
      "Step 2295 validation: mae_score=0.249646\n",
      "Step 2298 validation: mae_score=0.222305\n",
      "Step 2301 validation: mae_score=0.233055\n",
      "Step 2304 validation: mae_score=0.225895\n",
      "Step 2307 validation: mae_score=0.234956\n",
      "Step 2310 validation: mae_score=0.250217\n",
      "Step 2313 validation: mae_score=0.241905\n",
      "Step 2316 validation: mae_score=0.229236\n",
      "Step 2319 validation: mae_score=0.234487\n",
      "Step 2322 validation: mae_score=0.236307\n",
      "Step 2325 validation: mae_score=0.259153\n",
      "Step 2328 validation: mae_score=0.257733\n",
      "Step 2331 validation: mae_score=0.232393\n",
      "Step 2334 validation: mae_score=0.238136\n",
      "Step 2337 validation: mae_score=0.247483\n",
      "Step 2340 validation: mae_score=0.219552\n",
      "Step 2343 validation: mae_score=0.224875\n",
      "Step 2346 validation: mae_score=0.227826\n",
      "Step 2349 validation: mae_score=0.228015\n",
      "Step 2352 validation: mae_score=0.238845\n",
      "Step 2355 validation: mae_score=0.231023\n",
      "Step 2358 validation: mae_score=0.218039\n",
      "Step 2361 validation: mae_score=0.230401\n",
      "Step 2364 validation: mae_score=0.265079\n",
      "Step 2367 validation: mae_score=0.230869\n",
      "Step 2370 validation: mae_score=0.236716\n",
      "Step 2373 validation: mae_score=0.23787\n",
      "Step 2376 validation: mae_score=0.224803\n",
      "Step 2379 validation: mae_score=0.258952\n",
      "Step 2382 validation: mae_score=0.245947\n",
      "Step 2385 validation: mae_score=0.223416\n",
      "Step 2388 validation: mae_score=0.242023\n",
      "Step 2391 validation: mae_score=0.228239\n",
      "Step 2394 validation: mae_score=0.225891\n",
      "Step 2397 validation: mae_score=0.279504\n",
      "Step 2400 validation: mae_score=0.237263\n",
      "Step 2403 validation: mae_score=0.227455\n",
      "Step 2406 validation: mae_score=0.234238\n",
      "Step 2409 validation: mae_score=0.21617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2412 validation: mae_score=0.22813\n",
      "Step 2415 validation: mae_score=0.255228\n",
      "Step 2418 validation: mae_score=0.224798\n",
      "Step 2421 validation: mae_score=0.222453\n",
      "Step 2424 validation: mae_score=0.251977\n",
      "Step 2427 validation: mae_score=0.214231\n",
      "Step 2430 validation: mae_score=0.236901\n",
      "Step 2433 validation: mae_score=0.271709\n",
      "Step 2436 validation: mae_score=0.234109\n",
      "Step 2439 validation: mae_score=0.246757\n",
      "Step 2442 validation: mae_score=0.246686\n",
      "Step 2445 validation: mae_score=0.220199\n",
      "Step 2448 validation: mae_score=0.227028\n",
      "Step 2451 validation: mae_score=0.242914\n",
      "Step 2454 validation: mae_score=0.233723\n",
      "Step 2457 validation: mae_score=0.238606\n",
      "Step 2460 validation: mae_score=0.226245\n",
      "Step 2463 validation: mae_score=0.227522\n",
      "Step 2466 validation: mae_score=0.240758\n",
      "Step 2469 validation: mae_score=0.21741\n",
      "Step 2472 validation: mae_score=0.23249\n",
      "Step 2475 validation: mae_score=0.244299\n",
      "Step 2478 validation: mae_score=0.22645\n",
      "Step 2481 validation: mae_score=0.233739\n",
      "Step 2484 validation: mae_score=0.248483\n",
      "Step 2487 validation: mae_score=0.225332\n",
      "Step 2490 validation: mae_score=0.241728\n",
      "Step 2493 validation: mae_score=0.246008\n",
      "Step 2496 validation: mae_score=0.223354\n",
      "Step 2499 validation: mae_score=0.218076\n",
      "Step 2502 validation: mae_score=0.235963\n",
      "Step 2505 validation: mae_score=0.221793\n",
      "Step 2508 validation: mae_score=0.220817\n",
      "Step 2511 validation: mae_score=0.222113\n",
      "Step 2514 validation: mae_score=0.240953\n",
      "Step 2517 validation: mae_score=0.249119\n",
      "Step 2520 validation: mae_score=0.227928\n",
      "Step 2523 validation: mae_score=0.221467\n",
      "Step 2526 validation: mae_score=0.228251\n",
      "Step 2529 validation: mae_score=0.219681\n",
      "Step 2532 validation: mae_score=0.225148\n",
      "Step 2535 validation: mae_score=0.233227\n",
      "Step 2538 validation: mae_score=0.227469\n",
      "Step 2541 validation: mae_score=0.231792\n",
      "Step 2544 validation: mae_score=0.224713\n",
      "Step 2547 validation: mae_score=0.224613\n",
      "Step 2550 validation: mae_score=0.233948\n",
      "Step 2553 validation: mae_score=0.223855\n",
      "Step 2556 validation: mae_score=0.233024\n",
      "Step 2559 validation: mae_score=0.220066\n",
      "Step 2562 validation: mae_score=0.222244\n",
      "Step 2565 validation: mae_score=0.230131\n",
      "Step 2568 validation: mae_score=0.226673\n",
      "Step 2571 validation: mae_score=0.225608\n",
      "Step 2574 validation: mae_score=0.225706\n",
      "Step 2577 validation: mae_score=0.222153\n",
      "Step 2580 validation: mae_score=0.218176\n",
      "Step 2583 validation: mae_score=0.221192\n",
      "Step 2586 validation: mae_score=0.233048\n",
      "Step 2589 validation: mae_score=0.236394\n",
      "Step 2592 validation: mae_score=0.219865\n",
      "Step 2595 validation: mae_score=0.228716\n",
      "Step 2598 validation: mae_score=0.220567\n",
      "Step 2601 validation: mae_score=0.222388\n",
      "Step 2604 validation: mae_score=0.227544\n",
      "Step 2607 validation: mae_score=0.214061\n",
      "Step 2610 validation: mae_score=0.236421\n",
      "Step 2613 validation: mae_score=0.227388\n",
      "Step 2616 validation: mae_score=0.225465\n",
      "Step 2619 validation: mae_score=0.25166\n",
      "Step 2622 validation: mae_score=0.223385\n",
      "Step 2625 validation: mae_score=0.224087\n",
      "Step 2628 validation: mae_score=0.252691\n",
      "Step 2631 validation: mae_score=0.21014\n",
      "Step 2634 validation: mae_score=0.207451\n",
      "Step 2637 validation: mae_score=0.234079\n",
      "Step 2640 validation: mae_score=0.232303\n",
      "Step 2643 validation: mae_score=0.216851\n",
      "Step 2646 validation: mae_score=0.226671\n",
      "Step 2649 validation: mae_score=0.236402\n",
      "Step 2652 validation: mae_score=0.228255\n",
      "Step 2655 validation: mae_score=0.22662\n",
      "Step 2658 validation: mae_score=0.284103\n",
      "Step 2661 validation: mae_score=0.222752\n",
      "Step 2664 validation: mae_score=0.219414\n",
      "Step 2667 validation: mae_score=0.264255\n",
      "Step 2670 validation: mae_score=0.220427\n",
      "Step 2673 validation: mae_score=0.226676\n",
      "Step 2676 validation: mae_score=0.243674\n",
      "Step 2679 validation: mae_score=0.230759\n",
      "Step 2682 validation: mae_score=0.237785\n",
      "Step 2685 validation: mae_score=0.238559\n",
      "Step 2688 validation: mae_score=0.227561\n",
      "Step 2691 validation: mae_score=0.237758\n",
      "Step 2694 validation: mae_score=0.235773\n",
      "Step 2697 validation: mae_score=0.231055\n",
      "Step 2700 validation: mae_score=0.231864\n",
      "Step 2703 validation: mae_score=0.215496\n",
      "Step 2706 validation: mae_score=0.250117\n",
      "Step 2709 validation: mae_score=0.238801\n",
      "Step 2712 validation: mae_score=0.22185\n",
      "Step 2715 validation: mae_score=0.2438\n",
      "Step 2718 validation: mae_score=0.221395\n",
      "Step 2721 validation: mae_score=0.219555\n",
      "Step 2724 validation: mae_score=0.238397\n",
      "Step 2727 validation: mae_score=0.256839\n",
      "Step 2730 validation: mae_score=0.218133\n",
      "Step 2733 validation: mae_score=0.229312\n",
      "Step 2736 validation: mae_score=0.231437\n",
      "Step 2739 validation: mae_score=0.225916\n",
      "Step 2742 validation: mae_score=0.222509\n",
      "Step 2745 validation: mae_score=0.230719\n",
      "Step 2748 validation: mae_score=0.212295\n",
      "Step 2751 validation: mae_score=0.21561\n",
      "Step 2754 validation: mae_score=0.23856\n",
      "Step 2757 validation: mae_score=0.224367\n",
      "Step 2760 validation: mae_score=0.225763\n",
      "Step 2763 validation: mae_score=0.231783\n",
      "Step 2766 validation: mae_score=0.227646\n",
      "Step 2769 validation: mae_score=0.22005\n",
      "Step 2772 validation: mae_score=0.219862\n",
      "Step 2775 validation: mae_score=0.220159\n",
      "Step 2778 validation: mae_score=0.218219\n",
      "Step 2781 validation: mae_score=0.227834\n",
      "Step 2784 validation: mae_score=0.226555\n",
      "Step 2787 validation: mae_score=0.213999\n",
      "Step 2790 validation: mae_score=0.227742\n",
      "Step 2793 validation: mae_score=0.235481\n",
      "Step 2796 validation: mae_score=0.226957\n",
      "Step 2799 validation: mae_score=0.244557\n",
      "Step 2802 validation: mae_score=0.239308\n",
      "Step 2805 validation: mae_score=0.214997\n",
      "Step 2808 validation: mae_score=0.248794\n",
      "Step 2811 validation: mae_score=0.214093\n",
      "Step 2814 validation: mae_score=0.216091\n",
      "Step 2817 validation: mae_score=0.249347\n",
      "Step 2820 validation: mae_score=0.217871\n",
      "Step 2823 validation: mae_score=0.214989\n",
      "Step 2826 validation: mae_score=0.256794\n",
      "Step 2829 validation: mae_score=0.219715\n",
      "Step 2832 validation: mae_score=0.225123\n",
      "Step 2835 validation: mae_score=0.234304\n",
      "Step 2838 validation: mae_score=0.222586\n",
      "Step 2841 validation: mae_score=0.221952\n",
      "Step 2844 validation: mae_score=0.238474\n",
      "Step 2847 validation: mae_score=0.225242\n",
      "Step 2850 validation: mae_score=0.232964\n",
      "Step 2853 validation: mae_score=0.222985\n",
      "Step 2856 validation: mae_score=0.226057\n",
      "Step 2859 validation: mae_score=0.236034\n",
      "Step 2862 validation: mae_score=0.249215\n",
      "Step 2865 validation: mae_score=0.220263\n",
      "Step 2868 validation: mae_score=0.222915\n",
      "Step 2871 validation: mae_score=0.242918\n",
      "Step 2874 validation: mae_score=0.225533\n",
      "Step 2877 validation: mae_score=0.219927\n",
      "Step 2880 validation: mae_score=0.241353\n",
      "Step 2883 validation: mae_score=0.214098\n",
      "Step 2886 validation: mae_score=0.217593\n",
      "Step 2889 validation: mae_score=0.243362\n",
      "Step 2892 validation: mae_score=0.227432\n",
      "Step 2895 validation: mae_score=0.219353\n",
      "Step 2898 validation: mae_score=0.240951\n",
      "Step 2901 validation: mae_score=0.223534\n",
      "Step 2904 validation: mae_score=0.213566\n",
      "Step 2907 validation: mae_score=0.212898\n",
      "Step 2910 validation: mae_score=0.211775\n",
      "Step 2913 validation: mae_score=0.218997\n",
      "Step 2916 validation: mae_score=0.241067\n",
      "Step 2919 validation: mae_score=0.233217\n",
      "Step 2922 validation: mae_score=0.235175\n",
      "Step 2925 validation: mae_score=0.230977\n",
      "Step 2928 validation: mae_score=0.223891\n",
      "Step 2931 validation: mae_score=0.228006\n",
      "Step 2934 validation: mae_score=0.240955\n",
      "Step 2937 validation: mae_score=0.233473\n",
      "Step 2940 validation: mae_score=0.212895\n",
      "Step 2943 validation: mae_score=0.216436\n",
      "Step 2946 validation: mae_score=0.241968\n",
      "Step 2949 validation: mae_score=0.221029\n",
      "Step 2952 validation: mae_score=0.221013\n",
      "Step 2955 validation: mae_score=0.228531\n",
      "Step 2958 validation: mae_score=0.211417\n",
      "Step 2961 validation: mae_score=0.21765\n",
      "Step 2964 validation: mae_score=0.24749\n",
      "Step 2967 validation: mae_score=0.232451\n",
      "Step 2970 validation: mae_score=0.237742\n",
      "Step 2973 validation: mae_score=0.234509\n",
      "Step 2976 validation: mae_score=0.220794\n",
      "Step 2979 validation: mae_score=0.227551\n",
      "Step 2982 validation: mae_score=0.233866\n",
      "Step 2985 validation: mae_score=0.218254\n",
      "Step 2988 validation: mae_score=0.227522\n",
      "Step 2991 validation: mae_score=0.218258\n",
      "Step 2994 validation: mae_score=0.223579\n",
      "Step 2997 validation: mae_score=0.239081\n",
      "Step 3000 validation: mae_score=0.226032\n",
      "Step 3003 validation: mae_score=0.236407\n",
      "Step 3006 validation: mae_score=0.230953\n",
      "Step 3009 validation: mae_score=0.229994\n",
      "Step 3012 validation: mae_score=0.257743\n",
      "Step 3015 validation: mae_score=0.223241\n",
      "Step 3018 validation: mae_score=0.224638\n",
      "Step 3021 validation: mae_score=0.233441\n",
      "Step 3024 validation: mae_score=0.224973\n",
      "Step 3027 validation: mae_score=0.226323\n",
      "Step 3030 validation: mae_score=0.252185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3033 validation: mae_score=0.227659\n",
      "Step 3036 validation: mae_score=0.220973\n",
      "Step 3039 validation: mae_score=0.247495\n",
      "Step 3042 validation: mae_score=0.230535\n",
      "Step 3045 validation: mae_score=0.223911\n",
      "Step 3048 validation: mae_score=0.222473\n",
      "Step 3051 validation: mae_score=0.223846\n",
      "Step 3054 validation: mae_score=0.21549\n",
      "Step 3057 validation: mae_score=0.222776\n",
      "Step 3060 validation: mae_score=0.220364\n",
      "Step 3063 validation: mae_score=0.223751\n",
      "Step 3066 validation: mae_score=0.235224\n",
      "Step 3069 validation: mae_score=0.224178\n",
      "Step 3072 validation: mae_score=0.225888\n",
      "Step 3075 validation: mae_score=0.239394\n",
      "Step 3078 validation: mae_score=0.234993\n",
      "Step 3081 validation: mae_score=0.228914\n",
      "Step 3084 validation: mae_score=0.22919\n",
      "Step 3087 validation: mae_score=0.231317\n",
      "Step 3090 validation: mae_score=0.240846\n",
      "Step 3093 validation: mae_score=0.2206\n",
      "Step 3096 validation: mae_score=0.221888\n",
      "Step 3099 validation: mae_score=0.224323\n",
      "Step 3102 validation: mae_score=0.222403\n",
      "Step 3105 validation: mae_score=0.23767\n",
      "Step 3108 validation: mae_score=0.237307\n",
      "Step 3111 validation: mae_score=0.233857\n",
      "Step 3114 validation: mae_score=0.238259\n",
      "Step 3117 validation: mae_score=0.223183\n",
      "Step 3120 validation: mae_score=0.233026\n",
      "Step 3123 validation: mae_score=0.227588\n",
      "Step 3126 validation: mae_score=0.209813\n",
      "Step 3129 validation: mae_score=0.220875\n",
      "Step 3132 validation: mae_score=0.234277\n",
      "Step 3135 validation: mae_score=0.224356\n",
      "Step 3138 validation: mae_score=0.232014\n",
      "Step 3141 validation: mae_score=0.240167\n",
      "Step 3144 validation: mae_score=0.222117\n",
      "Step 3147 validation: mae_score=0.219828\n",
      "Step 3150 validation: mae_score=0.211665\n",
      "Step 3153 validation: mae_score=0.229316\n",
      "Step 3156 validation: mae_score=0.227299\n",
      "Step 3159 validation: mae_score=0.222893\n",
      "Step 3162 validation: mae_score=0.234716\n",
      "Step 3165 validation: mae_score=0.243558\n",
      "Step 3168 validation: mae_score=0.224873\n",
      "Step 3171 validation: mae_score=0.213701\n",
      "Step 3174 validation: mae_score=0.242682\n",
      "Step 3177 validation: mae_score=0.219197\n",
      "Step 3180 validation: mae_score=0.230147\n",
      "Step 3183 validation: mae_score=0.239251\n",
      "Step 3186 validation: mae_score=0.239068\n",
      "Step 3189 validation: mae_score=0.206702\n",
      "Step 3192 validation: mae_score=0.219672\n",
      "Step 3195 validation: mae_score=0.23529\n",
      "Step 3198 validation: mae_score=0.224036\n",
      "Step 3201 validation: mae_score=0.22932\n",
      "Step 3204 validation: mae_score=0.233372\n",
      "Step 3207 validation: mae_score=0.214363\n",
      "Step 3210 validation: mae_score=0.21637\n",
      "Step 3213 validation: mae_score=0.243482\n",
      "Step 3216 validation: mae_score=0.215633\n",
      "Step 3219 validation: mae_score=0.220861\n",
      "Step 3222 validation: mae_score=0.251261\n",
      "Step 3225 validation: mae_score=0.221133\n",
      "Step 3228 validation: mae_score=0.221381\n",
      "Step 3231 validation: mae_score=0.22855\n",
      "Step 3234 validation: mae_score=0.21259\n",
      "Step 3237 validation: mae_score=0.234352\n",
      "Step 3240 validation: mae_score=0.221344\n",
      "Step 3243 validation: mae_score=0.219927\n",
      "Step 3246 validation: mae_score=0.253548\n",
      "Step 3249 validation: mae_score=0.236783\n",
      "Step 3252 validation: mae_score=0.219752\n",
      "Step 3255 validation: mae_score=0.248907\n",
      "Step 3258 validation: mae_score=0.229989\n",
      "Step 3261 validation: mae_score=0.23211\n",
      "Step 3264 validation: mae_score=0.233\n",
      "Step 3267 validation: mae_score=0.222376\n",
      "Step 3270 validation: mae_score=0.231449\n",
      "Step 3273 validation: mae_score=0.235672\n",
      "Step 3276 validation: mae_score=0.225791\n",
      "Step 3279 validation: mae_score=0.226138\n",
      "Step 3282 validation: mae_score=0.243053\n",
      "Step 3285 validation: mae_score=0.218473\n",
      "Step 3288 validation: mae_score=0.229228\n",
      "Step 3291 validation: mae_score=0.223097\n",
      "Step 3294 validation: mae_score=0.219682\n",
      "Step 3297 validation: mae_score=0.23006\n",
      "Step 3300 validation: mae_score=0.218904\n",
      "Step 3303 validation: mae_score=0.215364\n",
      "Step 3306 validation: mae_score=0.228755\n",
      "Step 3309 validation: mae_score=0.227652\n",
      "Step 3312 validation: mae_score=0.238873\n",
      "Step 3315 validation: mae_score=0.222374\n",
      "Step 3318 validation: mae_score=0.236272\n",
      "Step 3321 validation: mae_score=0.230649\n",
      "Step 3324 validation: mae_score=0.229527\n",
      "Step 3327 validation: mae_score=0.234315\n",
      "Step 3330 validation: mae_score=0.213916\n",
      "Step 3333 validation: mae_score=0.203613\n",
      "Step 3336 validation: mae_score=0.234877\n",
      "Step 3339 validation: mae_score=0.224861\n",
      "Step 3342 validation: mae_score=0.240598\n",
      "Step 3345 validation: mae_score=0.228884\n",
      "Step 3348 validation: mae_score=0.234245\n",
      "Step 3351 validation: mae_score=0.230013\n",
      "Step 3354 validation: mae_score=0.227631\n",
      "Step 3357 validation: mae_score=0.232493\n",
      "Step 3360 validation: mae_score=0.241422\n",
      "Step 3363 validation: mae_score=0.235311\n",
      "Step 3366 validation: mae_score=0.216718\n",
      "Step 3369 validation: mae_score=0.21233\n",
      "Step 3372 validation: mae_score=0.217389\n",
      "Step 3375 validation: mae_score=0.262643\n",
      "Step 3378 validation: mae_score=0.217084\n",
      "Step 3381 validation: mae_score=0.221717\n",
      "Step 3384 validation: mae_score=0.253187\n",
      "Step 3387 validation: mae_score=0.22264\n",
      "Step 3390 validation: mae_score=0.234553\n",
      "Step 3393 validation: mae_score=0.238114\n",
      "Step 3396 validation: mae_score=0.231431\n",
      "Step 3399 validation: mae_score=0.234613\n",
      "Step 3402 validation: mae_score=0.23107\n",
      "Step 3405 validation: mae_score=0.215987\n",
      "Step 3408 validation: mae_score=0.231896\n",
      "Step 3411 validation: mae_score=0.221737\n",
      "Step 3414 validation: mae_score=0.230873\n",
      "Step 3417 validation: mae_score=0.219612\n",
      "Step 3420 validation: mae_score=0.219872\n",
      "Step 3423 validation: mae_score=0.235653\n",
      "Step 3426 validation: mae_score=0.22194\n",
      "Step 3429 validation: mae_score=0.227056\n",
      "Step 3432 validation: mae_score=0.218767\n",
      "Step 3435 validation: mae_score=0.22041\n",
      "Step 3438 validation: mae_score=0.231674\n",
      "Step 3441 validation: mae_score=0.227507\n",
      "Step 3444 validation: mae_score=0.223553\n",
      "Step 3447 validation: mae_score=0.224321\n",
      "Step 3450 validation: mae_score=0.234272\n",
      "Step 3453 validation: mae_score=0.228264\n",
      "Step 3456 validation: mae_score=0.26456\n",
      "Step 3459 validation: mae_score=0.225648\n",
      "Step 3462 validation: mae_score=0.228884\n",
      "Step 3465 validation: mae_score=0.230642\n",
      "Step 3468 validation: mae_score=0.208359\n",
      "Step 3471 validation: mae_score=0.220236\n",
      "Step 3474 validation: mae_score=0.222882\n",
      "Step 3477 validation: mae_score=0.232824\n",
      "Step 3480 validation: mae_score=0.234524\n",
      "Step 3483 validation: mae_score=0.21735\n",
      "Step 3486 validation: mae_score=0.218097\n",
      "Step 3489 validation: mae_score=0.239208\n",
      "Step 3492 validation: mae_score=0.21859\n",
      "Step 3495 validation: mae_score=0.223\n",
      "Step 3498 validation: mae_score=0.262602\n",
      "Step 3501 validation: mae_score=0.236649\n",
      "Step 3504 validation: mae_score=0.219142\n",
      "Step 3507 validation: mae_score=0.225704\n",
      "Step 3510 validation: mae_score=0.242174\n",
      "Step 3513 validation: mae_score=0.219553\n",
      "Step 3516 validation: mae_score=0.221158\n",
      "Step 3519 validation: mae_score=0.244111\n",
      "Step 3522 validation: mae_score=0.218614\n",
      "Step 3525 validation: mae_score=0.217402\n",
      "Step 3528 validation: mae_score=0.230319\n",
      "Step 3531 validation: mae_score=0.237749\n",
      "Step 3534 validation: mae_score=0.22427\n",
      "Step 3537 validation: mae_score=0.224712\n",
      "Step 3540 validation: mae_score=0.246562\n",
      "Step 3543 validation: mae_score=0.236809\n",
      "Step 3546 validation: mae_score=0.231573\n",
      "Step 3549 validation: mae_score=0.252541\n",
      "Step 3552 validation: mae_score=0.234722\n",
      "Step 3555 validation: mae_score=0.22289\n",
      "Step 3558 validation: mae_score=0.245624\n",
      "Step 3561 validation: mae_score=0.226972\n",
      "Step 3564 validation: mae_score=0.218471\n",
      "Step 3567 validation: mae_score=0.239255\n",
      "Step 3570 validation: mae_score=0.21873\n",
      "Step 3573 validation: mae_score=0.212883\n",
      "Step 3576 validation: mae_score=0.230984\n",
      "Step 3579 validation: mae_score=0.224216\n",
      "Step 3582 validation: mae_score=0.225104\n",
      "Step 3585 validation: mae_score=0.238288\n",
      "Step 3588 validation: mae_score=0.255175\n",
      "Step 3591 validation: mae_score=0.230341\n",
      "Step 3594 validation: mae_score=0.221789\n",
      "Step 3597 validation: mae_score=0.222453\n",
      "Step 3600 validation: mae_score=0.227835\n",
      "Step 3603 validation: mae_score=0.217016\n",
      "Step 3606 validation: mae_score=0.221844\n",
      "Step 3609 validation: mae_score=0.231582\n",
      "Step 3612 validation: mae_score=0.218447\n",
      "Step 3615 validation: mae_score=0.229476\n",
      "Step 3618 validation: mae_score=0.237818\n",
      "Step 3621 validation: mae_score=0.219647\n",
      "Step 3624 validation: mae_score=0.223555\n",
      "Step 3627 validation: mae_score=0.238611\n",
      "Step 3630 validation: mae_score=0.223429\n",
      "Step 3633 validation: mae_score=0.244546\n",
      "Step 3636 validation: mae_score=0.240202\n",
      "Step 3639 validation: mae_score=0.218998\n",
      "Step 3642 validation: mae_score=0.22113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3645 validation: mae_score=0.22366\n",
      "Step 3648 validation: mae_score=0.22905\n",
      "Step 3651 validation: mae_score=0.223216\n",
      "Step 3654 validation: mae_score=0.223388\n",
      "Step 3657 validation: mae_score=0.23373\n",
      "Step 3660 validation: mae_score=0.217463\n",
      "Step 3663 validation: mae_score=0.217919\n",
      "Step 3666 validation: mae_score=0.238113\n",
      "Step 3669 validation: mae_score=0.225914\n",
      "Step 3672 validation: mae_score=0.226379\n",
      "Step 3675 validation: mae_score=0.231718\n",
      "Step 3678 validation: mae_score=0.225123\n",
      "Step 3681 validation: mae_score=0.227099\n",
      "Step 3684 validation: mae_score=0.225223\n",
      "Step 3687 validation: mae_score=0.238084\n",
      "Step 3690 validation: mae_score=0.218423\n",
      "Step 3693 validation: mae_score=0.214119\n",
      "Step 3696 validation: mae_score=0.227641\n",
      "Step 3699 validation: mae_score=0.227765\n",
      "Step 3702 validation: mae_score=0.22303\n",
      "Step 3705 validation: mae_score=0.232847\n",
      "Step 3708 validation: mae_score=0.235472\n",
      "Step 3711 validation: mae_score=0.22134\n",
      "Step 3714 validation: mae_score=0.217818\n",
      "Step 3717 validation: mae_score=0.21766\n",
      "Step 3720 validation: mae_score=0.231677\n",
      "Step 3723 validation: mae_score=0.227261\n",
      "Step 3726 validation: mae_score=0.23348\n",
      "Step 3729 validation: mae_score=0.222485\n",
      "Step 3732 validation: mae_score=0.228894\n",
      "Step 3735 validation: mae_score=0.231749\n",
      "Step 3738 validation: mae_score=0.229115\n",
      "Step 3741 validation: mae_score=0.229089\n",
      "Step 3744 validation: mae_score=0.237017\n",
      "Step 3747 validation: mae_score=0.232557\n",
      "Step 3750 validation: mae_score=0.229505\n",
      "Step 3753 validation: mae_score=0.220187\n",
      "Step 3756 validation: mae_score=0.219739\n",
      "Step 3759 validation: mae_score=0.232462\n",
      "Step 3762 validation: mae_score=0.229653\n",
      "Step 3765 validation: mae_score=0.227717\n",
      "Step 3768 validation: mae_score=0.239187\n",
      "Step 3771 validation: mae_score=0.235899\n",
      "Step 3774 validation: mae_score=0.242135\n",
      "Step 3777 validation: mae_score=0.254913\n",
      "Step 3780 validation: mae_score=0.236325\n",
      "Step 3783 validation: mae_score=0.222051\n",
      "Step 3786 validation: mae_score=0.241969\n",
      "Step 3789 validation: mae_score=0.239277\n",
      "Step 3792 validation: mae_score=0.230509\n",
      "Step 3795 validation: mae_score=0.252511\n",
      "Step 3798 validation: mae_score=0.222198\n",
      "Step 3801 validation: mae_score=0.22979\n",
      "Step 3804 validation: mae_score=0.244377\n",
      "Step 3807 validation: mae_score=0.231865\n",
      "Step 3810 validation: mae_score=0.225881\n",
      "Step 3813 validation: mae_score=0.241319\n",
      "Step 3816 validation: mae_score=0.211425\n",
      "Step 3819 validation: mae_score=0.217174\n",
      "Step 3822 validation: mae_score=0.235783\n",
      "Step 3825 validation: mae_score=0.242485\n",
      "Step 3828 validation: mae_score=0.249939\n",
      "Step 3831 validation: mae_score=0.237587\n",
      "Step 3834 validation: mae_score=0.236183\n",
      "Step 3837 validation: mae_score=0.237484\n",
      "Step 3840 validation: mae_score=0.239369\n",
      "Step 3843 validation: mae_score=0.216561\n",
      "Step 3846 validation: mae_score=0.225603\n",
      "Step 3849 validation: mae_score=0.23475\n",
      "Step 3852 validation: mae_score=0.224791\n",
      "Step 3855 validation: mae_score=0.23784\n",
      "Step 3858 validation: mae_score=0.22755\n",
      "Step 3861 validation: mae_score=0.226814\n",
      "Step 3864 validation: mae_score=0.236269\n",
      "Step 3867 validation: mae_score=0.240331\n",
      "Step 3870 validation: mae_score=0.231204\n",
      "Step 3873 validation: mae_score=0.229789\n",
      "Step 3876 validation: mae_score=0.230503\n",
      "Step 3879 validation: mae_score=0.228548\n",
      "Step 3882 validation: mae_score=0.214759\n",
      "Step 3885 validation: mae_score=0.22187\n",
      "Step 3888 validation: mae_score=0.239305\n",
      "Step 3891 validation: mae_score=0.236072\n",
      "Step 3894 validation: mae_score=0.240894\n",
      "Step 3897 validation: mae_score=0.230613\n",
      "Step 3900 validation: mae_score=0.232596\n",
      "Step 3903 validation: mae_score=0.232618\n",
      "Step 3906 validation: mae_score=0.244059\n",
      "Step 3909 validation: mae_score=0.233096\n",
      "Step 3912 validation: mae_score=0.233166\n",
      "Step 3915 validation: mae_score=0.23555\n",
      "Step 3918 validation: mae_score=0.222868\n",
      "Step 3921 validation: mae_score=0.2214\n",
      "Step 3924 validation: mae_score=0.235112\n",
      "Step 3927 validation: mae_score=0.221783\n",
      "Step 3930 validation: mae_score=0.216703\n",
      "Step 3933 validation: mae_score=0.214865\n",
      "Step 3936 validation: mae_score=0.221855\n",
      "Step 3939 validation: mae_score=0.230318\n",
      "Step 3942 validation: mae_score=0.22998\n",
      "Step 3945 validation: mae_score=0.239598\n",
      "Step 3948 validation: mae_score=0.217165\n",
      "Step 3951 validation: mae_score=0.242119\n",
      "Step 3954 validation: mae_score=0.22933\n",
      "Step 3957 validation: mae_score=0.233315\n",
      "Step 3960 validation: mae_score=0.226255\n",
      "Step 3963 validation: mae_score=0.227577\n",
      "Step 3966 validation: mae_score=0.230545\n",
      "Step 3969 validation: mae_score=0.22833\n",
      "Step 3972 validation: mae_score=0.220194\n",
      "Step 3975 validation: mae_score=0.237314\n",
      "Step 3978 validation: mae_score=0.232072\n",
      "Step 3981 validation: mae_score=0.226322\n",
      "Step 3984 validation: mae_score=0.219792\n",
      "Step 3987 validation: mae_score=0.212202\n",
      "Step 3990 validation: mae_score=0.231478\n",
      "Step 3993 validation: mae_score=0.239245\n",
      "Step 3996 validation: mae_score=0.239545\n",
      "Step 3999 validation: mae_score=0.219901\n",
      "Step 4002 validation: mae_score=0.215759\n",
      "Step 4005 validation: mae_score=0.226162\n",
      "Step 4008 validation: mae_score=0.220702\n",
      "Step 4011 validation: mae_score=0.246275\n",
      "Step 4014 validation: mae_score=0.223307\n",
      "Step 4017 validation: mae_score=0.219809\n",
      "Step 4020 validation: mae_score=0.241902\n",
      "Step 4023 validation: mae_score=0.222768\n",
      "Step 4026 validation: mae_score=0.215331\n",
      "Step 4029 validation: mae_score=0.25005\n",
      "Step 4032 validation: mae_score=0.218957\n",
      "Step 4035 validation: mae_score=0.21718\n",
      "Step 4038 validation: mae_score=0.253909\n",
      "Step 4041 validation: mae_score=0.223788\n",
      "Step 4044 validation: mae_score=0.230213\n",
      "Step 4047 validation: mae_score=0.245852\n",
      "Step 4050 validation: mae_score=0.23171\n",
      "Step 4053 validation: mae_score=0.212791\n",
      "Step 4056 validation: mae_score=0.21143\n",
      "Step 4059 validation: mae_score=0.217863\n",
      "Step 4062 validation: mae_score=0.226457\n",
      "Step 4065 validation: mae_score=0.23156\n",
      "Step 4068 validation: mae_score=0.225767\n",
      "Step 4071 validation: mae_score=0.233408\n",
      "Step 4074 validation: mae_score=0.225364\n",
      "Step 4077 validation: mae_score=0.21724\n",
      "Step 4080 validation: mae_score=0.224971\n",
      "Step 4083 validation: mae_score=0.235116\n",
      "Step 4086 validation: mae_score=0.224947\n",
      "Step 4089 validation: mae_score=0.234876\n",
      "Step 4092 validation: mae_score=0.224525\n",
      "Step 4095 validation: mae_score=0.225806\n",
      "Step 4098 validation: mae_score=0.224236\n",
      "Step 4101 validation: mae_score=0.234823\n",
      "Step 4104 validation: mae_score=0.216307\n",
      "Step 4107 validation: mae_score=0.22551\n",
      "Step 4110 validation: mae_score=0.227943\n",
      "Step 4113 validation: mae_score=0.228994\n",
      "Step 4116 validation: mae_score=0.239694\n",
      "Step 4119 validation: mae_score=0.227495\n",
      "Step 4122 validation: mae_score=0.228069\n",
      "Step 4125 validation: mae_score=0.227661\n",
      "Step 4128 validation: mae_score=0.220127\n",
      "Step 4131 validation: mae_score=0.21058\n",
      "Step 4134 validation: mae_score=0.225136\n",
      "Step 4137 validation: mae_score=0.222246\n",
      "Step 4140 validation: mae_score=0.222609\n",
      "Step 4143 validation: mae_score=0.232763\n",
      "Step 4146 validation: mae_score=0.225124\n",
      "Step 4149 validation: mae_score=0.22464\n",
      "Step 4152 validation: mae_score=0.216643\n",
      "Step 4155 validation: mae_score=0.228177\n",
      "Step 4158 validation: mae_score=0.218696\n",
      "Step 4161 validation: mae_score=0.232836\n",
      "Step 4164 validation: mae_score=0.22758\n",
      "Step 4167 validation: mae_score=0.210696\n",
      "Step 4170 validation: mae_score=0.215995\n",
      "Step 4173 validation: mae_score=0.240838\n",
      "Step 4176 validation: mae_score=0.229661\n",
      "Step 4179 validation: mae_score=0.222069\n",
      "Step 4182 validation: mae_score=0.2101\n",
      "Step 4185 validation: mae_score=0.202607\n",
      "Step 4188 validation: mae_score=0.217948\n",
      "Step 4191 validation: mae_score=0.23581\n",
      "Step 4194 validation: mae_score=0.231107\n",
      "Step 4197 validation: mae_score=0.225412\n",
      "Step 4200 validation: mae_score=0.217685\n",
      "Step 4203 validation: mae_score=0.218127\n",
      "Step 4206 validation: mae_score=0.240997\n",
      "Step 4209 validation: mae_score=0.230032\n",
      "Step 4212 validation: mae_score=0.22197\n",
      "Step 4215 validation: mae_score=0.219039\n",
      "Step 4218 validation: mae_score=0.212405\n",
      "Step 4221 validation: mae_score=0.212253\n",
      "Step 4224 validation: mae_score=0.22126\n",
      "Step 4227 validation: mae_score=0.221821\n",
      "Step 4230 validation: mae_score=0.225113\n",
      "Step 4233 validation: mae_score=0.25907\n",
      "Step 4236 validation: mae_score=0.231028\n",
      "Step 4239 validation: mae_score=0.219517\n",
      "Step 4242 validation: mae_score=0.217365\n",
      "Step 4245 validation: mae_score=0.213071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4248 validation: mae_score=0.236037\n",
      "Step 4251 validation: mae_score=0.239747\n",
      "Step 4254 validation: mae_score=0.225098\n",
      "Step 4257 validation: mae_score=0.227978\n",
      "Step 4260 validation: mae_score=0.23136\n",
      "Step 4263 validation: mae_score=0.223728\n",
      "Step 4266 validation: mae_score=0.218713\n",
      "Step 4269 validation: mae_score=0.217948\n",
      "Step 4272 validation: mae_score=0.213979\n",
      "Step 4275 validation: mae_score=0.214466\n",
      "Step 4278 validation: mae_score=0.221287\n",
      "Step 4281 validation: mae_score=0.245724\n",
      "Step 4284 validation: mae_score=0.230038\n",
      "Step 4287 validation: mae_score=0.218647\n",
      "Step 4290 validation: mae_score=0.230322\n",
      "Step 4293 validation: mae_score=0.232982\n",
      "Step 4296 validation: mae_score=0.226006\n",
      "Step 4299 validation: mae_score=0.230475\n",
      "Step 4302 validation: mae_score=0.231467\n",
      "Step 4305 validation: mae_score=0.232135\n",
      "Step 4308 validation: mae_score=0.224559\n",
      "Step 4311 validation: mae_score=0.237147\n",
      "Step 4314 validation: mae_score=0.239637\n",
      "Step 4317 validation: mae_score=0.225992\n",
      "Step 4320 validation: mae_score=0.233538\n",
      "Step 4323 validation: mae_score=0.22483\n",
      "Step 4326 validation: mae_score=0.21509\n",
      "Step 4329 validation: mae_score=0.213358\n",
      "Step 4332 validation: mae_score=0.234904\n",
      "Step 4335 validation: mae_score=0.237246\n",
      "Step 4338 validation: mae_score=0.237915\n",
      "Step 4341 validation: mae_score=0.225324\n",
      "Step 4344 validation: mae_score=0.231051\n",
      "Step 4347 validation: mae_score=0.206593\n",
      "Step 4350 validation: mae_score=0.210666\n",
      "Step 4353 validation: mae_score=0.261042\n",
      "Step 4356 validation: mae_score=0.220025\n",
      "Step 4359 validation: mae_score=0.225498\n",
      "Step 4362 validation: mae_score=0.240316\n",
      "Step 4365 validation: mae_score=0.221443\n",
      "Step 4368 validation: mae_score=0.22683\n",
      "Step 4371 validation: mae_score=0.222498\n",
      "Step 4374 validation: mae_score=0.222346\n",
      "Step 4377 validation: mae_score=0.229197\n",
      "Step 4380 validation: mae_score=0.228938\n",
      "Step 4383 validation: mae_score=0.222443\n",
      "Step 4386 validation: mae_score=0.233023\n",
      "Step 4389 validation: mae_score=0.229716\n",
      "Step 4392 validation: mae_score=0.218924\n",
      "Step 4395 validation: mae_score=0.227576\n",
      "Step 4398 validation: mae_score=0.218483\n",
      "Step 4401 validation: mae_score=0.213846\n",
      "Step 4404 validation: mae_score=0.223001\n",
      "Step 4407 validation: mae_score=0.236483\n",
      "Step 4410 validation: mae_score=0.227142\n",
      "Step 4413 validation: mae_score=0.220911\n",
      "Step 4416 validation: mae_score=0.225463\n",
      "Step 4419 validation: mae_score=0.245202\n",
      "Step 4422 validation: mae_score=0.235191\n",
      "Step 4425 validation: mae_score=0.21503\n",
      "Step 4428 validation: mae_score=0.223699\n",
      "Step 4431 validation: mae_score=0.217536\n",
      "Step 4434 validation: mae_score=0.230594\n",
      "Step 4437 validation: mae_score=0.232727\n",
      "Step 4440 validation: mae_score=0.21809\n",
      "Step 4443 validation: mae_score=0.225914\n",
      "Step 4446 validation: mae_score=0.222047\n",
      "Step 4449 validation: mae_score=0.225674\n",
      "Step 4452 validation: mae_score=0.223768\n",
      "Step 4455 validation: mae_score=0.217296\n",
      "Step 4458 validation: mae_score=0.221268\n",
      "Step 4461 validation: mae_score=0.216735\n",
      "Step 4464 validation: mae_score=0.225299\n",
      "Step 4467 validation: mae_score=0.23174\n",
      "Step 4470 validation: mae_score=0.232824\n",
      "Step 4473 validation: mae_score=0.229946\n",
      "Step 4476 validation: mae_score=0.219256\n",
      "Step 4479 validation: mae_score=0.217179\n",
      "Step 4482 validation: mae_score=0.222667\n",
      "Step 4485 validation: mae_score=0.22571\n",
      "Step 4488 validation: mae_score=0.226782\n",
      "Step 4491 validation: mae_score=0.242696\n",
      "Step 4494 validation: mae_score=0.211368\n",
      "Step 4497 validation: mae_score=0.199436\n",
      "Step 4500 validation: mae_score=0.251426\n",
      "Step 4503 validation: mae_score=0.229624\n",
      "Step 4506 validation: mae_score=0.227104\n",
      "Step 4509 validation: mae_score=0.256079\n",
      "Step 4512 validation: mae_score=0.220037\n",
      "Step 4515 validation: mae_score=0.229322\n",
      "Step 4518 validation: mae_score=0.237428\n",
      "Step 4521 validation: mae_score=0.220761\n",
      "Step 4524 validation: mae_score=0.213981\n",
      "Step 4527 validation: mae_score=0.229857\n",
      "Step 4530 validation: mae_score=0.227775\n",
      "Step 4533 validation: mae_score=0.225433\n",
      "Step 4536 validation: mae_score=0.209799\n",
      "Step 4539 validation: mae_score=0.20274\n",
      "Step 4542 validation: mae_score=0.219657\n",
      "Step 4545 validation: mae_score=0.225902\n",
      "Step 4548 validation: mae_score=0.236801\n",
      "Step 4551 validation: mae_score=0.221741\n",
      "Step 4554 validation: mae_score=0.207028\n",
      "Step 4557 validation: mae_score=0.207192\n",
      "Step 4560 validation: mae_score=0.215565\n",
      "Step 4563 validation: mae_score=0.209537\n",
      "Step 4566 validation: mae_score=0.231687\n",
      "Step 4569 validation: mae_score=0.229713\n",
      "Step 4572 validation: mae_score=0.222931\n",
      "Step 4575 validation: mae_score=0.219617\n",
      "Step 4578 validation: mae_score=0.226596\n",
      "Step 4581 validation: mae_score=0.216109\n",
      "Step 4584 validation: mae_score=0.220521\n",
      "Step 4587 validation: mae_score=0.243906\n",
      "Step 4590 validation: mae_score=0.224045\n",
      "Step 4593 validation: mae_score=0.223123\n",
      "Step 4596 validation: mae_score=0.23648\n",
      "Step 4599 validation: mae_score=0.215389\n",
      "Step 4602 validation: mae_score=0.217879\n",
      "Step 4605 validation: mae_score=0.233817\n",
      "Step 4608 validation: mae_score=0.221937\n",
      "Step 4611 validation: mae_score=0.210896\n",
      "Step 4614 validation: mae_score=0.207802\n",
      "Step 4617 validation: mae_score=0.214818\n",
      "Step 4620 validation: mae_score=0.224256\n",
      "Step 4623 validation: mae_score=0.225252\n",
      "Step 4626 validation: mae_score=0.228624\n",
      "Step 4629 validation: mae_score=0.216144\n",
      "Step 4632 validation: mae_score=0.216449\n",
      "Step 4635 validation: mae_score=0.220313\n",
      "Step 4638 validation: mae_score=0.213239\n",
      "Step 4641 validation: mae_score=0.206431\n",
      "Step 4644 validation: mae_score=0.224613\n",
      "Step 4647 validation: mae_score=0.213345\n",
      "Step 4650 validation: mae_score=0.20988\n",
      "Step 4653 validation: mae_score=0.227147\n",
      "Step 4656 validation: mae_score=0.218652\n",
      "Step 4659 validation: mae_score=0.218353\n",
      "Step 4662 validation: mae_score=0.22904\n",
      "Step 4665 validation: mae_score=0.22267\n",
      "Step 4668 validation: mae_score=0.208348\n",
      "Step 4671 validation: mae_score=0.234642\n",
      "Step 4674 validation: mae_score=0.226354\n",
      "Step 4677 validation: mae_score=0.222164\n",
      "Step 4680 validation: mae_score=0.244591\n",
      "Step 4683 validation: mae_score=0.229036\n",
      "Step 4686 validation: mae_score=0.219821\n",
      "Step 4689 validation: mae_score=0.248875\n",
      "Step 4692 validation: mae_score=0.212139\n",
      "Step 4695 validation: mae_score=0.207977\n",
      "Step 4698 validation: mae_score=0.234634\n",
      "Step 4701 validation: mae_score=0.218109\n",
      "Step 4704 validation: mae_score=0.224033\n",
      "Step 4707 validation: mae_score=0.231986\n",
      "Step 4710 validation: mae_score=0.22275\n",
      "Step 4713 validation: mae_score=0.211979\n",
      "Step 4716 validation: mae_score=0.231159\n",
      "Step 4719 validation: mae_score=0.210745\n",
      "Step 4722 validation: mae_score=0.208375\n",
      "Step 4725 validation: mae_score=0.221395\n",
      "Step 4728 validation: mae_score=0.217879\n",
      "Step 4731 validation: mae_score=0.223218\n",
      "Step 4734 validation: mae_score=0.228044\n",
      "Step 4737 validation: mae_score=0.216962\n",
      "Step 4740 validation: mae_score=0.230058\n",
      "Step 4743 validation: mae_score=0.235868\n",
      "Step 4746 validation: mae_score=0.220266\n",
      "Step 4749 validation: mae_score=0.218471\n",
      "Step 4752 validation: mae_score=0.235259\n",
      "Step 4755 validation: mae_score=0.204683\n",
      "Step 4758 validation: mae_score=0.215731\n",
      "Step 4761 validation: mae_score=0.242285\n",
      "Step 4764 validation: mae_score=0.230565\n",
      "Step 4767 validation: mae_score=0.22219\n",
      "Step 4770 validation: mae_score=0.224939\n",
      "Step 4773 validation: mae_score=0.213261\n",
      "Step 4776 validation: mae_score=0.238552\n",
      "Step 4779 validation: mae_score=0.228526\n",
      "Step 4782 validation: mae_score=0.20588\n",
      "Step 4785 validation: mae_score=0.218878\n",
      "Step 4788 validation: mae_score=0.224259\n",
      "Step 4791 validation: mae_score=0.222905\n",
      "Step 4794 validation: mae_score=0.219342\n",
      "Step 4797 validation: mae_score=0.211788\n",
      "Step 4800 validation: mae_score=0.208189\n",
      "Step 4803 validation: mae_score=0.211595\n",
      "Step 4806 validation: mae_score=0.217394\n",
      "Step 4809 validation: mae_score=0.2258\n",
      "Step 4812 validation: mae_score=0.226849\n",
      "Step 4815 validation: mae_score=0.219287\n",
      "Step 4818 validation: mae_score=0.211537\n",
      "Step 4821 validation: mae_score=0.219099\n",
      "Step 4824 validation: mae_score=0.223156\n",
      "Step 4827 validation: mae_score=0.219176\n",
      "Step 4830 validation: mae_score=0.225993\n",
      "Step 4833 validation: mae_score=0.211559\n",
      "Step 4836 validation: mae_score=0.20975\n",
      "Step 4839 validation: mae_score=0.209478\n",
      "Step 4842 validation: mae_score=0.213615\n",
      "Step 4845 validation: mae_score=0.212472\n",
      "Step 4848 validation: mae_score=0.22811\n",
      "Step 4851 validation: mae_score=0.217741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4854 validation: mae_score=0.223612\n",
      "Step 4857 validation: mae_score=0.226421\n",
      "Step 4860 validation: mae_score=0.213556\n",
      "Step 4863 validation: mae_score=0.217926\n",
      "Step 4866 validation: mae_score=0.223738\n",
      "Step 4869 validation: mae_score=0.223525\n",
      "Step 4872 validation: mae_score=0.224382\n",
      "Step 4875 validation: mae_score=0.222304\n",
      "Step 4878 validation: mae_score=0.212551\n",
      "Step 4881 validation: mae_score=0.211717\n",
      "Step 4884 validation: mae_score=0.219012\n",
      "Step 4887 validation: mae_score=0.217996\n",
      "Step 4890 validation: mae_score=0.224869\n",
      "Step 4893 validation: mae_score=0.216351\n",
      "Step 4896 validation: mae_score=0.211291\n",
      "Step 4899 validation: mae_score=0.219036\n",
      "Step 4902 validation: mae_score=0.225095\n",
      "Step 4905 validation: mae_score=0.228187\n",
      "Step 4908 validation: mae_score=0.223445\n",
      "Step 4911 validation: mae_score=0.212138\n",
      "Step 4914 validation: mae_score=0.207991\n",
      "Step 4917 validation: mae_score=0.222945\n",
      "Step 4920 validation: mae_score=0.208433\n",
      "Step 4923 validation: mae_score=0.224531\n",
      "Step 4926 validation: mae_score=0.232737\n",
      "Step 4929 validation: mae_score=0.213208\n",
      "Step 4932 validation: mae_score=0.205345\n",
      "Step 4935 validation: mae_score=0.216219\n",
      "Step 4938 validation: mae_score=0.220729\n",
      "Step 4941 validation: mae_score=0.220426\n",
      "Step 4944 validation: mae_score=0.22394\n",
      "Step 4947 validation: mae_score=0.215817\n",
      "Step 4950 validation: mae_score=0.214956\n",
      "Step 4953 validation: mae_score=0.220633\n",
      "Step 4956 validation: mae_score=0.21673\n",
      "Step 4959 validation: mae_score=0.217702\n",
      "Step 4962 validation: mae_score=0.213047\n",
      "Step 4965 validation: mae_score=0.208444\n",
      "Step 4968 validation: mae_score=0.2119\n",
      "Step 4971 validation: mae_score=0.214214\n",
      "Step 4974 validation: mae_score=0.211614\n",
      "Step 4977 validation: mae_score=0.227673\n",
      "Step 4980 validation: mae_score=0.216214\n",
      "Step 4983 validation: mae_score=0.208357\n",
      "Step 4986 validation: mae_score=0.210294\n",
      "Step 4989 validation: mae_score=0.20838\n",
      "Step 4992 validation: mae_score=0.214685\n",
      "Step 4995 validation: mae_score=0.21319\n",
      "Step 4998 validation: mae_score=0.219851\n",
      "Step 5001 validation: mae_score=0.217428\n",
      "Step 5004 validation: mae_score=0.218796\n",
      "Step 5007 validation: mae_score=0.227331\n",
      "Step 5010 validation: mae_score=0.217379\n",
      "Step 5013 validation: mae_score=0.207686\n",
      "Step 5016 validation: mae_score=0.200527\n",
      "Step 5019 validation: mae_score=0.220325\n",
      "Step 5022 validation: mae_score=0.227539\n",
      "Step 5025 validation: mae_score=0.222065\n",
      "Step 5028 validation: mae_score=0.213683\n",
      "Step 5031 validation: mae_score=0.209874\n",
      "Step 5034 validation: mae_score=0.210172\n",
      "Step 5037 validation: mae_score=0.221608\n",
      "Step 5040 validation: mae_score=0.213293\n",
      "Step 5043 validation: mae_score=0.210632\n",
      "Step 5046 validation: mae_score=0.217787\n",
      "Step 5049 validation: mae_score=0.209127\n",
      "Step 5052 validation: mae_score=0.212093\n",
      "Step 5055 validation: mae_score=0.223672\n",
      "Step 5058 validation: mae_score=0.214614\n",
      "Step 5061 validation: mae_score=0.219987\n",
      "Step 5064 validation: mae_score=0.217264\n",
      "Step 5067 validation: mae_score=0.215769\n",
      "Step 5070 validation: mae_score=0.214085\n",
      "Step 5073 validation: mae_score=0.223329\n",
      "Step 5076 validation: mae_score=0.213179\n",
      "Step 5079 validation: mae_score=0.21749\n",
      "Step 5082 validation: mae_score=0.210359\n",
      "Step 5085 validation: mae_score=0.212555\n",
      "Step 5088 validation: mae_score=0.227631\n",
      "Step 5091 validation: mae_score=0.222642\n",
      "Step 5094 validation: mae_score=0.208723\n",
      "Step 5097 validation: mae_score=0.223145\n",
      "Step 5100 validation: mae_score=0.208326\n",
      "Step 5103 validation: mae_score=0.225005\n",
      "Step 5106 validation: mae_score=0.227914\n",
      "Step 5109 validation: mae_score=0.218283\n",
      "Step 5112 validation: mae_score=0.214199\n",
      "Step 5115 validation: mae_score=0.226401\n",
      "Step 5118 validation: mae_score=0.223531\n",
      "Step 5121 validation: mae_score=0.224219\n",
      "Step 5124 validation: mae_score=0.214626\n",
      "Step 5127 validation: mae_score=0.216065\n",
      "Step 5130 validation: mae_score=0.222781\n",
      "Step 5133 validation: mae_score=0.220121\n",
      "Step 5136 validation: mae_score=0.212882\n",
      "Step 5139 validation: mae_score=0.207461\n",
      "Step 5142 validation: mae_score=0.209337\n",
      "Step 5145 validation: mae_score=0.229093\n",
      "Step 5148 validation: mae_score=0.22695\n",
      "Step 5151 validation: mae_score=0.226314\n",
      "Step 5154 validation: mae_score=0.215479\n",
      "Step 5157 validation: mae_score=0.215173\n",
      "Step 5160 validation: mae_score=0.223459\n",
      "Step 5163 validation: mae_score=0.210125\n",
      "Step 5166 validation: mae_score=0.195693\n",
      "Step 5169 validation: mae_score=0.223622\n",
      "Step 5172 validation: mae_score=0.212692\n",
      "Step 5175 validation: mae_score=0.216267\n",
      "Step 5178 validation: mae_score=0.230342\n",
      "Step 5181 validation: mae_score=0.222516\n",
      "Step 5184 validation: mae_score=0.220543\n",
      "Step 5187 validation: mae_score=0.218607\n",
      "Step 5190 validation: mae_score=0.208294\n",
      "Step 5193 validation: mae_score=0.221349\n",
      "Step 5196 validation: mae_score=0.210714\n",
      "Step 5199 validation: mae_score=0.214808\n",
      "Step 5202 validation: mae_score=0.215883\n",
      "Step 5205 validation: mae_score=0.208102\n",
      "Step 5208 validation: mae_score=0.221897\n",
      "Step 5211 validation: mae_score=0.22628\n",
      "Step 5214 validation: mae_score=0.211455\n",
      "Step 5217 validation: mae_score=0.215805\n",
      "Step 5220 validation: mae_score=0.242008\n",
      "Step 5223 validation: mae_score=0.212118\n",
      "Step 5226 validation: mae_score=0.226418\n",
      "Step 5229 validation: mae_score=0.262635\n",
      "Step 5232 validation: mae_score=0.213945\n",
      "Step 5235 validation: mae_score=0.208887\n",
      "Step 5238 validation: mae_score=0.216014\n",
      "Step 5241 validation: mae_score=0.225823\n",
      "Step 5244 validation: mae_score=0.212278\n",
      "Step 5247 validation: mae_score=0.215422\n",
      "Step 5250 validation: mae_score=0.215187\n",
      "Step 5253 validation: mae_score=0.216722\n",
      "Step 5256 validation: mae_score=0.224758\n",
      "Step 5259 validation: mae_score=0.217877\n",
      "Step 5262 validation: mae_score=0.216537\n",
      "Step 5265 validation: mae_score=0.227485\n",
      "Step 5268 validation: mae_score=0.211508\n",
      "Step 5271 validation: mae_score=0.209994\n",
      "Step 5274 validation: mae_score=0.20967\n",
      "Step 5277 validation: mae_score=0.211472\n",
      "Step 5280 validation: mae_score=0.220932\n",
      "Step 5283 validation: mae_score=0.225345\n",
      "Step 5286 validation: mae_score=0.2208\n",
      "Step 5289 validation: mae_score=0.212245\n",
      "Step 5292 validation: mae_score=0.211124\n",
      "Step 5295 validation: mae_score=0.21148\n",
      "Step 5298 validation: mae_score=0.220535\n",
      "Step 5301 validation: mae_score=0.226135\n",
      "Step 5304 validation: mae_score=0.231338\n",
      "Step 5307 validation: mae_score=0.234417\n",
      "Step 5310 validation: mae_score=0.224766\n",
      "Step 5313 validation: mae_score=0.212946\n",
      "Step 5316 validation: mae_score=0.215391\n",
      "Step 5319 validation: mae_score=0.229689\n",
      "Step 5322 validation: mae_score=0.226745\n",
      "Step 5325 validation: mae_score=0.227095\n",
      "Step 5328 validation: mae_score=0.2226\n",
      "Step 5331 validation: mae_score=0.210188\n",
      "Step 5334 validation: mae_score=0.217202\n",
      "Step 5337 validation: mae_score=0.216765\n",
      "Step 5340 validation: mae_score=0.218276\n",
      "Step 5343 validation: mae_score=0.235814\n",
      "Step 5346 validation: mae_score=0.215981\n",
      "Step 5349 validation: mae_score=0.214831\n",
      "Step 5352 validation: mae_score=0.217078\n",
      "Step 5355 validation: mae_score=0.22138\n",
      "Step 5358 validation: mae_score=0.225514\n",
      "Step 5361 validation: mae_score=0.22566\n",
      "Step 5364 validation: mae_score=0.218737\n",
      "Step 5367 validation: mae_score=0.222845\n",
      "Step 5370 validation: mae_score=0.208531\n",
      "Step 5373 validation: mae_score=0.206883\n",
      "Step 5376 validation: mae_score=0.215207\n",
      "Step 5379 validation: mae_score=0.215928\n",
      "Step 5382 validation: mae_score=0.219888\n",
      "Step 5385 validation: mae_score=0.2137\n",
      "Step 5388 validation: mae_score=0.217091\n",
      "Step 5391 validation: mae_score=0.237548\n",
      "Step 5394 validation: mae_score=0.232316\n",
      "Step 5397 validation: mae_score=0.220187\n",
      "Step 5400 validation: mae_score=0.21964\n",
      "Step 5403 validation: mae_score=0.208987\n",
      "Step 5406 validation: mae_score=0.21723\n",
      "Step 5409 validation: mae_score=0.234972\n",
      "Step 5412 validation: mae_score=0.228773\n",
      "Step 5415 validation: mae_score=0.211302\n",
      "Step 5418 validation: mae_score=0.213499\n",
      "Step 5421 validation: mae_score=0.215129\n",
      "Step 5424 validation: mae_score=0.213443\n",
      "Step 5427 validation: mae_score=0.206332\n",
      "Step 5430 validation: mae_score=0.229137\n",
      "Step 5433 validation: mae_score=0.202887\n",
      "Step 5436 validation: mae_score=0.212011\n",
      "Step 5439 validation: mae_score=0.226282\n",
      "Step 5442 validation: mae_score=0.216326\n",
      "Step 5445 validation: mae_score=0.226067\n",
      "Step 5448 validation: mae_score=0.22262\n",
      "Step 5451 validation: mae_score=0.216919\n",
      "Step 5454 validation: mae_score=0.236918\n",
      "Step 5457 validation: mae_score=0.234383\n",
      "Step 5460 validation: mae_score=0.215493\n",
      "Step 5463 validation: mae_score=0.208491\n",
      "Step 5466 validation: mae_score=0.227498\n",
      "Step 5469 validation: mae_score=0.218113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5472 validation: mae_score=0.21668\n",
      "Step 5475 validation: mae_score=0.225054\n",
      "Step 5478 validation: mae_score=0.22774\n",
      "Step 5481 validation: mae_score=0.210717\n",
      "Step 5484 validation: mae_score=0.217212\n",
      "Step 5487 validation: mae_score=0.226536\n",
      "Step 5490 validation: mae_score=0.217824\n",
      "Step 5493 validation: mae_score=0.224681\n",
      "Step 5496 validation: mae_score=0.220924\n",
      "Step 5499 validation: mae_score=0.213486\n",
      "Step 5502 validation: mae_score=0.208493\n",
      "Step 5505 validation: mae_score=0.21773\n",
      "Step 5508 validation: mae_score=0.213057\n",
      "Step 5511 validation: mae_score=0.223039\n",
      "Step 5514 validation: mae_score=0.21194\n",
      "Step 5517 validation: mae_score=0.212797\n",
      "Step 5520 validation: mae_score=0.217127\n",
      "Step 5523 validation: mae_score=0.229552\n",
      "Step 5526 validation: mae_score=0.219525\n",
      "Step 5529 validation: mae_score=0.241119\n",
      "Step 5532 validation: mae_score=0.224079\n",
      "Step 5535 validation: mae_score=0.219471\n",
      "Step 5538 validation: mae_score=0.238235\n",
      "Step 5541 validation: mae_score=0.210627\n",
      "Step 5544 validation: mae_score=0.203924\n",
      "Step 5547 validation: mae_score=0.222703\n",
      "Step 5550 validation: mae_score=0.215816\n",
      "Step 5553 validation: mae_score=0.221597\n",
      "Step 5556 validation: mae_score=0.217002\n",
      "Step 5559 validation: mae_score=0.219965\n",
      "Step 5562 validation: mae_score=0.215191\n",
      "Step 5565 validation: mae_score=0.208323\n",
      "Step 5568 validation: mae_score=0.218112\n",
      "Step 5571 validation: mae_score=0.205668\n",
      "Step 5574 validation: mae_score=0.222294\n",
      "Step 5577 validation: mae_score=0.217988\n",
      "Step 5580 validation: mae_score=0.216276\n",
      "Step 5583 validation: mae_score=0.220934\n",
      "Step 5586 validation: mae_score=0.215589\n",
      "Step 5589 validation: mae_score=0.220479\n",
      "Step 5592 validation: mae_score=0.222023\n",
      "Step 5595 validation: mae_score=0.211754\n",
      "Step 5598 validation: mae_score=0.20765\n",
      "Step 5601 validation: mae_score=0.217564\n",
      "Step 5604 validation: mae_score=0.221708\n",
      "Step 5607 validation: mae_score=0.220192\n",
      "Step 5610 validation: mae_score=0.228745\n",
      "Step 5613 validation: mae_score=0.212649\n",
      "Step 5616 validation: mae_score=0.20565\n",
      "Step 5619 validation: mae_score=0.215129\n",
      "Step 5622 validation: mae_score=0.220511\n",
      "Step 5625 validation: mae_score=0.21538\n",
      "Step 5628 validation: mae_score=0.216637\n",
      "Step 5631 validation: mae_score=0.212368\n",
      "Step 5634 validation: mae_score=0.218271\n",
      "Step 5637 validation: mae_score=0.232957\n",
      "Step 5640 validation: mae_score=0.213778\n",
      "Step 5643 validation: mae_score=0.205359\n",
      "Step 5646 validation: mae_score=0.208206\n",
      "Step 5649 validation: mae_score=0.203581\n",
      "Step 5652 validation: mae_score=0.216001\n",
      "Step 5655 validation: mae_score=0.201962\n",
      "Step 5658 validation: mae_score=0.209696\n",
      "Step 5661 validation: mae_score=0.227501\n",
      "Step 5664 validation: mae_score=0.217897\n",
      "Step 5667 validation: mae_score=0.2105\n",
      "Step 5670 validation: mae_score=0.218309\n",
      "Step 5673 validation: mae_score=0.222046\n",
      "Step 5676 validation: mae_score=0.222338\n",
      "Step 5679 validation: mae_score=0.236293\n",
      "Step 5682 validation: mae_score=0.212447\n",
      "Step 5685 validation: mae_score=0.202512\n",
      "Step 5688 validation: mae_score=0.216947\n",
      "Step 5691 validation: mae_score=0.221335\n",
      "Step 5694 validation: mae_score=0.22162\n",
      "Step 5697 validation: mae_score=0.221511\n",
      "Step 5700 validation: mae_score=0.22482\n",
      "Step 5703 validation: mae_score=0.204331\n",
      "Step 5706 validation: mae_score=0.200948\n",
      "Step 5709 validation: mae_score=0.215004\n",
      "Step 5712 validation: mae_score=0.21763\n",
      "Step 5715 validation: mae_score=0.221507\n",
      "Step 5718 validation: mae_score=0.221872\n",
      "Step 5721 validation: mae_score=0.223292\n",
      "Step 5724 validation: mae_score=0.212128\n",
      "Step 5727 validation: mae_score=0.209953\n",
      "Step 5730 validation: mae_score=0.212211\n",
      "Step 5733 validation: mae_score=0.213797\n",
      "Step 5736 validation: mae_score=0.224086\n",
      "Step 5739 validation: mae_score=0.226308\n",
      "Step 5742 validation: mae_score=0.218284\n",
      "Step 5745 validation: mae_score=0.214206\n",
      "Step 5748 validation: mae_score=0.208947\n",
      "Step 5751 validation: mae_score=0.207367\n",
      "Step 5754 validation: mae_score=0.218712\n",
      "Step 5757 validation: mae_score=0.219109\n",
      "Step 5760 validation: mae_score=0.218181\n",
      "Step 5763 validation: mae_score=0.215917\n",
      "Step 5766 validation: mae_score=0.211448\n",
      "Step 5769 validation: mae_score=0.219415\n",
      "Step 5772 validation: mae_score=0.218569\n",
      "Step 5775 validation: mae_score=0.220352\n",
      "Step 5778 validation: mae_score=0.216374\n",
      "Step 5781 validation: mae_score=0.21361\n",
      "Step 5784 validation: mae_score=0.211063\n",
      "Step 5787 validation: mae_score=0.216783\n",
      "Step 5790 validation: mae_score=0.229856\n",
      "Step 5793 validation: mae_score=0.225584\n",
      "Step 5796 validation: mae_score=0.242552\n",
      "Step 5799 validation: mae_score=0.21855\n",
      "Step 5802 validation: mae_score=0.206125\n",
      "Step 5805 validation: mae_score=0.212565\n",
      "Step 5808 validation: mae_score=0.203886\n",
      "Step 5811 validation: mae_score=0.210307\n",
      "Step 5814 validation: mae_score=0.229337\n",
      "Step 5817 validation: mae_score=0.21841\n",
      "Step 5820 validation: mae_score=0.213454\n",
      "Step 5823 validation: mae_score=0.215902\n",
      "Step 5826 validation: mae_score=0.224835\n",
      "Step 5829 validation: mae_score=0.242401\n",
      "Step 5832 validation: mae_score=0.240002\n",
      "Step 5835 validation: mae_score=0.221443\n",
      "Step 5838 validation: mae_score=0.213383\n",
      "Step 5841 validation: mae_score=0.214457\n",
      "Step 5844 validation: mae_score=0.215326\n",
      "Step 5847 validation: mae_score=0.213531\n",
      "Step 5850 validation: mae_score=0.204299\n",
      "Step 5853 validation: mae_score=0.203369\n",
      "Step 5856 validation: mae_score=0.223761\n",
      "Step 5859 validation: mae_score=0.217578\n",
      "Step 5862 validation: mae_score=0.212085\n",
      "Step 5865 validation: mae_score=0.213493\n",
      "Step 5868 validation: mae_score=0.212815\n",
      "Step 5871 validation: mae_score=0.209913\n",
      "Step 5874 validation: mae_score=0.213324\n",
      "Step 5877 validation: mae_score=0.22017\n",
      "Step 5880 validation: mae_score=0.209977\n",
      "Step 5883 validation: mae_score=0.210584\n",
      "Step 5886 validation: mae_score=0.232022\n",
      "Step 5889 validation: mae_score=0.210023\n",
      "Step 5892 validation: mae_score=0.208496\n",
      "Step 5895 validation: mae_score=0.212682\n",
      "Step 5898 validation: mae_score=0.219154\n",
      "Step 5901 validation: mae_score=0.224037\n",
      "Step 5904 validation: mae_score=0.222316\n",
      "Step 5907 validation: mae_score=0.21829\n",
      "Step 5910 validation: mae_score=0.225453\n",
      "Step 5913 validation: mae_score=0.22489\n",
      "Step 5916 validation: mae_score=0.231734\n",
      "Step 5919 validation: mae_score=0.217944\n",
      "Step 5922 validation: mae_score=0.208844\n",
      "Step 5925 validation: mae_score=0.218723\n",
      "Step 5928 validation: mae_score=0.21651\n",
      "Step 5931 validation: mae_score=0.207906\n",
      "Step 5934 validation: mae_score=0.210706\n",
      "Step 5937 validation: mae_score=0.213453\n",
      "Step 5940 validation: mae_score=0.213309\n",
      "Step 5943 validation: mae_score=0.208987\n",
      "Step 5946 validation: mae_score=0.216571\n",
      "Step 5949 validation: mae_score=0.217591\n",
      "Step 5952 validation: mae_score=0.218488\n",
      "Step 5955 validation: mae_score=0.203363\n",
      "Step 5958 validation: mae_score=0.203201\n",
      "Step 5961 validation: mae_score=0.207408\n",
      "Step 5964 validation: mae_score=0.222542\n",
      "Step 5967 validation: mae_score=0.216117\n",
      "Step 5970 validation: mae_score=0.2163\n",
      "Step 5973 validation: mae_score=0.215982\n",
      "Step 5976 validation: mae_score=0.206138\n",
      "Step 5979 validation: mae_score=0.218273\n",
      "Step 5982 validation: mae_score=0.221958\n",
      "Step 5985 validation: mae_score=0.21527\n",
      "Step 5988 validation: mae_score=0.208355\n",
      "Step 5991 validation: mae_score=0.216636\n",
      "Step 5994 validation: mae_score=0.216613\n",
      "Step 5997 validation: mae_score=0.211599\n",
      "Step 6000 validation: mae_score=0.220331\n",
      "Step 6003 validation: mae_score=0.22624\n",
      "Step 6006 validation: mae_score=0.220231\n",
      "Step 6009 validation: mae_score=0.214053\n",
      "Step 6012 validation: mae_score=0.230328\n",
      "Step 6015 validation: mae_score=0.216601\n",
      "Step 6018 validation: mae_score=0.20863\n",
      "Step 6021 validation: mae_score=0.213553\n",
      "Step 6024 validation: mae_score=0.216667\n",
      "Step 6027 validation: mae_score=0.211756\n",
      "Step 6030 validation: mae_score=0.211188\n",
      "Step 6033 validation: mae_score=0.20902\n",
      "Step 6036 validation: mae_score=0.21093\n",
      "Step 6039 validation: mae_score=0.226689\n",
      "Step 6042 validation: mae_score=0.219088\n",
      "Step 6045 validation: mae_score=0.210322\n",
      "Step 6048 validation: mae_score=0.209844\n",
      "Step 6051 validation: mae_score=0.211956\n",
      "Step 6054 validation: mae_score=0.218681\n",
      "Step 6057 validation: mae_score=0.219038\n",
      "Step 6060 validation: mae_score=0.218084\n",
      "Step 6063 validation: mae_score=0.209855\n",
      "Step 6066 validation: mae_score=0.201684\n",
      "Step 6069 validation: mae_score=0.241284\n",
      "Step 6072 validation: mae_score=0.222078\n",
      "Step 6075 validation: mae_score=0.220849\n",
      "Step 6078 validation: mae_score=0.210783\n",
      "Step 6081 validation: mae_score=0.206449\n",
      "Step 6084 validation: mae_score=0.212971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6087 validation: mae_score=0.234043\n",
      "Step 6090 validation: mae_score=0.225324\n",
      "Step 6093 validation: mae_score=0.216323\n",
      "Step 6096 validation: mae_score=0.215596\n",
      "Step 6099 validation: mae_score=0.204869\n",
      "Step 6102 validation: mae_score=0.2075\n",
      "Step 6105 validation: mae_score=0.216016\n",
      "Step 6108 validation: mae_score=0.219509\n",
      "Step 6111 validation: mae_score=0.212705\n",
      "Step 6114 validation: mae_score=0.211703\n",
      "Step 6117 validation: mae_score=0.198648\n",
      "Step 6120 validation: mae_score=0.204019\n",
      "Step 6123 validation: mae_score=0.207396\n",
      "Step 6126 validation: mae_score=0.199603\n",
      "Step 6129 validation: mae_score=0.205072\n",
      "Step 6132 validation: mae_score=0.223334\n",
      "Step 6135 validation: mae_score=0.225791\n",
      "Step 6138 validation: mae_score=0.212906\n",
      "Step 6141 validation: mae_score=0.213943\n",
      "Step 6144 validation: mae_score=0.218317\n",
      "Step 6147 validation: mae_score=0.212309\n",
      "Step 6150 validation: mae_score=0.204574\n",
      "Step 6153 validation: mae_score=0.208223\n",
      "Step 6156 validation: mae_score=0.209401\n",
      "Step 6159 validation: mae_score=0.208235\n",
      "Step 6162 validation: mae_score=0.219262\n",
      "Step 6165 validation: mae_score=0.221497\n",
      "Step 6168 validation: mae_score=0.217239\n",
      "Step 6171 validation: mae_score=0.216843\n",
      "Step 6174 validation: mae_score=0.209093\n",
      "Step 6177 validation: mae_score=0.206524\n",
      "Step 6180 validation: mae_score=0.206812\n",
      "Step 6183 validation: mae_score=0.210992\n",
      "Step 6186 validation: mae_score=0.206365\n",
      "Step 6189 validation: mae_score=0.202426\n",
      "Step 6192 validation: mae_score=0.211968\n",
      "Step 6195 validation: mae_score=0.203475\n",
      "Step 6198 validation: mae_score=0.206059\n",
      "Step 6201 validation: mae_score=0.213021\n",
      "Step 6204 validation: mae_score=0.214783\n",
      "Step 6207 validation: mae_score=0.206173\n",
      "Step 6210 validation: mae_score=0.203695\n",
      "Step 6213 validation: mae_score=0.212716\n",
      "Step 6216 validation: mae_score=0.218699\n",
      "Step 6219 validation: mae_score=0.227718\n",
      "Step 6222 validation: mae_score=0.218182\n",
      "Step 6225 validation: mae_score=0.20377\n",
      "Step 6228 validation: mae_score=0.204249\n",
      "Step 6231 validation: mae_score=0.19878\n",
      "Step 6234 validation: mae_score=0.21398\n",
      "Step 6237 validation: mae_score=0.223986\n",
      "Step 6240 validation: mae_score=0.221031\n",
      "Step 6243 validation: mae_score=0.213244\n",
      "Step 6246 validation: mae_score=0.200497\n",
      "Step 6249 validation: mae_score=0.200688\n",
      "Step 6252 validation: mae_score=0.210296\n",
      "Step 6255 validation: mae_score=0.225642\n",
      "Step 6258 validation: mae_score=0.214014\n",
      "Step 6261 validation: mae_score=0.201067\n",
      "Step 6264 validation: mae_score=0.209412\n",
      "Step 6267 validation: mae_score=0.205108\n",
      "Step 6270 validation: mae_score=0.206041\n",
      "Step 6273 validation: mae_score=0.203091\n",
      "Step 6276 validation: mae_score=0.204028\n",
      "Step 6279 validation: mae_score=0.210246\n",
      "Step 6282 validation: mae_score=0.204791\n",
      "Step 6285 validation: mae_score=0.208191\n",
      "Step 6288 validation: mae_score=0.21739\n",
      "Step 6291 validation: mae_score=0.212549\n",
      "Step 6294 validation: mae_score=0.202843\n",
      "Step 6297 validation: mae_score=0.210132\n",
      "Step 6300 validation: mae_score=0.213238\n",
      "Step 6303 validation: mae_score=0.210491\n",
      "Step 6306 validation: mae_score=0.21719\n",
      "Step 6309 validation: mae_score=0.229135\n",
      "Step 6312 validation: mae_score=0.207101\n",
      "Step 6315 validation: mae_score=0.204978\n",
      "Step 6318 validation: mae_score=0.220854\n",
      "Step 6321 validation: mae_score=0.224655\n",
      "Step 6324 validation: mae_score=0.217436\n",
      "Step 6327 validation: mae_score=0.216355\n",
      "Step 6330 validation: mae_score=0.213117\n",
      "Step 6333 validation: mae_score=0.217784\n",
      "Step 6336 validation: mae_score=0.214933\n",
      "Step 6339 validation: mae_score=0.21631\n",
      "Step 6342 validation: mae_score=0.214045\n",
      "Step 6345 validation: mae_score=0.206299\n",
      "Step 6348 validation: mae_score=0.219691\n",
      "Step 6351 validation: mae_score=0.222512\n",
      "Step 6354 validation: mae_score=0.208601\n",
      "Step 6357 validation: mae_score=0.208534\n",
      "Step 6360 validation: mae_score=0.230677\n",
      "Step 6363 validation: mae_score=0.201707\n",
      "Step 6366 validation: mae_score=0.221152\n",
      "Step 6369 validation: mae_score=0.22044\n",
      "Step 6372 validation: mae_score=0.211068\n",
      "Step 6375 validation: mae_score=0.205932\n",
      "Step 6378 validation: mae_score=0.209682\n",
      "Step 6381 validation: mae_score=0.222887\n",
      "Step 6384 validation: mae_score=0.215905\n",
      "Step 6387 validation: mae_score=0.22791\n",
      "Step 6390 validation: mae_score=0.224871\n",
      "Step 6393 validation: mae_score=0.217193\n",
      "Step 6396 validation: mae_score=0.217917\n",
      "Step 6399 validation: mae_score=0.221063\n",
      "Step 6402 validation: mae_score=0.20693\n",
      "Step 6405 validation: mae_score=0.207444\n",
      "Step 6408 validation: mae_score=0.232659\n",
      "Step 6411 validation: mae_score=0.20508\n",
      "Step 6414 validation: mae_score=0.200784\n",
      "Step 6417 validation: mae_score=0.201244\n",
      "Step 6420 validation: mae_score=0.224257\n",
      "Step 6423 validation: mae_score=0.206013\n",
      "Step 6426 validation: mae_score=0.211898\n",
      "Step 6429 validation: mae_score=0.226795\n",
      "Step 6432 validation: mae_score=0.226846\n",
      "Step 6435 validation: mae_score=0.208533\n",
      "Step 6438 validation: mae_score=0.223137\n",
      "Step 6441 validation: mae_score=0.222473\n",
      "Step 6444 validation: mae_score=0.217459\n",
      "Step 6447 validation: mae_score=0.215242\n",
      "Step 6450 validation: mae_score=0.219023\n",
      "Step 6453 validation: mae_score=0.209244\n",
      "Step 6456 validation: mae_score=0.20723\n",
      "Step 6459 validation: mae_score=0.231685\n",
      "Step 6462 validation: mae_score=0.215085\n",
      "Step 6465 validation: mae_score=0.216045\n",
      "Step 6468 validation: mae_score=0.225147\n",
      "Step 6471 validation: mae_score=0.208586\n",
      "Step 6474 validation: mae_score=0.208491\n",
      "Step 6477 validation: mae_score=0.218867\n",
      "Step 6480 validation: mae_score=0.21128\n",
      "Step 6483 validation: mae_score=0.205561\n",
      "Step 6486 validation: mae_score=0.210329\n",
      "Step 6489 validation: mae_score=0.217374\n",
      "Step 6492 validation: mae_score=0.222856\n",
      "Step 6495 validation: mae_score=0.217619\n",
      "Step 6498 validation: mae_score=0.223734\n",
      "Step 6501 validation: mae_score=0.224795\n",
      "Step 6504 validation: mae_score=0.207964\n",
      "Step 6507 validation: mae_score=0.202565\n",
      "Step 6510 validation: mae_score=0.208406\n",
      "Step 6513 validation: mae_score=0.208672\n",
      "Step 6516 validation: mae_score=0.207812\n",
      "Step 6519 validation: mae_score=0.210676\n",
      "Step 6522 validation: mae_score=0.230067\n",
      "Step 6525 validation: mae_score=0.229293\n",
      "Step 6528 validation: mae_score=0.21067\n",
      "Step 6531 validation: mae_score=0.201747\n",
      "Step 6534 validation: mae_score=0.211693\n",
      "Step 6537 validation: mae_score=0.222393\n",
      "Step 6540 validation: mae_score=0.222977\n",
      "Step 6543 validation: mae_score=0.221653\n",
      "Step 6546 validation: mae_score=0.230533\n",
      "Step 6549 validation: mae_score=0.209698\n",
      "Step 6552 validation: mae_score=0.201723\n",
      "Step 6555 validation: mae_score=0.203846\n",
      "Step 6558 validation: mae_score=0.20696\n",
      "Step 6561 validation: mae_score=0.211919\n",
      "Step 6564 validation: mae_score=0.214196\n",
      "Step 6567 validation: mae_score=0.20782\n",
      "Step 6570 validation: mae_score=0.203899\n",
      "Step 6573 validation: mae_score=0.209027\n",
      "Step 6576 validation: mae_score=0.207405\n",
      "Step 6579 validation: mae_score=0.215649\n",
      "Step 6582 validation: mae_score=0.205441\n",
      "Step 6585 validation: mae_score=0.205093\n",
      "Step 6588 validation: mae_score=0.214895\n",
      "Step 6591 validation: mae_score=0.218008\n",
      "Step 6594 validation: mae_score=0.215712\n",
      "Step 6597 validation: mae_score=0.212546\n",
      "Step 6600 validation: mae_score=0.214831\n",
      "Step 6603 validation: mae_score=0.218353\n",
      "Step 6606 validation: mae_score=0.208245\n",
      "Step 6609 validation: mae_score=0.201818\n",
      "Step 6612 validation: mae_score=0.209931\n",
      "Step 6615 validation: mae_score=0.218685\n",
      "Step 6618 validation: mae_score=0.201846\n",
      "Step 6621 validation: mae_score=0.198875\n",
      "Step 6624 validation: mae_score=0.218809\n",
      "Step 6627 validation: mae_score=0.199801\n",
      "Step 6630 validation: mae_score=0.204939\n",
      "Step 6633 validation: mae_score=0.212545\n",
      "Step 6636 validation: mae_score=0.209696\n",
      "Step 6639 validation: mae_score=0.208539\n",
      "Step 6642 validation: mae_score=0.211991\n",
      "Step 6645 validation: mae_score=0.215353\n",
      "Step 6648 validation: mae_score=0.209483\n",
      "Step 6651 validation: mae_score=0.21172\n",
      "Step 6654 validation: mae_score=0.211225\n",
      "Step 6657 validation: mae_score=0.203041\n",
      "Step 6660 validation: mae_score=0.214268\n",
      "Step 6663 validation: mae_score=0.216324\n",
      "Step 6666 validation: mae_score=0.220375\n",
      "Step 6669 validation: mae_score=0.212332\n",
      "Step 6672 validation: mae_score=0.219637\n",
      "Step 6675 validation: mae_score=0.214736\n",
      "Step 6678 validation: mae_score=0.213832\n",
      "Step 6681 validation: mae_score=0.211486\n",
      "Step 6684 validation: mae_score=0.208416\n",
      "Step 6687 validation: mae_score=0.215051\n",
      "Step 6690 validation: mae_score=0.228268\n",
      "Step 6693 validation: mae_score=0.227021\n",
      "Step 6696 validation: mae_score=0.222374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6699 validation: mae_score=0.200064\n",
      "Step 6702 validation: mae_score=0.204393\n",
      "Step 6705 validation: mae_score=0.211563\n",
      "Step 6708 validation: mae_score=0.211592\n",
      "Step 6711 validation: mae_score=0.215028\n",
      "Step 6714 validation: mae_score=0.221958\n",
      "Step 6717 validation: mae_score=0.217347\n",
      "Step 6720 validation: mae_score=0.226061\n",
      "Step 6723 validation: mae_score=0.221621\n",
      "Step 6726 validation: mae_score=0.207537\n",
      "Step 6729 validation: mae_score=0.213158\n",
      "Step 6732 validation: mae_score=0.234577\n",
      "Step 6735 validation: mae_score=0.220447\n",
      "Step 6738 validation: mae_score=0.215173\n",
      "Step 6741 validation: mae_score=0.216492\n",
      "Step 6744 validation: mae_score=0.208708\n",
      "Step 6747 validation: mae_score=0.208564\n",
      "Step 6750 validation: mae_score=0.222884\n",
      "Step 6753 validation: mae_score=0.207485\n",
      "Step 6756 validation: mae_score=0.208232\n",
      "Step 6759 validation: mae_score=0.216022\n",
      "Step 6762 validation: mae_score=0.218959\n",
      "Step 6765 validation: mae_score=0.219685\n",
      "Step 6768 validation: mae_score=0.209008\n",
      "Step 6771 validation: mae_score=0.19928\n",
      "Step 6774 validation: mae_score=0.205391\n",
      "Step 6777 validation: mae_score=0.201486\n",
      "Step 6780 validation: mae_score=0.215142\n",
      "Step 6783 validation: mae_score=0.228008\n",
      "Step 6786 validation: mae_score=0.215711\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tdaf-tf2p7h2\\lib\\site-packages\\deepchem\\models\\keras_model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             deterministic=deterministic), max_checkpoints_to_keep,\n\u001b[1;32m--> 324\u001b[1;33m         checkpoint_interval, restore, variables, loss, callbacks, all_losses)\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m   def fit_generator(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\tdaf-tf2p7h2\\lib\\site-packages\\deepchem\\models\\keras_model.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[0;32m    410\u001b[0m       \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_gradient_for_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[1;32mraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3131\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3133\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1960\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    604\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fit da model\n",
    "model.fit(train_dataset, nb_epoch=num_epochs, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e77b1132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tr_mse</th>\n",
       "      <th>tr_r2</th>\n",
       "      <th>tr_mae</th>\n",
       "      <th>tr_rmse</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>val_r2</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>te_mse</th>\n",
       "      <th>te_r2</th>\n",
       "      <th>te_mae</th>\n",
       "      <th>te_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009318</td>\n",
       "      <td>0.951397</td>\n",
       "      <td>0.040878</td>\n",
       "      <td>0.096528</td>\n",
       "      <td>0.138653</td>\n",
       "      <td>0.291353</td>\n",
       "      <td>0.199119</td>\n",
       "      <td>0.372361</td>\n",
       "      <td>0.116042</td>\n",
       "      <td>0.389178</td>\n",
       "      <td>0.184053</td>\n",
       "      <td>0.34065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tr_mse     tr_r2    tr_mae   tr_rmse   val_mse    val_r2   val_mae  \\\n",
       "0  0.009318  0.951397  0.040878  0.096528  0.138653  0.291353  0.199119   \n",
       "\n",
       "   val_rmse    te_mse     te_r2    te_mae  te_rmse  \n",
       "0  0.372361  0.116042  0.389178  0.184053  0.34065  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# little function to calc metrics on this data\n",
    "out=h.get_them_metrics(\n",
    "    model,\n",
    "    datasets,\n",
    "    metrics,\n",
    "    metric_labels,\n",
    "    transformers=transformers_tf)\n",
    "\n",
    "pd_out = pd.DataFrame([out], \n",
    "        columns=['tr_mse', 'tr_r2', 'tr_mae', 'tr_rmse', # training metrics\n",
    "        'val_mse', 'val_r2', 'val_mae', 'val_rmse', # valdiation metrics\n",
    "        'te_mse', 'te_r2', 'te_mae', 'te_rmse']) # testing metrics\n",
    "pd_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aabe7c",
   "metadata": {},
   "source": [
    "## Using tf with NO DEEPCHEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "75718282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63.2970758 , 24.3690392 , 24.36830369,  0.46850576,  0.28909361,\n",
       "        0.22151853,  0.2214124 ,  0.1697016 ,  0.169665  ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.91842496, -1.        ,\n",
       "       -1.        ,  8.        ,  0.        ,  0.        ,  0.75130147,\n",
       "        0.        ,  0.        ,  1.70020294,  0.        ,  0.        ,\n",
       "        0.53168398,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset.X[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7b9cd030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db1483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f2856cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 11585.5742 - mean_absolute_error: 11585.5742 - val_loss: 1402.2545 - val_mean_absolute_error: 1402.2545\n",
      "Epoch 2/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1521.2905 - mean_absolute_error: 1521.2905 - val_loss: 1386.8643 - val_mean_absolute_error: 1386.8643\n",
      "Epoch 3/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1505.4926 - mean_absolute_error: 1505.4926 - val_loss: 1371.4764 - val_mean_absolute_error: 1371.4764\n",
      "Epoch 4/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1490.1049 - mean_absolute_error: 1490.1049 - val_loss: 1356.0892 - val_mean_absolute_error: 1356.0892\n",
      "Epoch 5/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1474.7153 - mean_absolute_error: 1474.7153 - val_loss: 1340.6963 - val_mean_absolute_error: 1340.6963\n",
      "Epoch 6/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1459.3224 - mean_absolute_error: 1459.3224 - val_loss: 1325.3026 - val_mean_absolute_error: 1325.3026\n",
      "Epoch 7/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1443.9294 - mean_absolute_error: 1443.9294 - val_loss: 1309.9100 - val_mean_absolute_error: 1309.9100\n",
      "Epoch 8/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1428.5361 - mean_absolute_error: 1428.5361 - val_loss: 1294.5170 - val_mean_absolute_error: 1294.5170\n",
      "Epoch 9/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1413.1464 - mean_absolute_error: 1413.1464 - val_loss: 1279.1306 - val_mean_absolute_error: 1279.1306\n",
      "Epoch 10/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1397.7634 - mean_absolute_error: 1397.7634 - val_loss: 1263.7495 - val_mean_absolute_error: 1263.7495\n",
      "Epoch 11/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1382.3816 - mean_absolute_error: 1382.3816 - val_loss: 1248.3680 - val_mean_absolute_error: 1248.3680\n",
      "Epoch 12/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1366.9987 - mean_absolute_error: 1366.9987 - val_loss: 1232.9863 - val_mean_absolute_error: 1232.9863\n",
      "Epoch 13/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1351.6177 - mean_absolute_error: 1351.6177 - val_loss: 1217.6052 - val_mean_absolute_error: 1217.6052\n",
      "Epoch 14/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1336.2372 - mean_absolute_error: 1336.2372 - val_loss: 1202.2239 - val_mean_absolute_error: 1202.2239\n",
      "Epoch 15/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1320.8556 - mean_absolute_error: 1320.8556 - val_loss: 1186.8423 - val_mean_absolute_error: 1186.8423\n",
      "Epoch 16/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1305.4756 - mean_absolute_error: 1305.4756 - val_loss: 1171.4606 - val_mean_absolute_error: 1171.4606\n",
      "Epoch 17/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1290.0905 - mean_absolute_error: 1290.0905 - val_loss: 1156.0708 - val_mean_absolute_error: 1156.0708\n",
      "Epoch 18/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1274.6888 - mean_absolute_error: 1274.6888 - val_loss: 1140.6655 - val_mean_absolute_error: 1140.6655\n",
      "Epoch 19/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1259.2861 - mean_absolute_error: 1259.2861 - val_loss: 1125.2610 - val_mean_absolute_error: 1125.2610\n",
      "Epoch 20/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1243.8804 - mean_absolute_error: 1243.8804 - val_loss: 1109.8552 - val_mean_absolute_error: 1109.8552\n",
      "Epoch 21/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1228.4758 - mean_absolute_error: 1228.4758 - val_loss: 1094.4507 - val_mean_absolute_error: 1094.4507\n",
      "Epoch 22/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1213.0702 - mean_absolute_error: 1213.0702 - val_loss: 1079.0455 - val_mean_absolute_error: 1079.0455\n",
      "Epoch 23/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1197.6646 - mean_absolute_error: 1197.6646 - val_loss: 1063.6403 - val_mean_absolute_error: 1063.6403\n",
      "Epoch 24/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1182.2609 - mean_absolute_error: 1182.2609 - val_loss: 1048.2352 - val_mean_absolute_error: 1048.2352\n",
      "Epoch 25/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1166.8555 - mean_absolute_error: 1166.8555 - val_loss: 1032.8301 - val_mean_absolute_error: 1032.8301\n",
      "Epoch 26/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1151.4503 - mean_absolute_error: 1151.4503 - val_loss: 1017.4254 - val_mean_absolute_error: 1017.4254\n",
      "Epoch 27/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1136.0487 - mean_absolute_error: 1136.0487 - val_loss: 1002.0253 - val_mean_absolute_error: 1002.0253\n",
      "Epoch 28/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1120.6573 - mean_absolute_error: 1120.6573 - val_loss: 986.6301 - val_mean_absolute_error: 986.6301\n",
      "Epoch 29/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1105.2725 - mean_absolute_error: 1105.2725 - val_loss: 971.2351 - val_mean_absolute_error: 971.2351\n",
      "Epoch 30/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1089.8881 - mean_absolute_error: 1089.8881 - val_loss: 955.8403 - val_mean_absolute_error: 955.8403\n",
      "Epoch 31/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1074.5028 - mean_absolute_error: 1074.5028 - val_loss: 940.4454 - val_mean_absolute_error: 940.4454\n",
      "Epoch 32/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1059.1177 - mean_absolute_error: 1059.1177 - val_loss: 925.0502 - val_mean_absolute_error: 925.0502\n",
      "Epoch 33/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1043.7332 - mean_absolute_error: 1043.7332 - val_loss: 909.6551 - val_mean_absolute_error: 909.6551\n",
      "Epoch 34/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1028.3475 - mean_absolute_error: 1028.3475 - val_loss: 894.2601 - val_mean_absolute_error: 894.2601\n",
      "Epoch 35/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 1012.9633 - mean_absolute_error: 1012.9633 - val_loss: 878.8649 - val_mean_absolute_error: 878.8649\n",
      "Epoch 36/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 997.5778 - mean_absolute_error: 997.5778 - val_loss: 863.4703 - val_mean_absolute_error: 863.4703\n",
      "Epoch 37/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 982.1941 - mean_absolute_error: 982.1941 - val_loss: 848.0801 - val_mean_absolute_error: 848.0801\n",
      "Epoch 38/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 966.8195 - mean_absolute_error: 966.8195 - val_loss: 832.6899 - val_mean_absolute_error: 832.6899\n",
      "Epoch 39/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 951.4453 - mean_absolute_error: 951.4453 - val_loss: 817.3002 - val_mean_absolute_error: 817.3002\n",
      "Epoch 40/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 936.0755 - mean_absolute_error: 936.0755 - val_loss: 801.9147 - val_mean_absolute_error: 801.9147\n",
      "Epoch 41/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 920.7105 - mean_absolute_error: 920.7105 - val_loss: 786.5301 - val_mean_absolute_error: 786.5301\n",
      "Epoch 42/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 905.3554 - mean_absolute_error: 905.3554 - val_loss: 771.1502 - val_mean_absolute_error: 771.1502\n",
      "Epoch 43/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 890.0023 - mean_absolute_error: 890.0023 - val_loss: 755.7752 - val_mean_absolute_error: 755.7752\n",
      "Epoch 44/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 874.6613 - mean_absolute_error: 874.6613 - val_loss: 740.4051 - val_mean_absolute_error: 740.4051\n",
      "Epoch 45/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 859.3300 - mean_absolute_error: 859.3300 - val_loss: 725.0399 - val_mean_absolute_error: 725.0399\n",
      "Epoch 46/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 844.0108 - mean_absolute_error: 844.0108 - val_loss: 709.6801 - val_mean_absolute_error: 709.6801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 828.7039 - mean_absolute_error: 828.7039 - val_loss: 694.3301 - val_mean_absolute_error: 694.3301\n",
      "Epoch 48/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 813.4124 - mean_absolute_error: 813.4124 - val_loss: 678.9848 - val_mean_absolute_error: 678.9848\n",
      "Epoch 49/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 798.1271 - mean_absolute_error: 798.1271 - val_loss: 663.6399 - val_mean_absolute_error: 663.6399\n",
      "Epoch 50/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 782.8427 - mean_absolute_error: 782.8427 - val_loss: 648.2953 - val_mean_absolute_error: 648.2953\n",
      "Epoch 51/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 767.5620 - mean_absolute_error: 767.5620 - val_loss: 632.9551 - val_mean_absolute_error: 632.9551\n",
      "Epoch 52/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 752.2919 - mean_absolute_error: 752.2919 - val_loss: 617.6205 - val_mean_absolute_error: 617.6205\n",
      "Epoch 53/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 737.0457 - mean_absolute_error: 737.0457 - val_loss: 602.3103 - val_mean_absolute_error: 602.3103\n",
      "Epoch 54/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 721.8346 - mean_absolute_error: 721.8346 - val_loss: 587.0190 - val_mean_absolute_error: 587.0190\n",
      "Epoch 55/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 706.6342 - mean_absolute_error: 706.6342 - val_loss: 571.7639 - val_mean_absolute_error: 571.7639\n",
      "Epoch 56/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 691.4419 - mean_absolute_error: 691.4419 - val_loss: 556.5187 - val_mean_absolute_error: 556.5187\n",
      "Epoch 57/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 676.3110 - mean_absolute_error: 676.3110 - val_loss: 541.3186 - val_mean_absolute_error: 541.3186\n",
      "Epoch 58/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 661.2438 - mean_absolute_error: 661.2438 - val_loss: 526.1380 - val_mean_absolute_error: 526.1380\n",
      "Epoch 59/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 646.2042 - mean_absolute_error: 646.2042 - val_loss: 511.0020 - val_mean_absolute_error: 511.0020\n",
      "Epoch 60/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 631.1796 - mean_absolute_error: 631.1796 - val_loss: 495.8808 - val_mean_absolute_error: 495.8808\n",
      "Epoch 61/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 616.1733 - mean_absolute_error: 616.1733 - val_loss: 480.8020 - val_mean_absolute_error: 480.8020\n",
      "Epoch 62/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 601.1974 - mean_absolute_error: 601.1974 - val_loss: 465.7554 - val_mean_absolute_error: 465.7554\n",
      "Epoch 63/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 586.2731 - mean_absolute_error: 586.2731 - val_loss: 450.7435 - val_mean_absolute_error: 450.7435\n",
      "Epoch 64/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 571.4243 - mean_absolute_error: 571.4243 - val_loss: 435.7762 - val_mean_absolute_error: 435.7762\n",
      "Epoch 65/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 556.6671 - mean_absolute_error: 556.6671 - val_loss: 420.8521 - val_mean_absolute_error: 420.8521\n",
      "Epoch 66/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 541.9697 - mean_absolute_error: 541.9697 - val_loss: 406.1599 - val_mean_absolute_error: 406.1599\n",
      "Epoch 67/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 527.3384 - mean_absolute_error: 527.3384 - val_loss: 391.7248 - val_mean_absolute_error: 391.7248\n",
      "Epoch 68/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 512.8251 - mean_absolute_error: 512.8251 - val_loss: 377.4680 - val_mean_absolute_error: 377.4680\n",
      "Epoch 69/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 498.3853 - mean_absolute_error: 498.3853 - val_loss: 363.3703 - val_mean_absolute_error: 363.3703\n",
      "Epoch 70/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 484.0145 - mean_absolute_error: 484.0145 - val_loss: 349.6234 - val_mean_absolute_error: 349.6234\n",
      "Epoch 71/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 469.7647 - mean_absolute_error: 469.7647 - val_loss: 336.1581 - val_mean_absolute_error: 336.1581\n",
      "Epoch 72/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 455.6424 - mean_absolute_error: 455.6424 - val_loss: 322.8677 - val_mean_absolute_error: 322.8677\n",
      "Epoch 73/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 441.6785 - mean_absolute_error: 441.6785 - val_loss: 309.8187 - val_mean_absolute_error: 309.8187\n",
      "Epoch 74/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 427.9046 - mean_absolute_error: 427.9046 - val_loss: 297.1385 - val_mean_absolute_error: 297.1385\n",
      "Epoch 75/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 414.3987 - mean_absolute_error: 414.3987 - val_loss: 284.7197 - val_mean_absolute_error: 284.7197\n",
      "Epoch 76/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 401.0653 - mean_absolute_error: 401.0653 - val_loss: 272.8756 - val_mean_absolute_error: 272.8756\n",
      "Epoch 77/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 388.1195 - mean_absolute_error: 388.1195 - val_loss: 261.6408 - val_mean_absolute_error: 261.6408\n",
      "Epoch 78/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 375.4953 - mean_absolute_error: 375.4953 - val_loss: 250.8878 - val_mean_absolute_error: 250.8878\n",
      "Epoch 79/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 363.1273 - mean_absolute_error: 363.1273 - val_loss: 240.5574 - val_mean_absolute_error: 240.5574\n",
      "Epoch 80/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 350.9764 - mean_absolute_error: 350.9764 - val_loss: 230.6147 - val_mean_absolute_error: 230.6147\n",
      "Epoch 81/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 339.1760 - mean_absolute_error: 339.1760 - val_loss: 221.1771 - val_mean_absolute_error: 221.1771\n",
      "Epoch 82/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 327.8181 - mean_absolute_error: 327.8181 - val_loss: 212.5483 - val_mean_absolute_error: 212.5483\n",
      "Epoch 83/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 316.9106 - mean_absolute_error: 316.9106 - val_loss: 205.2432 - val_mean_absolute_error: 205.2432\n",
      "Epoch 84/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 306.5189 - mean_absolute_error: 306.5189 - val_loss: 198.8598 - val_mean_absolute_error: 198.8598\n",
      "Epoch 85/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 296.6541 - mean_absolute_error: 296.6541 - val_loss: 193.0550 - val_mean_absolute_error: 193.0550\n",
      "Epoch 86/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 287.3155 - mean_absolute_error: 287.3155 - val_loss: 187.8886 - val_mean_absolute_error: 187.8886\n",
      "Epoch 87/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 278.4275 - mean_absolute_error: 278.4275 - val_loss: 183.4365 - val_mean_absolute_error: 183.4365\n",
      "Epoch 88/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 269.9203 - mean_absolute_error: 269.9203 - val_loss: 179.8967 - val_mean_absolute_error: 179.8967\n",
      "Epoch 89/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 261.9213 - mean_absolute_error: 261.9213 - val_loss: 177.3190 - val_mean_absolute_error: 177.3190\n",
      "Epoch 90/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 254.5163 - mean_absolute_error: 254.5163 - val_loss: 175.5471 - val_mean_absolute_error: 175.5471\n",
      "Epoch 91/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 247.5685 - mean_absolute_error: 247.5685 - val_loss: 174.0126 - val_mean_absolute_error: 174.0126\n",
      "Epoch 92/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 241.0786 - mean_absolute_error: 241.0786 - val_loss: 172.6016 - val_mean_absolute_error: 172.6016\n",
      "Epoch 93/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 235.1153 - mean_absolute_error: 235.1153 - val_loss: 171.5296 - val_mean_absolute_error: 171.5296\n",
      "Epoch 94/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 229.6152 - mean_absolute_error: 229.6152 - val_loss: 170.8845 - val_mean_absolute_error: 170.8845\n",
      "Epoch 95/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 224.5965 - mean_absolute_error: 224.5965 - val_loss: 170.5910 - val_mean_absolute_error: 170.5910\n",
      "Epoch 96/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 219.9339 - mean_absolute_error: 219.9339 - val_loss: 170.5040 - val_mean_absolute_error: 170.5040\n",
      "Epoch 97/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 215.6155 - mean_absolute_error: 215.6155 - val_loss: 170.5369 - val_mean_absolute_error: 170.5369\n",
      "Epoch 98/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 211.6110 - mean_absolute_error: 211.6110 - val_loss: 170.7004 - val_mean_absolute_error: 170.7004\n",
      "Epoch 99/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 207.9177 - mean_absolute_error: 207.9177 - val_loss: 171.0449 - val_mean_absolute_error: 171.0449\n",
      "Epoch 100/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 204.5758 - mean_absolute_error: 204.5758 - val_loss: 171.5548 - val_mean_absolute_error: 171.5548\n",
      "Epoch 101/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 201.5666 - mean_absolute_error: 201.5666 - val_loss: 172.2681 - val_mean_absolute_error: 172.2681\n",
      "Epoch 102/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 198.9045 - mean_absolute_error: 198.9045 - val_loss: 173.1533 - val_mean_absolute_error: 173.1533\n",
      "Epoch 103/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 196.4973 - mean_absolute_error: 196.4973 - val_loss: 174.1311 - val_mean_absolute_error: 174.1311\n",
      "Epoch 104/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 194.2851 - mean_absolute_error: 194.2851 - val_loss: 175.2543 - val_mean_absolute_error: 175.2543\n",
      "Epoch 105/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 192.2339 - mean_absolute_error: 192.2339 - val_loss: 176.6429 - val_mean_absolute_error: 176.6429\n",
      "Epoch 106/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 190.3618 - mean_absolute_error: 190.3618 - val_loss: 178.1788 - val_mean_absolute_error: 178.1788\n",
      "Epoch 107/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 188.6863 - mean_absolute_error: 188.6863 - val_loss: 179.7312 - val_mean_absolute_error: 179.7312\n",
      "Epoch 108/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 187.2090 - mean_absolute_error: 187.2090 - val_loss: 181.2752 - val_mean_absolute_error: 181.2752\n",
      "Epoch 109/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 185.8896 - mean_absolute_error: 185.8896 - val_loss: 182.8040 - val_mean_absolute_error: 182.8040\n",
      "Epoch 110/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 184.6959 - mean_absolute_error: 184.6959 - val_loss: 184.2902 - val_mean_absolute_error: 184.2902\n",
      "Epoch 111/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 183.6108 - mean_absolute_error: 183.6108 - val_loss: 185.7266 - val_mean_absolute_error: 185.7266\n",
      "Epoch 112/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 182.6191 - mean_absolute_error: 182.6191 - val_loss: 187.1178 - val_mean_absolute_error: 187.1178\n",
      "Epoch 113/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 181.7250 - mean_absolute_error: 181.7250 - val_loss: 188.4361 - val_mean_absolute_error: 188.4361\n",
      "Epoch 114/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 180.9363 - mean_absolute_error: 180.9363 - val_loss: 189.6808 - val_mean_absolute_error: 189.6808\n",
      "Epoch 115/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 180.2304 - mean_absolute_error: 180.2304 - val_loss: 190.8664 - val_mean_absolute_error: 190.8664\n",
      "Epoch 116/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 179.6012 - mean_absolute_error: 179.6012 - val_loss: 191.9801 - val_mean_absolute_error: 191.9801\n",
      "Epoch 117/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 179.0446 - mean_absolute_error: 179.0446 - val_loss: 193.0358 - val_mean_absolute_error: 193.0358\n",
      "Epoch 118/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 178.5407 - mean_absolute_error: 178.5407 - val_loss: 194.0506 - val_mean_absolute_error: 194.0506\n",
      "Epoch 119/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 178.0789 - mean_absolute_error: 178.0789 - val_loss: 195.0249 - val_mean_absolute_error: 195.0249\n",
      "Epoch 120/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 177.6465 - mean_absolute_error: 177.6465 - val_loss: 195.9727 - val_mean_absolute_error: 195.9727\n",
      "Epoch 121/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 177.2504 - mean_absolute_error: 177.2504 - val_loss: 196.8776 - val_mean_absolute_error: 196.8776\n",
      "Epoch 122/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 176.8915 - mean_absolute_error: 176.8915 - val_loss: 197.7455 - val_mean_absolute_error: 197.7455\n",
      "Epoch 123/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 176.5638 - mean_absolute_error: 176.5638 - val_loss: 198.5806 - val_mean_absolute_error: 198.5806\n",
      "Epoch 124/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 176.2788 - mean_absolute_error: 176.2788 - val_loss: 199.3399 - val_mean_absolute_error: 199.3399\n",
      "Epoch 125/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 176.0340 - mean_absolute_error: 176.0340 - val_loss: 200.0682 - val_mean_absolute_error: 200.0682\n",
      "Epoch 126/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 175.8203 - mean_absolute_error: 175.8203 - val_loss: 200.7662 - val_mean_absolute_error: 200.7662\n",
      "Epoch 127/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 175.6325 - mean_absolute_error: 175.6325 - val_loss: 201.4261 - val_mean_absolute_error: 201.4261\n",
      "Epoch 128/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 175.4656 - mean_absolute_error: 175.4656 - val_loss: 202.0795 - val_mean_absolute_error: 202.0795\n",
      "Epoch 129/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 175.3112 - mean_absolute_error: 175.3112 - val_loss: 202.7032 - val_mean_absolute_error: 202.7032\n",
      "Epoch 130/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 175.1735 - mean_absolute_error: 175.1735 - val_loss: 203.2961 - val_mean_absolute_error: 203.2961\n",
      "Epoch 131/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 175.0515 - mean_absolute_error: 175.0515 - val_loss: 203.8634 - val_mean_absolute_error: 203.8634\n",
      "Epoch 132/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.9429 - mean_absolute_error: 174.9429 - val_loss: 204.4062 - val_mean_absolute_error: 204.4062\n",
      "Epoch 133/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.8445 - mean_absolute_error: 174.8445 - val_loss: 204.9238 - val_mean_absolute_error: 204.9238\n",
      "Epoch 134/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.7553 - mean_absolute_error: 174.7553 - val_loss: 205.4202 - val_mean_absolute_error: 205.4202\n",
      "Epoch 135/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.6733 - mean_absolute_error: 174.6733 - val_loss: 205.9022 - val_mean_absolute_error: 205.9022\n",
      "Epoch 136/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.5981 - mean_absolute_error: 174.5981 - val_loss: 206.3579 - val_mean_absolute_error: 206.3579\n",
      "Epoch 137/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.5288 - mean_absolute_error: 174.5288 - val_loss: 206.8010 - val_mean_absolute_error: 206.8010\n",
      "Epoch 138/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.4648 - mean_absolute_error: 174.4648 - val_loss: 207.2310 - val_mean_absolute_error: 207.2310\n",
      "Epoch 139/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.4045 - mean_absolute_error: 174.4045 - val_loss: 207.6487 - val_mean_absolute_error: 207.6487\n",
      "Epoch 140/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.3482 - mean_absolute_error: 174.3482 - val_loss: 208.0552 - val_mean_absolute_error: 208.0552\n",
      "Epoch 141/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.2964 - mean_absolute_error: 174.2964 - val_loss: 208.4442 - val_mean_absolute_error: 208.4442\n",
      "Epoch 142/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.2476 - mean_absolute_error: 174.2476 - val_loss: 208.8198 - val_mean_absolute_error: 208.8198\n",
      "Epoch 143/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.2027 - mean_absolute_error: 174.2027 - val_loss: 209.1778 - val_mean_absolute_error: 209.1778\n",
      "Epoch 144/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.1612 - mean_absolute_error: 174.1612 - val_loss: 209.5269 - val_mean_absolute_error: 209.5269\n",
      "Epoch 145/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.1233 - mean_absolute_error: 174.1233 - val_loss: 209.8496 - val_mean_absolute_error: 209.8496\n",
      "Epoch 146/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.0887 - mean_absolute_error: 174.0887 - val_loss: 210.1727 - val_mean_absolute_error: 210.1727\n",
      "Epoch 147/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.0558 - mean_absolute_error: 174.0558 - val_loss: 210.4779 - val_mean_absolute_error: 210.4779\n",
      "Epoch 148/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 174.0262 - mean_absolute_error: 174.0262 - val_loss: 210.7757 - val_mean_absolute_error: 210.7757\n",
      "Epoch 149/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.9984 - mean_absolute_error: 173.9984 - val_loss: 211.0633 - val_mean_absolute_error: 211.0633\n",
      "Epoch 150/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.9737 - mean_absolute_error: 173.9737 - val_loss: 211.3355 - val_mean_absolute_error: 211.3355\n",
      "Epoch 151/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.9512 - mean_absolute_error: 173.9512 - val_loss: 211.6010 - val_mean_absolute_error: 211.6010\n",
      "Epoch 152/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.9310 - mean_absolute_error: 173.9310 - val_loss: 211.8396 - val_mean_absolute_error: 211.8396\n",
      "Epoch 153/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.9139 - mean_absolute_error: 173.9139 - val_loss: 212.0702 - val_mean_absolute_error: 212.0702\n",
      "Epoch 154/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8980 - mean_absolute_error: 173.8980 - val_loss: 212.2856 - val_mean_absolute_error: 212.2856\n",
      "Epoch 155/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8841 - mean_absolute_error: 173.8841 - val_loss: 212.4914 - val_mean_absolute_error: 212.4914\n",
      "Epoch 156/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8719 - mean_absolute_error: 173.8719 - val_loss: 212.6833 - val_mean_absolute_error: 212.6833\n",
      "Epoch 157/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8613 - mean_absolute_error: 173.8613 - val_loss: 212.8613 - val_mean_absolute_error: 212.8613\n",
      "Epoch 158/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8523 - mean_absolute_error: 173.8523 - val_loss: 213.0302 - val_mean_absolute_error: 213.0302\n",
      "Epoch 159/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8439 - mean_absolute_error: 173.8439 - val_loss: 213.1882 - val_mean_absolute_error: 213.1882\n",
      "Epoch 160/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8367 - mean_absolute_error: 173.8367 - val_loss: 213.3442 - val_mean_absolute_error: 213.3442\n",
      "Epoch 161/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8299 - mean_absolute_error: 173.8299 - val_loss: 213.4889 - val_mean_absolute_error: 213.4889\n",
      "Epoch 162/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8246 - mean_absolute_error: 173.8246 - val_loss: 213.6090 - val_mean_absolute_error: 213.6090\n",
      "Epoch 163/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8204 - mean_absolute_error: 173.8204 - val_loss: 213.7316 - val_mean_absolute_error: 213.7316\n",
      "Epoch 164/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8166 - mean_absolute_error: 173.8166 - val_loss: 213.8450 - val_mean_absolute_error: 213.8450\n",
      "Epoch 165/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8132 - mean_absolute_error: 173.8132 - val_loss: 213.9398 - val_mean_absolute_error: 213.9398\n",
      "Epoch 166/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8107 - mean_absolute_error: 173.8107 - val_loss: 214.0299 - val_mean_absolute_error: 214.0299\n",
      "Epoch 167/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8087 - mean_absolute_error: 173.8087 - val_loss: 214.1176 - val_mean_absolute_error: 214.1176\n",
      "Epoch 168/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8065 - mean_absolute_error: 173.8065 - val_loss: 214.2009 - val_mean_absolute_error: 214.2009\n",
      "Epoch 169/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8045 - mean_absolute_error: 173.8045 - val_loss: 214.2794 - val_mean_absolute_error: 214.2794\n",
      "Epoch 170/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8031 - mean_absolute_error: 173.8031 - val_loss: 214.3462 - val_mean_absolute_error: 214.3462\n",
      "Epoch 171/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8015 - mean_absolute_error: 173.8015 - val_loss: 214.4178 - val_mean_absolute_error: 214.4178\n",
      "Epoch 172/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.8003 - mean_absolute_error: 173.8003 - val_loss: 214.4823 - val_mean_absolute_error: 214.4823\n",
      "Epoch 173/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7991 - mean_absolute_error: 173.7991 - val_loss: 214.5471 - val_mean_absolute_error: 214.5471\n",
      "Epoch 174/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7981 - mean_absolute_error: 173.7981 - val_loss: 214.6096 - val_mean_absolute_error: 214.6096\n",
      "Epoch 175/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7971 - mean_absolute_error: 173.7971 - val_loss: 214.6600 - val_mean_absolute_error: 214.6600\n",
      "Epoch 176/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7964 - mean_absolute_error: 173.7964 - val_loss: 214.7080 - val_mean_absolute_error: 214.7080\n",
      "Epoch 177/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7957 - mean_absolute_error: 173.7957 - val_loss: 214.7535 - val_mean_absolute_error: 214.7535\n",
      "Epoch 178/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7952 - mean_absolute_error: 173.7952 - val_loss: 214.7969 - val_mean_absolute_error: 214.7969\n",
      "Epoch 179/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7948 - mean_absolute_error: 173.7948 - val_loss: 214.8379 - val_mean_absolute_error: 214.8379\n",
      "Epoch 180/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7945 - mean_absolute_error: 173.7945 - val_loss: 214.8788 - val_mean_absolute_error: 214.8788\n",
      "Epoch 181/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7939 - mean_absolute_error: 173.7939 - val_loss: 214.9198 - val_mean_absolute_error: 214.9198\n",
      "Epoch 182/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7936 - mean_absolute_error: 173.7936 - val_loss: 214.9561 - val_mean_absolute_error: 214.9561\n",
      "Epoch 183/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7930 - mean_absolute_error: 173.7930 - val_loss: 214.9947 - val_mean_absolute_error: 214.9947\n",
      "Epoch 184/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7924 - mean_absolute_error: 173.7924 - val_loss: 215.0261 - val_mean_absolute_error: 215.0261\n",
      "Epoch 185/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7923 - mean_absolute_error: 173.7923 - val_loss: 215.0647 - val_mean_absolute_error: 215.0647\n",
      "Epoch 186/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7920 - mean_absolute_error: 173.7920 - val_loss: 215.1010 - val_mean_absolute_error: 215.1010\n",
      "Epoch 187/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7915 - mean_absolute_error: 173.7915 - val_loss: 215.1251 - val_mean_absolute_error: 215.1251\n",
      "Epoch 188/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7915 - mean_absolute_error: 173.7915 - val_loss: 215.1517 - val_mean_absolute_error: 215.1517\n",
      "Epoch 189/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7913 - mean_absolute_error: 173.7913 - val_loss: 215.1783 - val_mean_absolute_error: 215.1783\n",
      "Epoch 190/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7910 - mean_absolute_error: 173.7910 - val_loss: 215.2025 - val_mean_absolute_error: 215.2025\n",
      "Epoch 191/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7909 - mean_absolute_error: 173.7909 - val_loss: 215.2268 - val_mean_absolute_error: 215.2268\n",
      "Epoch 192/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7907 - mean_absolute_error: 173.7907 - val_loss: 215.2511 - val_mean_absolute_error: 215.2511\n",
      "Epoch 193/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7908 - mean_absolute_error: 173.7908 - val_loss: 215.2681 - val_mean_absolute_error: 215.2681\n",
      "Epoch 194/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7906 - mean_absolute_error: 173.7906 - val_loss: 215.2899 - val_mean_absolute_error: 215.2899\n",
      "Epoch 195/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7906 - mean_absolute_error: 173.7906 - val_loss: 215.3069 - val_mean_absolute_error: 215.3069\n",
      "Epoch 196/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7905 - mean_absolute_error: 173.7905 - val_loss: 215.3215 - val_mean_absolute_error: 215.3215\n",
      "Epoch 197/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7903 - mean_absolute_error: 173.7903 - val_loss: 215.3434 - val_mean_absolute_error: 215.3434\n",
      "Epoch 198/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7902 - mean_absolute_error: 173.7902 - val_loss: 215.3604 - val_mean_absolute_error: 215.3604\n",
      "Epoch 199/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7900 - mean_absolute_error: 173.7900 - val_loss: 215.3774 - val_mean_absolute_error: 215.3774\n",
      "Epoch 200/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 173.7900 - mean_absolute_error: 173.7900 - val_loss: 215.3896 - val_mean_absolute_error: 215.3896\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='mae', patience=3)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(1000, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(20, activation='relu'),#\n",
    "  #tf.keras.layers.Dropout(0.1),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "batch_size = 4\n",
    "epochs = 200\n",
    "\n",
    "model.compile(\n",
    "    tf.keras.optimizers.SGD(), \n",
    "    loss='mae',\n",
    "    metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "#model.compile(\n",
    "#    loss=\"mae\", \n",
    "#    optimizer=\"adam\", \n",
    "#    metrics=[\"mean_absolute_error\"],\n",
    "#    callbacks=[callback])\n",
    "\n",
    "history=model.fit(\n",
    "    combined_dataset.X, \n",
    "    combined_dataset.y, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs, \n",
    "    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e8cc2354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1123a431248>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f0ca1e81",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'History'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6812/3376775401.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tdaf-tf2p7h2\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2757\u001b[0m     return gca().plot(\n\u001b[0;32m   2758\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2759\u001b[1;33m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tdaf-tf2p7h2\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1634\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1635\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request_autoscale_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1636\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tdaf-tf2p7h2\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36madd_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   2281\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2283\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2284\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2285\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'_child{len(self._children)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tdaf-tf2p7h2\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   2304\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2305\u001b[0m         \"\"\"\n\u001b[1;32m-> 2306\u001b[1;33m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2307\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2308\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tdaf-tf2p7h2\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mget_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[1;34m\"\"\"Return the `~matplotlib.path.Path` associated with this line.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tdaf-tf2p7h2\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mrecache\u001b[1;34m(self, always)\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m             \u001b[0myconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tdaf-tf2p7h2\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1298\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'History'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc651b80",
   "metadata": {},
   "source": [
    "MACCS + TDAF\n",
    "1377 to 1059 with combined\n",
    "to 411 with a smaller network\n",
    "233 smaller network\n",
    "CM+TDAF\n",
    "215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b75e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the data without shuffling or splitting\n",
    "tasks, datasets, transformers = dc.molnet.load_qm7(\n",
    "    shard_size=2000,\n",
    "    featurizer=dc.feat.CoulombMatrix\n",
    "    (max_atoms=23),\n",
    "    splitter=None)\n",
    "\n",
    "# this makes the actual topological datasets\n",
    "combined_dataset = dc.data.DiskDataset.from_numpy(\n",
    "    new_X_data, \n",
    "    y_data, \n",
    "    ids=SMILES_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001ccb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1539/1539 [==============================] - 4s 2ms/step - loss: 0.7803 - mean_absolute_error: 0.7803 - val_loss: 0.9547 - val_mean_absolute_error: 0.9547\n",
      "Epoch 2/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7712 - mean_absolute_error: 0.7712 - val_loss: 1.0089 - val_mean_absolute_error: 1.0089\n",
      "Epoch 3/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7651 - mean_absolute_error: 0.7651 - val_loss: 0.9996 - val_mean_absolute_error: 0.9996\n",
      "Epoch 4/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7609 - mean_absolute_error: 0.7609 - val_loss: 1.0178 - val_mean_absolute_error: 1.0178\n",
      "Epoch 5/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7582 - mean_absolute_error: 0.7582 - val_loss: 0.9583 - val_mean_absolute_error: 0.9583\n",
      "Epoch 6/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7555 - mean_absolute_error: 0.7555 - val_loss: 1.0339 - val_mean_absolute_error: 1.0339\n",
      "Epoch 7/200\n",
      "1539/1539 [==============================] - 4s 2ms/step - loss: 0.7534 - mean_absolute_error: 0.7534 - val_loss: 0.9599 - val_mean_absolute_error: 0.9599\n",
      "Epoch 8/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7524 - mean_absolute_error: 0.7524 - val_loss: 1.0005 - val_mean_absolute_error: 1.0005\n",
      "Epoch 9/200\n",
      "1539/1539 [==============================] - 4s 2ms/step - loss: 0.7509 - mean_absolute_error: 0.7509 - val_loss: 0.9783 - val_mean_absolute_error: 0.9783\n",
      "Epoch 10/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7494 - mean_absolute_error: 0.7494 - val_loss: 1.0306 - val_mean_absolute_error: 1.0306\n",
      "Epoch 11/200\n",
      "1539/1539 [==============================] - 4s 2ms/step - loss: 0.7479 - mean_absolute_error: 0.7479 - val_loss: 1.0216 - val_mean_absolute_error: 1.0216\n",
      "Epoch 12/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7467 - mean_absolute_error: 0.7467 - val_loss: 0.9735 - val_mean_absolute_error: 0.9735\n",
      "Epoch 13/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7456 - mean_absolute_error: 0.7456 - val_loss: 0.9692 - val_mean_absolute_error: 0.9692\n",
      "Epoch 14/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7444 - mean_absolute_error: 0.7444 - val_loss: 0.9892 - val_mean_absolute_error: 0.9892\n",
      "Epoch 15/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7437 - mean_absolute_error: 0.7437 - val_loss: 1.0235 - val_mean_absolute_error: 1.0235\n",
      "Epoch 16/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7431 - mean_absolute_error: 0.7431 - val_loss: 0.9671 - val_mean_absolute_error: 0.9671\n",
      "Epoch 17/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7434 - mean_absolute_error: 0.7434 - val_loss: 1.0021 - val_mean_absolute_error: 1.0021\n",
      "Epoch 18/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7414 - mean_absolute_error: 0.7414 - val_loss: 1.0256 - val_mean_absolute_error: 1.0256\n",
      "Epoch 19/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7416 - mean_absolute_error: 0.7416 - val_loss: 1.0166 - val_mean_absolute_error: 1.0166\n",
      "Epoch 20/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7410 - mean_absolute_error: 0.7410 - val_loss: 0.9733 - val_mean_absolute_error: 0.9733\n",
      "Epoch 21/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7403 - mean_absolute_error: 0.7403 - val_loss: 1.0170 - val_mean_absolute_error: 1.0170\n",
      "Epoch 22/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7405 - mean_absolute_error: 0.7405 - val_loss: 1.0164 - val_mean_absolute_error: 1.0164\n",
      "Epoch 23/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7394 - mean_absolute_error: 0.7394 - val_loss: 1.1078 - val_mean_absolute_error: 1.1078\n",
      "Epoch 24/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7391 - mean_absolute_error: 0.7391 - val_loss: 1.0019 - val_mean_absolute_error: 1.0019\n",
      "Epoch 25/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7386 - mean_absolute_error: 0.7386 - val_loss: 1.0161 - val_mean_absolute_error: 1.0161\n",
      "Epoch 26/200\n",
      "1539/1539 [==============================] - 4s 2ms/step - loss: 0.7383 - mean_absolute_error: 0.7383 - val_loss: 1.0093 - val_mean_absolute_error: 1.0093\n",
      "Epoch 27/200\n",
      "1539/1539 [==============================] - 4s 2ms/step - loss: 0.7376 - mean_absolute_error: 0.7376 - val_loss: 1.0681 - val_mean_absolute_error: 1.0681\n",
      "Epoch 28/200\n",
      "1539/1539 [==============================] - 4s 2ms/step - loss: 0.7377 - mean_absolute_error: 0.7377 - val_loss: 0.9851 - val_mean_absolute_error: 0.9851\n",
      "Epoch 29/200\n",
      "1539/1539 [==============================] - 4s 3ms/step - loss: 0.7371 - mean_absolute_error: 0.7371 - val_loss: 1.0145 - val_mean_absolute_error: 1.0145\n",
      "Epoch 30/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7352 - mean_absolute_error: 0.7352 - val_loss: 1.0172 - val_mean_absolute_error: 1.0172\n",
      "Epoch 31/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7364 - mean_absolute_error: 0.7364 - val_loss: 0.9950 - val_mean_absolute_error: 0.9950\n",
      "Epoch 32/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7351 - mean_absolute_error: 0.7351 - val_loss: 1.0379 - val_mean_absolute_error: 1.0379\n",
      "Epoch 33/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7358 - mean_absolute_error: 0.7358 - val_loss: 1.0262 - val_mean_absolute_error: 1.0262\n",
      "Epoch 34/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7345 - mean_absolute_error: 0.7345 - val_loss: 1.0005 - val_mean_absolute_error: 1.0005\n",
      "Epoch 35/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7352 - mean_absolute_error: 0.7352 - val_loss: 0.9878 - val_mean_absolute_error: 0.9878\n",
      "Epoch 36/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7339 - mean_absolute_error: 0.7339 - val_loss: 1.0495 - val_mean_absolute_error: 1.0495\n",
      "Epoch 37/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7338 - mean_absolute_error: 0.7338 - val_loss: 1.0453 - val_mean_absolute_error: 1.0453\n",
      "Epoch 38/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7336 - mean_absolute_error: 0.7336 - val_loss: 1.0473 - val_mean_absolute_error: 1.0473\n",
      "Epoch 39/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7338 - mean_absolute_error: 0.7338 - val_loss: 1.0110 - val_mean_absolute_error: 1.0110\n",
      "Epoch 40/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7322 - mean_absolute_error: 0.7322 - val_loss: 0.9751 - val_mean_absolute_error: 0.9751\n",
      "Epoch 41/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7332 - mean_absolute_error: 0.7332 - val_loss: 0.9902 - val_mean_absolute_error: 0.9902\n",
      "Epoch 42/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7332 - mean_absolute_error: 0.7332 - val_loss: 0.9953 - val_mean_absolute_error: 0.9953\n",
      "Epoch 43/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7332 - mean_absolute_error: 0.7332 - val_loss: 1.0611 - val_mean_absolute_error: 1.0611\n",
      "Epoch 44/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7319 - mean_absolute_error: 0.7319 - val_loss: 0.9626 - val_mean_absolute_error: 0.9626\n",
      "Epoch 45/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7326 - mean_absolute_error: 0.7326 - val_loss: 1.0599 - val_mean_absolute_error: 1.0599\n",
      "Epoch 46/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7317 - mean_absolute_error: 0.7317 - val_loss: 1.0091 - val_mean_absolute_error: 1.0091\n",
      "Epoch 47/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7313 - mean_absolute_error: 0.7313 - val_loss: 1.0118 - val_mean_absolute_error: 1.0118\n",
      "Epoch 48/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7321 - mean_absolute_error: 0.7321 - val_loss: 0.9955 - val_mean_absolute_error: 0.9955\n",
      "Epoch 49/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7311 - mean_absolute_error: 0.7311 - val_loss: 0.9690 - val_mean_absolute_error: 0.9690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7319 - mean_absolute_error: 0.7319 - val_loss: 0.9790 - val_mean_absolute_error: 0.9790\n",
      "Epoch 51/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7308 - mean_absolute_error: 0.7308 - val_loss: 0.8942 - val_mean_absolute_error: 0.8942\n",
      "Epoch 52/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7317 - mean_absolute_error: 0.7317 - val_loss: 0.9793 - val_mean_absolute_error: 0.9793\n",
      "Epoch 53/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7306 - mean_absolute_error: 0.7306 - val_loss: 1.0109 - val_mean_absolute_error: 1.0109\n",
      "Epoch 54/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7308 - mean_absolute_error: 0.7308 - val_loss: 1.0062 - val_mean_absolute_error: 1.0062\n",
      "Epoch 55/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7314 - mean_absolute_error: 0.7314 - val_loss: 0.9872 - val_mean_absolute_error: 0.9872\n",
      "Epoch 56/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7315 - mean_absolute_error: 0.7315 - val_loss: 1.0847 - val_mean_absolute_error: 1.0847\n",
      "Epoch 57/200\n",
      "1539/1539 [==============================] - 4s 2ms/step - loss: 0.7305 - mean_absolute_error: 0.7305 - val_loss: 1.0265 - val_mean_absolute_error: 1.0265\n",
      "Epoch 58/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7309 - mean_absolute_error: 0.7309 - val_loss: 1.0218 - val_mean_absolute_error: 1.0218\n",
      "Epoch 59/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7302 - mean_absolute_error: 0.7302 - val_loss: 1.0063 - val_mean_absolute_error: 1.0063\n",
      "Epoch 60/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7302 - mean_absolute_error: 0.7302 - val_loss: 0.9741 - val_mean_absolute_error: 0.9741\n",
      "Epoch 61/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7303 - mean_absolute_error: 0.7303 - val_loss: 1.0592 - val_mean_absolute_error: 1.0592\n",
      "Epoch 62/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7300 - mean_absolute_error: 0.7300 - val_loss: 1.0286 - val_mean_absolute_error: 1.0286\n",
      "Epoch 63/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7302 - mean_absolute_error: 0.7302 - val_loss: 0.9794 - val_mean_absolute_error: 0.9794\n",
      "Epoch 64/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7301 - mean_absolute_error: 0.7301 - val_loss: 1.0696 - val_mean_absolute_error: 1.0696\n",
      "Epoch 65/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7299 - mean_absolute_error: 0.7299 - val_loss: 0.9752 - val_mean_absolute_error: 0.9752\n",
      "Epoch 66/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7303 - mean_absolute_error: 0.7303 - val_loss: 0.9359 - val_mean_absolute_error: 0.9359\n",
      "Epoch 67/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7296 - mean_absolute_error: 0.7296 - val_loss: 0.9964 - val_mean_absolute_error: 0.9964\n",
      "Epoch 68/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7298 - mean_absolute_error: 0.7298 - val_loss: 1.0342 - val_mean_absolute_error: 1.0342\n",
      "Epoch 69/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7297 - mean_absolute_error: 0.7297 - val_loss: 0.9731 - val_mean_absolute_error: 0.9731\n",
      "Epoch 70/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7295 - mean_absolute_error: 0.7295 - val_loss: 0.9976 - val_mean_absolute_error: 0.9976\n",
      "Epoch 71/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7287 - mean_absolute_error: 0.7287 - val_loss: 1.0111 - val_mean_absolute_error: 1.0111\n",
      "Epoch 72/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7290 - mean_absolute_error: 0.7290 - val_loss: 1.0076 - val_mean_absolute_error: 1.0076\n",
      "Epoch 73/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7291 - mean_absolute_error: 0.7291 - val_loss: 1.1131 - val_mean_absolute_error: 1.1131\n",
      "Epoch 74/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7290 - mean_absolute_error: 0.7290 - val_loss: 1.1209 - val_mean_absolute_error: 1.1209\n",
      "Epoch 75/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7293 - mean_absolute_error: 0.7293 - val_loss: 1.0290 - val_mean_absolute_error: 1.0290\n",
      "Epoch 76/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7281 - mean_absolute_error: 0.7281 - val_loss: 1.0054 - val_mean_absolute_error: 1.0054\n",
      "Epoch 77/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7270 - mean_absolute_error: 0.7270 - val_loss: 1.0308 - val_mean_absolute_error: 1.0308\n",
      "Epoch 78/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7291 - mean_absolute_error: 0.7291 - val_loss: 0.9926 - val_mean_absolute_error: 0.9926\n",
      "Epoch 79/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7279 - mean_absolute_error: 0.7279 - val_loss: 1.0211 - val_mean_absolute_error: 1.0211\n",
      "Epoch 80/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7275 - mean_absolute_error: 0.7275 - val_loss: 1.0662 - val_mean_absolute_error: 1.0662\n",
      "Epoch 81/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7277 - mean_absolute_error: 0.7277 - val_loss: 1.0180 - val_mean_absolute_error: 1.0180\n",
      "Epoch 82/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7282 - mean_absolute_error: 0.7282 - val_loss: 0.9747 - val_mean_absolute_error: 0.9747\n",
      "Epoch 83/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7279 - mean_absolute_error: 0.7279 - val_loss: 1.0640 - val_mean_absolute_error: 1.0640\n",
      "Epoch 84/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7283 - mean_absolute_error: 0.7283 - val_loss: 1.0292 - val_mean_absolute_error: 1.0292\n",
      "Epoch 85/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7286 - mean_absolute_error: 0.7286 - val_loss: 0.9839 - val_mean_absolute_error: 0.9839\n",
      "Epoch 86/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7271 - mean_absolute_error: 0.7271 - val_loss: 1.0629 - val_mean_absolute_error: 1.0629\n",
      "Epoch 87/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7276 - mean_absolute_error: 0.7276 - val_loss: 0.9854 - val_mean_absolute_error: 0.9854\n",
      "Epoch 88/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7274 - mean_absolute_error: 0.7274 - val_loss: 1.0707 - val_mean_absolute_error: 1.0707\n",
      "Epoch 89/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7274 - mean_absolute_error: 0.7274 - val_loss: 0.9939 - val_mean_absolute_error: 0.9939\n",
      "Epoch 90/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7274 - mean_absolute_error: 0.7274 - val_loss: 0.9747 - val_mean_absolute_error: 0.9747\n",
      "Epoch 91/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7271 - mean_absolute_error: 0.7271 - val_loss: 0.9483 - val_mean_absolute_error: 0.9483\n",
      "Epoch 92/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7274 - mean_absolute_error: 0.7274 - val_loss: 1.0049 - val_mean_absolute_error: 1.0049\n",
      "Epoch 93/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7270 - mean_absolute_error: 0.7270 - val_loss: 0.9626 - val_mean_absolute_error: 0.9626\n",
      "Epoch 94/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7271 - mean_absolute_error: 0.7271 - val_loss: 1.0022 - val_mean_absolute_error: 1.0022\n",
      "Epoch 95/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7271 - mean_absolute_error: 0.7271 - val_loss: 1.0097 - val_mean_absolute_error: 1.0097\n",
      "Epoch 96/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7275 - mean_absolute_error: 0.7275 - val_loss: 1.0044 - val_mean_absolute_error: 1.0044\n",
      "Epoch 97/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7275 - mean_absolute_error: 0.7275 - val_loss: 0.9911 - val_mean_absolute_error: 0.9911\n",
      "Epoch 98/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7273 - mean_absolute_error: 0.7273 - val_loss: 0.9451 - val_mean_absolute_error: 0.9451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7266 - mean_absolute_error: 0.7266 - val_loss: 0.9917 - val_mean_absolute_error: 0.9917\n",
      "Epoch 100/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7279 - mean_absolute_error: 0.7279 - val_loss: 1.0348 - val_mean_absolute_error: 1.0348\n",
      "Epoch 101/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7272 - mean_absolute_error: 0.7272 - val_loss: 1.0202 - val_mean_absolute_error: 1.0202\n",
      "Epoch 102/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7268 - mean_absolute_error: 0.7268 - val_loss: 1.0273 - val_mean_absolute_error: 1.0273\n",
      "Epoch 103/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7257 - mean_absolute_error: 0.7257 - val_loss: 1.0752 - val_mean_absolute_error: 1.0752\n",
      "Epoch 104/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7270 - mean_absolute_error: 0.7270 - val_loss: 1.0519 - val_mean_absolute_error: 1.0519\n",
      "Epoch 105/200\n",
      "1539/1539 [==============================] - 4s 2ms/step - loss: 0.7266 - mean_absolute_error: 0.7266 - val_loss: 1.0251 - val_mean_absolute_error: 1.0251\n",
      "Epoch 106/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7258 - mean_absolute_error: 0.7258 - val_loss: 0.9889 - val_mean_absolute_error: 0.9889\n",
      "Epoch 107/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7263 - mean_absolute_error: 0.7263 - val_loss: 0.9602 - val_mean_absolute_error: 0.9602\n",
      "Epoch 108/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7266 - mean_absolute_error: 0.7266 - val_loss: 1.0943 - val_mean_absolute_error: 1.0943\n",
      "Epoch 109/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7266 - mean_absolute_error: 0.7266 - val_loss: 0.9767 - val_mean_absolute_error: 0.9767\n",
      "Epoch 110/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7257 - mean_absolute_error: 0.7257 - val_loss: 0.9933 - val_mean_absolute_error: 0.9933\n",
      "Epoch 111/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7254 - mean_absolute_error: 0.7254 - val_loss: 0.9556 - val_mean_absolute_error: 0.9556\n",
      "Epoch 112/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7269 - mean_absolute_error: 0.7269 - val_loss: 1.0439 - val_mean_absolute_error: 1.0439\n",
      "Epoch 113/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7269 - mean_absolute_error: 0.7269 - val_loss: 1.0263 - val_mean_absolute_error: 1.0263\n",
      "Epoch 114/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7264 - mean_absolute_error: 0.7264 - val_loss: 0.9665 - val_mean_absolute_error: 0.9665\n",
      "Epoch 115/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7260 - mean_absolute_error: 0.7260 - val_loss: 0.9718 - val_mean_absolute_error: 0.9718\n",
      "Epoch 116/200\n",
      "1539/1539 [==============================] - 4s 2ms/step - loss: 0.7262 - mean_absolute_error: 0.7262 - val_loss: 1.1383 - val_mean_absolute_error: 1.1383\n",
      "Epoch 117/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7263 - mean_absolute_error: 0.7263 - val_loss: 1.0222 - val_mean_absolute_error: 1.0222\n",
      "Epoch 118/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7261 - mean_absolute_error: 0.7261 - val_loss: 1.0088 - val_mean_absolute_error: 1.0088\n",
      "Epoch 119/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7267 - mean_absolute_error: 0.7267 - val_loss: 1.0784 - val_mean_absolute_error: 1.0784\n",
      "Epoch 120/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7265 - mean_absolute_error: 0.7265 - val_loss: 1.0469 - val_mean_absolute_error: 1.0469\n",
      "Epoch 121/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7254 - mean_absolute_error: 0.7254 - val_loss: 1.1043 - val_mean_absolute_error: 1.1043\n",
      "Epoch 122/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7255 - mean_absolute_error: 0.7255 - val_loss: 0.9989 - val_mean_absolute_error: 0.9989\n",
      "Epoch 123/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7263 - mean_absolute_error: 0.7263 - val_loss: 0.9924 - val_mean_absolute_error: 0.9924\n",
      "Epoch 124/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7254 - mean_absolute_error: 0.7254 - val_loss: 1.0081 - val_mean_absolute_error: 1.0081\n",
      "Epoch 125/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7253 - mean_absolute_error: 0.7253 - val_loss: 1.0393 - val_mean_absolute_error: 1.0393\n",
      "Epoch 126/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7256 - mean_absolute_error: 0.7256 - val_loss: 0.9857 - val_mean_absolute_error: 0.9857\n",
      "Epoch 127/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7255 - mean_absolute_error: 0.7255 - val_loss: 1.0100 - val_mean_absolute_error: 1.0100\n",
      "Epoch 128/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7260 - mean_absolute_error: 0.7260 - val_loss: 1.0251 - val_mean_absolute_error: 1.0251\n",
      "Epoch 129/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7255 - mean_absolute_error: 0.7255 - val_loss: 0.9484 - val_mean_absolute_error: 0.9484\n",
      "Epoch 130/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7258 - mean_absolute_error: 0.7258 - val_loss: 0.9719 - val_mean_absolute_error: 0.9719\n",
      "Epoch 131/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7257 - mean_absolute_error: 0.7257 - val_loss: 0.9950 - val_mean_absolute_error: 0.9950\n",
      "Epoch 132/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7255 - mean_absolute_error: 0.7255 - val_loss: 1.0769 - val_mean_absolute_error: 1.0769\n",
      "Epoch 133/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7261 - mean_absolute_error: 0.7261 - val_loss: 1.0271 - val_mean_absolute_error: 1.0271\n",
      "Epoch 134/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7251 - mean_absolute_error: 0.7251 - val_loss: 0.9576 - val_mean_absolute_error: 0.9576\n",
      "Epoch 135/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7251 - mean_absolute_error: 0.7251 - val_loss: 1.0253 - val_mean_absolute_error: 1.0253\n",
      "Epoch 136/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7260 - mean_absolute_error: 0.7260 - val_loss: 0.9925 - val_mean_absolute_error: 0.9925\n",
      "Epoch 137/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7251 - mean_absolute_error: 0.7251 - val_loss: 0.9621 - val_mean_absolute_error: 0.9621\n",
      "Epoch 138/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7239 - mean_absolute_error: 0.7239 - val_loss: 0.9731 - val_mean_absolute_error: 0.9731\n",
      "Epoch 139/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7247 - mean_absolute_error: 0.7247 - val_loss: 0.9509 - val_mean_absolute_error: 0.9509\n",
      "Epoch 140/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7253 - mean_absolute_error: 0.7253 - val_loss: 1.0083 - val_mean_absolute_error: 1.0083\n",
      "Epoch 141/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7249 - mean_absolute_error: 0.7249 - val_loss: 1.0192 - val_mean_absolute_error: 1.0192\n",
      "Epoch 142/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7250 - mean_absolute_error: 0.7250 - val_loss: 0.9552 - val_mean_absolute_error: 0.9552\n",
      "Epoch 143/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7250 - mean_absolute_error: 0.7250 - val_loss: 1.0311 - val_mean_absolute_error: 1.0311\n",
      "Epoch 144/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7249 - mean_absolute_error: 0.7249 - val_loss: 1.0916 - val_mean_absolute_error: 1.0916\n",
      "Epoch 145/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7240 - mean_absolute_error: 0.7240 - val_loss: 0.9718 - val_mean_absolute_error: 0.9718\n",
      "Epoch 146/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7249 - mean_absolute_error: 0.7249 - val_loss: 0.9611 - val_mean_absolute_error: 0.9611\n",
      "Epoch 147/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7248 - mean_absolute_error: 0.7248 - val_loss: 1.0578 - val_mean_absolute_error: 1.0578\n",
      "Epoch 148/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7249 - mean_absolute_error: 0.7249 - val_loss: 0.9939 - val_mean_absolute_error: 0.9939\n",
      "Epoch 149/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7239 - mean_absolute_error: 0.7239 - val_loss: 0.9156 - val_mean_absolute_error: 0.9156\n",
      "Epoch 150/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7265 - mean_absolute_error: 0.7265 - val_loss: 1.0062 - val_mean_absolute_error: 1.0062\n",
      "Epoch 151/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7231 - mean_absolute_error: 0.7231 - val_loss: 1.0815 - val_mean_absolute_error: 1.0815\n",
      "Epoch 152/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7244 - mean_absolute_error: 0.7244 - val_loss: 1.0374 - val_mean_absolute_error: 1.0374\n",
      "Epoch 153/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7241 - mean_absolute_error: 0.7241 - val_loss: 1.0575 - val_mean_absolute_error: 1.0575\n",
      "Epoch 154/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7246 - mean_absolute_error: 0.7246 - val_loss: 1.0288 - val_mean_absolute_error: 1.0288\n",
      "Epoch 155/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7236 - mean_absolute_error: 0.7236 - val_loss: 1.0332 - val_mean_absolute_error: 1.0332\n",
      "Epoch 156/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7244 - mean_absolute_error: 0.7244 - val_loss: 1.0565 - val_mean_absolute_error: 1.0565\n",
      "Epoch 157/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7251 - mean_absolute_error: 0.7251 - val_loss: 1.0194 - val_mean_absolute_error: 1.0194\n",
      "Epoch 158/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7239 - mean_absolute_error: 0.7239 - val_loss: 1.0180 - val_mean_absolute_error: 1.0180\n",
      "Epoch 159/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7240 - mean_absolute_error: 0.7240 - val_loss: 1.0317 - val_mean_absolute_error: 1.0317\n",
      "Epoch 160/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7237 - mean_absolute_error: 0.7237 - val_loss: 1.0547 - val_mean_absolute_error: 1.0547\n",
      "Epoch 161/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7244 - mean_absolute_error: 0.7244 - val_loss: 0.9930 - val_mean_absolute_error: 0.9930\n",
      "Epoch 162/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7239 - mean_absolute_error: 0.7239 - val_loss: 1.0007 - val_mean_absolute_error: 1.0007\n",
      "Epoch 163/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7243 - mean_absolute_error: 0.7243 - val_loss: 0.9362 - val_mean_absolute_error: 0.9362\n",
      "Epoch 164/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7242 - mean_absolute_error: 0.7242 - val_loss: 1.0288 - val_mean_absolute_error: 1.0288\n",
      "Epoch 165/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7241 - mean_absolute_error: 0.7241 - val_loss: 1.0398 - val_mean_absolute_error: 1.0398\n",
      "Epoch 166/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7245 - mean_absolute_error: 0.7245 - val_loss: 1.0058 - val_mean_absolute_error: 1.0058\n",
      "Epoch 167/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7236 - mean_absolute_error: 0.7236 - val_loss: 1.0082 - val_mean_absolute_error: 1.0082\n",
      "Epoch 168/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7241 - mean_absolute_error: 0.7241 - val_loss: 0.9648 - val_mean_absolute_error: 0.9648\n",
      "Epoch 169/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7239 - mean_absolute_error: 0.7239 - val_loss: 1.0182 - val_mean_absolute_error: 1.0182\n",
      "Epoch 170/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7242 - mean_absolute_error: 0.7242 - val_loss: 0.9907 - val_mean_absolute_error: 0.9907\n",
      "Epoch 171/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7236 - mean_absolute_error: 0.7236 - val_loss: 0.9695 - val_mean_absolute_error: 0.9695\n",
      "Epoch 172/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7238 - mean_absolute_error: 0.7238 - val_loss: 1.0716 - val_mean_absolute_error: 1.0716\n",
      "Epoch 173/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7238 - mean_absolute_error: 0.7238 - val_loss: 1.0942 - val_mean_absolute_error: 1.0942\n",
      "Epoch 174/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7243 - mean_absolute_error: 0.7243 - val_loss: 0.9597 - val_mean_absolute_error: 0.9597\n",
      "Epoch 175/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7233 - mean_absolute_error: 0.7233 - val_loss: 1.0223 - val_mean_absolute_error: 1.0223\n",
      "Epoch 176/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7235 - mean_absolute_error: 0.7235 - val_loss: 0.9908 - val_mean_absolute_error: 0.9908\n",
      "Epoch 177/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7232 - mean_absolute_error: 0.7232 - val_loss: 0.9798 - val_mean_absolute_error: 0.9798\n",
      "Epoch 178/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7232 - mean_absolute_error: 0.7232 - val_loss: 1.0107 - val_mean_absolute_error: 1.0107\n",
      "Epoch 179/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7241 - mean_absolute_error: 0.7241 - val_loss: 1.0239 - val_mean_absolute_error: 1.0239\n",
      "Epoch 180/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7240 - mean_absolute_error: 0.7240 - val_loss: 1.0509 - val_mean_absolute_error: 1.0509\n",
      "Epoch 181/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7232 - mean_absolute_error: 0.7232 - val_loss: 1.0067 - val_mean_absolute_error: 1.0067\n",
      "Epoch 182/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7237 - mean_absolute_error: 0.7237 - val_loss: 1.0056 - val_mean_absolute_error: 1.0056\n",
      "Epoch 183/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7240 - mean_absolute_error: 0.7240 - val_loss: 1.0492 - val_mean_absolute_error: 1.0492\n",
      "Epoch 184/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7238 - mean_absolute_error: 0.7238 - val_loss: 1.0510 - val_mean_absolute_error: 1.0510\n",
      "Epoch 185/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7240 - mean_absolute_error: 0.7240 - val_loss: 1.0237 - val_mean_absolute_error: 1.0237\n",
      "Epoch 186/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7237 - mean_absolute_error: 0.7237 - val_loss: 1.0176 - val_mean_absolute_error: 1.0176\n",
      "Epoch 187/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7244 - mean_absolute_error: 0.7244 - val_loss: 1.0060 - val_mean_absolute_error: 1.0060\n",
      "Epoch 188/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7232 - mean_absolute_error: 0.7232 - val_loss: 1.0062 - val_mean_absolute_error: 1.0062\n",
      "Epoch 189/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7235 - mean_absolute_error: 0.7235 - val_loss: 1.0487 - val_mean_absolute_error: 1.0487\n",
      "Epoch 190/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7231 - mean_absolute_error: 0.7231 - val_loss: 1.0446 - val_mean_absolute_error: 1.0446\n",
      "Epoch 191/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7230 - mean_absolute_error: 0.7230 - val_loss: 0.9286 - val_mean_absolute_error: 0.9286\n",
      "Epoch 192/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7243 - mean_absolute_error: 0.7243 - val_loss: 0.9844 - val_mean_absolute_error: 0.9844\n",
      "Epoch 193/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7236 - mean_absolute_error: 0.7236 - val_loss: 1.0319 - val_mean_absolute_error: 1.0319\n",
      "Epoch 194/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7239 - mean_absolute_error: 0.7239 - val_loss: 1.0472 - val_mean_absolute_error: 1.0472\n",
      "Epoch 195/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7229 - mean_absolute_error: 0.7229 - val_loss: 0.9639 - val_mean_absolute_error: 0.9639\n",
      "Epoch 196/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7233 - mean_absolute_error: 0.7233 - val_loss: 0.9671 - val_mean_absolute_error: 0.9671\n",
      "Epoch 197/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7234 - mean_absolute_error: 0.7234 - val_loss: 0.9915 - val_mean_absolute_error: 0.9915\n",
      "Epoch 198/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7238 - mean_absolute_error: 0.7238 - val_loss: 1.0372 - val_mean_absolute_error: 1.0372\n",
      "Epoch 199/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7231 - mean_absolute_error: 0.7231 - val_loss: 0.9570 - val_mean_absolute_error: 0.9570\n",
      "Epoch 200/200\n",
      "1539/1539 [==============================] - 3s 2ms/step - loss: 0.7229 - mean_absolute_error: 0.7229 - val_loss: 1.0115 - val_mean_absolute_error: 1.0115\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import tensorflow as tf\n",
    "# This loads the data without shuffling or splitting\n",
    "tasks, datasets, transformers = dc.molnet.load_qm7(\n",
    "    shard_size=2000,\n",
    "    featurizer=dc.feat.CoulombMatrix\n",
    "    (max_atoms=23),\n",
    "    splitter=None)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='mae', patience=3)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(1000, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(20, activation='relu'),#\n",
    "  #tf.keras.layers.Dropout(0.1),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "batch_size = 4\n",
    "epochs = 200\n",
    "\n",
    "model.compile(\n",
    "    tf.keras.optimizers.SGD(), \n",
    "    loss='mae',\n",
    "    metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "#model.compile(\n",
    "#    loss=\"mae\", \n",
    "#    optimizer=\"adam\", \n",
    "#    metrics=[\"mean_absolute_error\"],\n",
    "#    callbacks=[callback])\n",
    "\n",
    "history=model.fit(\n",
    "    datasets[0].X, \n",
    "    datasets[0].y, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs, \n",
    "    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea0f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='mae', patience=3)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(200, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(20, activation='relu'),#\n",
    "  #tf.keras.layers.Dropout(0.1),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "batch_size = 31\n",
    "epochs = 100\n",
    "\n",
    "model.compile(\n",
    "    tf.keras.optimizers.SGD(), \n",
    "    loss='mae',\n",
    "    metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "#model.compile(\n",
    "#    loss=\"mae\", \n",
    "#    optimizer=\"adam\", \n",
    "#    metrics=[\"mean_absolute_error\"],\n",
    "#    callbacks=[callback])\n",
    "\n",
    "model.fit(\n",
    "    combined_dataset.X, \n",
    "    combined_dataset.y, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs, \n",
    "    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23965edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df6a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
